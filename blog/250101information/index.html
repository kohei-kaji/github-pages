<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: January 13, 2025 --><html lang="en-us" >


<head><script src="/github-pages/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=github-pages/livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/github-pages/css/vendor-bundle.min.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/github-pages/css/wowchemy.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/github-pages/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/github-pages/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Kohei Kajikawa" />





  

<meta name="description" content="確率モデル・情報理論を使った心理言語学・計算心理言語学研究の紹介記事" />



<link rel="alternate" hreflang="en-us" href="http://localhost:1313/github-pages/blog/250101information/" />
<link rel="canonical" href="http://localhost:1313/github-pages/blog/250101information/" />



  <link rel="manifest" href="/github-pages/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/github-pages/media/icon_hu10978985619530458252.png" />
<link rel="apple-touch-icon" type="image/png" href="/github-pages/media/icon_hu10391704213710329260.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@GetResearchDev" />
  <meta property="twitter:creator" content="@GetResearchDev" />
<meta property="twitter:image" content="http://localhost:1313/github-pages/media/icon_hu10226378635250344412.png" />



  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Kohei Kajikawa" />
<meta property="og:url" content="http://localhost:1313/github-pages/blog/250101information/" />
<meta property="og:title" content="確率・情報理論を使った言語研究 | Kohei Kajikawa" />
<meta property="og:description" content="確率モデル・情報理論を使った心理言語学・計算心理言語学研究の紹介記事" /><meta property="og:image" content="http://localhost:1313/github-pages/media/icon_hu10226378635250344412.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2025-01-01T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2025-01-01T00:00:00&#43;00:00">
  







  




  
  
  

  
  

  


  
  <title>確率・情報理論を使った言語研究 | Kohei Kajikawa</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="d5d52f0ce60808c6878b6cba286aa3dc" >

  
  
  
  
  
  
  
  
  
  <script src="/github-pages/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/github-pages/">Kohei Kajikawa</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/github-pages/">Kohei Kajikawa</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#international_conference"><span>International Conference</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#journal"><span>Journal</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#domestic_conference"><span>Domestic Conference</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#blog"><span>Blog</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#misc"><span>MISC</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#experience"><span>Experience</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>確率・情報理論を使った言語研究</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jan 1, 2025
  </span>
  

  

  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    options: {
      processHtmlClass: "mathjax-process"
    }
  };
</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
<p>Chomskyを読んで言語学が面白いと思った人の中には、確率モデル・情報理論は「工学とかの実用性を追う人には大事なのかもしれないが、言語の本質を捉えるのに寄与しないもの」という認識の人も少なくないと思います（めちゃくちゃ急な決めつけですが、少なくとも私はそうだった）。</p>
<p>だって、たとえば、<a href="https://www.degruyter.com/document/doi/10.1515/9783112316009/html" target="_blank" rel="noopener">Chomsky (1957)</a>には、「」と、
<a href="https://www.jstor.org/stable/j.ctt17kk81z" target="_blank" rel="noopener">Chomsky (1965)</a>には、「」と書いてあるし。</p>
<p>しかし、心理言語学とか、計算言語学、計算心理言語学、認知科学とか言われる分野の研究をみていると（これらの分野がそれぞれどういった範囲を指しているのか正直よくわからない）、当然のように確率の概念や、情報理論が大活躍しています。
特に最近の研究だと、確率モデルや情報理論は「当然使わないとおかしいよね！？」という雰囲気があるくらいみんな使っています（私の主観。だって言語学論文最近読んでいないし）。
昨年、2024年夏にオランダのロッテルダムであったCogSci（認知科学の国際会議）に参加したら、これが私にとって初めての海外での国際会議だったわけですが、何とまあ端から端まで情報理論かベイジアンモデリングで、否応なく時代の波を感じたわけです（初めてではありましたが）。
まあ実際そういった研究をちゃんと聞いたり読んだりすると面白く、確率モデルや情報理論は人間言語を理解するのに非常に有用であるように思われるので、そういった研究についてうだうだ語りたいのです。</p>
<p>確率を取り入れた言語研究は結構有用じゃないでしょうか、という話は、
<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318" target="_blank" rel="noopener">Chater and Manning (2006, Trends in Cognitive Sciences)</a>の解説がおすすめです。</p>
<p>と、適当な前置きを書きましたが、このブログは、単に私が「面白い！自分もやってみたい！」と思った研究について、日本語で適当な研究紹介をネットの海に放流しておけば、日本で同じあたりについて「面白い！」と思う人が増えるのではないか、そういった布教活動に自分も貢献できるのではないか、一応もう博士課程だし！？と思った次第です。
そして、これまでのブログについて、「読んだよ！」といってもらったとき、恥ずかしさがありつつも、承認欲求が満たされるのを感じ、もっと承認欲求が満たされる可能性を増やしておこうと思った次第でもあります。</p>
<ul>
<li><a href="#%e6%96%87%e5%87%a6%e7%90%86%e7%b3%bb">文処理系</a>
<ul>
<li><a href="#surprisal">Surprisal</a></li>
<li><a href="#lossy-context-surprisal--memory--surprisal-trade-off">Lossy-context surprisal/Memory&ndash;surprisal trade-off</a></li>
<li><a href="#rational-speech-act-rsa">Rational Speech Act</a></li>
<li><a href="#noisy-channel-model">Noisy-channel model</a></li>
</ul>
</li>
<li><a href="#%e5%8a%b9%e7%8e%87%e7%9a%84%e3%81%aa%e3%82%b3%e3%83%9f%e3%83%a5%e3%83%8b%e3%82%b1%e3%83%bc%e3%82%b7%e3%83%a7%e3%83%b3-efficient-communication">効率的なコミュニケーション</a>
<ul>
<li><a href="#%e3%82%b3%e3%83%9f%e3%83%a5%e3%83%8b%e3%82%b1%e3%83%bc%e3%82%b7%e3%83%a7%e3%83%b3%e3%81%ae%e5%8a%b9%e7%8e%87%e6%80%a7">コミュニケーションの効率性</a>
<ul>
<li><a href="#ferrer-i-cancho-and-sole-2003-pnas">Ferrer i Cancho and Sole (2003)</a></li>
</ul>
</li>
<li><a href="#%e5%8f%8d%e4%ba%8b%e5%ae%9f%e7%9a%84-counterfactual-%e8%a8%80%e8%aa%9e%e3%81%ae%e8%a8%ad%e8%a8%88">反事実的 (counterfactual) 言語の設計</a></li>
<li><a href="#information-bottleneck">Information Bottleneck</a></li>
<li><a href="#uniform-information-density">Uniform Information Density</a></li>
<li><a href="#dependency-length-minimization">Dependency Length Minimization</a></li>
</ul>
</li>
<li><a href="#%e8%a8%80%e8%aa%9e%e7%8d%b2%e5%be%97">言語獲得</a>
<ul>
<li><a href="#xu-and-tenenbaum-2007-psycho-rev">Xu and Tenenbaum (2007)</a></li>
<li><a href="#perfors-et-al-2011-cognition">Perfors et al. (2011)</a></li>
<li><a href="#abend-et-al-2017-cognition">Abend et al. (2017)</a></li>
</ul>
</li>
<li><a href="#%e8%a8%80%e8%aa%9e%e9%80%b2%e5%8c%96">言語進化</a>
<ul>
<li><a href="#iterated-learning-model">繰り返し学習モデル</a></li>
</ul>
</li>
</ul>
<h1 id="文処理系">文処理系</h1>
<p>我々の産出・理解といった言語使用において、確率モデルや情報理論の道具を使うことで割と適切にモデル化できることがわかっている側面が多々あります。</p>
<h2 id="surprisal">Surprisal</h2>
<p>ある単語 $w$ の生起確率 $P(w)$ の 負の対数 $-\log P(w)$ のことをサプライザルと呼びます。
<a href="https://aclanthology.org/N01-1021" target="_blank" rel="noopener">Hale (2001, NAACL)</a> は、
サプライザル理論 (<a href="https://aclanthology.org/N01-1021" target="_blank" rel="noopener">Hale, 2001, NAACL</a>; <a href="https://www.sciencedirect.com/science/article/pii/S0010027707001436" target="_blank" rel="noopener">Levy, 2008, Cognition</a>) では、ある要素の予測のしにくさ（サプライザル）はその要素の処理に難しさに比例する、と予測します：
\begin{equation}
\text{difficulty}\propto -\log P(w|C)
\end{equation}</p>
<p>サプライザルの値は、$P(x)$の値が小さいほど大きい、という関係になり、まさに予測のしにくさに応じて処理の困難さが上がる、という関係を描けている式です。</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="" srcset="
               /github-pages/blog/250101information/surprisal_hu3673861848599141302.webp 400w,
               /github-pages/blog/250101information/surprisal_hu8183784539060068456.webp 760w,
               /github-pages/blog/250101information/surprisal_hu9282218982883452785.webp 1200w"
               src="/github-pages/blog/250101information/surprisal_hu3673861848599141302.webp"
               width="376"
               height="394"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>そして、実際にこの比例関係が読み時間で成立することが示されています (<a href="">Smith and Levy, 2012, Cognition</a>; <a href="">Shain et al., 2024, PNAS</a>)。</p>
<p>では、サプライザルがとらえているものは何なのか？
<a href="https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12196" target="_blank" rel="noopener">Hale (2016)</a> は、各単語時点での選択肢の数。
<a href="https://www.sciencedirect.com/science/article/pii/S0010027707001436" target="_blank" rel="noopener">Levy, (2008, Cognition)</a> は、それまでの木構造分布と、その単語を受けたときの木構造分布の間の距離（KL Divergence）としました。</p>
<h2 id="lossy-context-surprisal">lossy-context surprisal</h2>
<p>\begin{equation}
\text{difficulty}\propto -\log P(w|m)
\end{equation}</p>
<h2 id="noisy-channel-model">noisy-channel model</h2>
<p>これまでのものは、例えば入力が正確になっていると
good-enough modelみたい (Ferreira and Ferreira (2024))</p>
<p>\begin{equation}
P(s_i\mid s_p)=\frac{P(s_p\mid s_i)P(s_i)}{\sum_{s_j\in\mathcal{S}}P(s_p\mid s_j)P(s_j)}\propto P(s_p\mid s_i)P(s_i)
\end{equation}</p>
<h1 id="効率的なコミュニケーション-efficient-communication">効率的なコミュニケーション (Efficient Communication)</h1>
<p>言語には、普遍性 (universalities) や 強い統計的傾向 (strong statistical tendencies) が多々ありますが、こうした性質は<strong>なぜ</strong>あるのでしょうか。</p>
<p><strong>効率的なコミュニケーション仮説</strong> (Efficient Communication Hypothesis) では、言語構造が効率的なコミュニケーションを実現するように形作られてきた、と考えます。
近年、この「効率性」を厳密に定義し、心理的に「妥当」に定量化する方法論が確立されてきたことで、研究が進みつつあります。</p>
<p>おすすめreview論文：</p>
<ul>
<li><a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.126" target="_blank" rel="noopener">Jaeger and Tily (2011, WIREs Cognitive Science)</a></li>
<li><a href="https://doi.org/10.1146/annurev-linguistics-011817-045406" target="_blank" rel="noopener">Kemp et al. (2018, Annual Review of Linguistics)</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S1364661319300580" target="_blank" rel="noopener">Gibson et al. (2019, Trends in Cognitive Sciences)</a></li>
<li><a href="https://www.frontiersin.org/articles/10.3389/fcomm.2022.657725/full" target="_blank" rel="noopener">Futrell and Hahn (2022, Frontiers in Communication)</a></li>
</ul>
<p>本：</p>
<ul>
<li><a href="https://www.cambridge.org/core/books/communicative-efficiency/F5AA238FB82B9739592CFAC62BF89708" target="_blank" rel="noopener">Levshina (2022)</a></li>
</ul>
<h2 id="コミュニケーションの効率性">コミュニケーションの効率性</h2>
<p>コミュニケーションが効率的であるとは、人間の認知能力の制約のもと、「意図・情報の伝達が最大化されている一方、産出や理解といった使用のコストが最小化されている状況」のことを指す、という表現を私は使います。
つまり、できる限り informative でありかつできる限り simple である状況が効率的であります。</p>
<p>この情報伝達性 (<em>informativeness</em>) と単純性 (<em>simplicity</em>) にはトレードオフ関係があり、つまり、どちらか一方だけを高めるともう一方は低くなってしまう、という関係であり、自然言語は、このトレードオフのもとで（ほとんど）最適解である側面が数々観察されています。</p>
<p>* <a href="">『自然言語処理』の学会記事 (to appear)</a> でも書いたのですが、<em>simplicity</em> と <em>informativeness</em> という用語は専門用語として固まってきてしまっているようなのですが、その意味するところが伝わりにくい表現であります。
私自身は <em>informative</em> という単語の語感がわからないので、違和感を覚えないままこの用語を使うようになってしまったのですが、これまで多くの人に <em>informativeness</em> の意味するところは何なのか？この用語自体が informative ではないのではないか？と聞かれます。
確かにそうかもしれません。
<a href="https://www.sciencedirect.com/science/article/pii/S0010027711002496" target="_blank" rel="noopener">Piantadosi et al. (2012, Cognition)</a> で使われていた、<em>clarity</em> と <em>ease</em> の方がいいですよね。</p>
<p>語彙に関して：</p>
<ul>
<li>親族名称: <a href="https://www.science.org/doi/abs/10.1126/science.1218811" target="_blank" rel="noopener">Kemp and Regier (2012, Science)</a></li>
<li>色: <a href="https://www.pnas.org/doi/abs/10.1073/pnas.0610341104" target="_blank" rel="noopener">Regier et al. (2007, PNAS)</a>, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118346136.ch11" target="_blank" rel="noopener">Regier et al. (2015, The Handbook of Language Emergence)</a>, <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1619666114" target="_blank" rel="noopener">Gibson et al. (2017, PNAS)</a>, <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1800521115" target="_blank" rel="noopener">Zaslavsky et al. (2018, PNAS)</a></li>
<li>数: <a href="https://doi.org/10.1162/opmi%5c_a%5c_00034" target="_blank" rel="noopener">Xu et al. (2020, Open Mind)</a>, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13424" target="_blank" rel="noopener">Denic and Szymanik (2024, Cognitive Science)</a></li>
<li>文法標識 (number, tense, evidentiality): <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2025993118" target="_blank" rel="noopener">Mollica et al. (2021, PNAS)</a></li>
<li>量化子: <a href="https://www.mdpi.com/1099-4300/23/10/1335" target="_blank" rel="noopener">Steinert-Threlkeld (2021, Entropy)</a></li>
<li>人称代名詞: <a href="https://escholarship.org/uc/item/2sj4t8m3" target="_blank" rel="noopener">Zaslavsky et al. (2021, Proc. CogSci)</a></li>
<li>不定代名詞: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13142" target="_blank" rel="noopener">Denic et al. (2022, Cognitive Science)</a></li>
<li>Boolean connectives: <a href="https://doi.org/10.1162/ling%5c_a%5c_00461" target="_blank" rel="noopener">Uegaki (2022, Linguistic Inquiry)</a></li>
<li>Spatial demonstratives: <a href="https://www.sciencedirect.com/science/article/pii/S0010027723001397" target="_blank" rel="noopener">Chen et al. (2023, Cognition)</a></li>
<li>単語の長さ: <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1012551108" target="_blank" rel="noopener">Piantadosi et al. (2011, PNAS)</a>, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12689" target="_blank" rel="noopener">Mahowald et al. (2018, Cognitive Science)</a>, <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2406971121" target="_blank" rel="noopener">Xu et al. (2024, PNAS)</a></li>
<li>Zipf則: <a href="https://www.pnas.org/doi/abs/10.1073/pnas.0335980100" target="_blank" rel="noopener">Ferrer i Cancho and Sole (2003, PNAS)</a>, <a href="https://doi.org/10.1140/epjb/e2005-00340-y" target="_blank" rel="noopener">Ferrer i Cancho (2005, The European Physical Journal B)</a></li>
<li>Zipf&rsquo;s meaning-frequency law: <a href="https://www.sciencedirect.com/science/article/pii/S0010027711002496" target="_blank" rel="noopener">Piantadosi et al. (2012, Cognition)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0010027722000828" target="_blank" rel="noopener">Trott and Bergen (2022, Cognition)</a></li>
<li>&hellip;</li>
</ul>
<p>文法に関して：</p>
<ul>
<li>Greenbergの語順普遍: <a href="">Hahn et al. (2020, PNAS)</a></li>
<li>等位接続における構造依存性: <a href="">Kajikawa et al. (2024, CoNLL)</a></li>
</ul>
<h3 id="ferrer-i-cancho-and-sole-2003-pnashttpswwwpnasorgdoiabs101073pnas0335980100"><a href="https://www.pnas.org/doi/abs/10.1073/pnas.0335980100" target="_blank" rel="noopener">Ferrer i Cancho and Sole (2003, PNAS)</a></h3>
<p>Zipf則 (Zipf&rsquo;s law) (<a href="https://www.routledge.com/The-Psycho-Biology-Of-Language-AN-INTRODUCTION-TO-DYNAMIC-PHILOLOGY/ZipfGeorgeKingsley/p/book/9781138875098?srsltid=AfmBOopeIeJMNPikd6dTWUgyGseCgJeRgqiKdyvD5JXpZ5Qxcuzrgx3m" target="_blank" rel="noopener">Zipf, 1936</a>; <a href="https://psycnet.apa.org/record/1950-00412-000" target="_blank" rel="noopener">1949</a>)とは、単語頻度に関する経験則で、全体で$k$番目に多く使用される単語の頻度 $f(k)$ は、$f(k)=C\cdot k^{-\alpha}$ と冪乗則に従う、というものです。
$C$ は比例定数、$\alpha$ は <a href="https://psycnet.apa.org/record/1950-00412-000" target="_blank" rel="noopener">Zipf (1949)</a> では $1$ です。
両辺に対数を適用すると、$\log f(k) = -\alpha\log k + \log C$ と線形な関係になります。</p>
<p>が、式で見てもわかりにくいので、実際の様子を示します。
実際の頻度と頻度ランクの関係を見たら、式の意味もわかりやすいかと思います。
ちょうど手元に<a href="https://github.com/UniversalDependencies/UD_Japanese-BCCWJ" target="_blank" rel="noopener">UD_Japanese-BCCWJ</a>のv2.10、国語研長単位 (LUW) 分割があったので、これの単語頻度と頻度ランクを数えてみます。
全部で57,109文、995,632単語（長単位）で、<em>x</em>軸を頻度ランク、<em>y</em>軸を実際の頻度としてプロットしたのが左図、そして両軸に $\log_{10}$ を適用してプロットしたものが右図です。</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blog/250101information/bccwj_zipf_hu6585653125791012963.webp 400w,
               /github-pages/blog/250101information/bccwj_zipf_hu9883852135442870022.webp 760w,
               /github-pages/blog/250101information/bccwj_zipf_hu10125593117757427291.webp 1200w"
               src="/github-pages/blog/250101information/bccwj_zipf_hu6585653125791012963.webp"
               width="760"
               height="378"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>右図については、最小二乗法での回帰直線も引いてみました。
$\alpha = 1$ とはなっていないようですが、ある程度はZipf則に従っているようにみえます。</p>
<p>* これを「従っている」とみるかは結構重要な問題な気はします。
詳しくは <a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" target="_blank" rel="noopener">Piantadosi (2014, Psychon Bull Rev)</a> をお読みください。</p>
<p>では、（一旦単語分布はZipf則に従っているとして）なぜ単語分布はZipf則に従うのでしょうか。
<a href="https://psycnet.apa.org/record/1950-00412-000" target="_blank" rel="noopener">Zipf (1949)</a> は、言語は、省エネでありたいという話し手の要求と、意図・情報の復元が容易でありたいという聞き手の要求のトレードオフのもとで形作られているのだと説明しました。
具体例として、言語を「単語」と「意味」の対応関係と考えてみましょう。
すべての意味をたった一つの単語で表現する言語があったら、話し手は覚えるべき単語が一つだけになるため、話者の負担は最小限になります。
しかし、このような言語では、単語がどの意味を指すのか曖昧になり、聞き手が話者の意図を理解するのに困難が生じます。
一方で、すべての単語が明確に異なる意味を表現するシステムでは、聞き手の理解（意図推定）は容易になりますが、話し手の負担は増加します。
Zipf は、言語はこうした話し手と聞き手の相反する要求のバランスによって形作られていると主張しました。</p>
<p>では、それは本当か？と計算機上で実験してみたのが、<a href="https://www.pnas.org/doi/abs/10.1073/pnas.0335980100" target="_blank" rel="noopener">Ferrer i Cancho and Sole (2003, PNAS)</a>です。</p>
<h4 id="モデル">モデル</h4>
<p>Ferrer i Cancho と Sole は、上述した Zipf の説明を検証するため、信号（単語）と意味の対応関係について、話し手と聞き手双方のコストがトレードオフのもとで最小となるような関係になるまで進化アルゴリズムを使って推定する、ということを行いました。</p>
<p>$n$ 個のシグナル $\mathcal{S} = {s_1,&hellip;,s_i,&hellip;,s_n}$ と
$m$ 個の意味（objects of reference）$\mathcal{R} = {r_1,&hellip;,r_i,&hellip;,r_m}$ があるシステムについて、<br>
それらのインターラクションをバイナリ行列 $\mathbf{A} = {a_{ij}}\ (1\leq i\leq n,\ 1\leq j\leq m)$ で表す。</p>
<p>例えば、もし$a_{ij} = 1$ならば、$i$番目のシグナルが$j$番目のオブジェクトを指しているし、もし$a_{ij} = 0$ならば、$i$番目のシグナルが$j$番目のオブジェクトを指しないということ。</p>
<p>いま、synonymがあるなら、シグナルと意味の確率分布の関係は以下になる：</p>
<p>\begin{equation}
P(s_i) = \sum_j P(s_i,r_j).
\end{equation}</p>
<p>なお、$P(r_i) = 1/m$ と仮定する（<strong>仮定１</strong>）。</p>
<p>ベイズの定理より、
\begin{equation}
P(s_i,r_j) = P(r_j)P(s_i|r_j).
\end{equation}
であり、$P(s_i|r_j)$は、
\begin{equation}
P(s_i|r_j) = a_{ij}\frac{1}{\omega_j}.
\end{equation}
で定義する。
$\omega_i = \sum_j a_{ij}$は意味$j$における同義語の数である。</p>
<p>代入すると、
\begin{equation}
P(s_i,r_j) = a_{ij}\frac{P(r_j)}{\omega_j}.
\end{equation}</p>
<h4 id="再現コードと結果">再現コードと結果</h4>
<p>非常に単純なモデルなので、再現実装してみました（いくつか簡略化しています）。
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blog/250101information/freq_rank_hu1559019237969630005.webp 400w,
               /github-pages/blog/250101information/freq_rank_hu10479040455501117085.webp 760w,
               /github-pages/blog/250101information/freq_rank_hu526095839200649385.webp 1200w"
               src="/github-pages/blog/250101information/freq_rank_hu1559019237969630005.webp"
               width="691"
               height="760"
               loading="lazy" data-zoomable /></div>
  </div></figure>

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blog/250101information/log_freq_rank_hu2101583012652647821.webp 400w,
               /github-pages/blog/250101information/log_freq_rank_hu9408570637108809242.webp 760w,
               /github-pages/blog/250101information/log_freq_rank_hu4437273928768977922.webp 1200w"
               src="/github-pages/blog/250101information/log_freq_rank_hu2101583012652647821.webp"
               width="691"
               height="760"
               loading="lazy" data-zoomable /></div>
  </div></figure>

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blog/250101information/mi_lexicon_size_hu12214877015639816266.webp 400w,
               /github-pages/blog/250101information/mi_lexicon_size_hu16297562656130702712.webp 760w,
               /github-pages/blog/250101information/mi_lexicon_size_hu9567190417810425306.webp 1200w"
               src="/github-pages/blog/250101information/mi_lexicon_size_hu12214877015639816266.webp"
               width="760"
               height="456"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>もちろん、
<a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" target="_blank" rel="noopener">Piantadosi (2014, Psychon Bull Rev)</a>
と
<a href="https://www.utp.or.jp/book/b559376.html" target="_blank" rel="noopener">田中 (2021)</a>
は必読です。</p>
<h3 id="反事実的-counterfactual-言語の設計">反事実的 (counterfactual) 言語の設計</h3>
<h3 id="information-bottleneck">Information Bottleneck</h3>
<h2 id="uniform-information-density">Uniform Information Density</h2>
<ul>
<li>operationalizationが複数あるが、どれが最も良いのか不明。</li>
<li>&ldquo;Uniformity&quot;の定義があやふや</li>
</ul>
<h2 id="rational-speech-act-rsa">Rational Speech Act (RSA)</h2>
<p>私たちは、何らかの意図を伝達したいとき、それをふまえて何らかの語や文を発話するわけですが、このとき、どの語・文を選ぶかをモデル化したものが、Rational Speech Act (RSA) モデルです。</p>
<p>accessibility (cost) とinformativenessのトレードオフ</p>
<p>より詳しい解説は、まさにこのRSAモデルでの代名詞選択に関する研究をしていた折田先生の解説記事があります。</p>
<h2 id="dependency-length-minimization">Dependency Length Minimization</h2>
<h2 id="memory--surprisal-trade-off">Memory&ndash;surprisal trade-off</h2>
<h1 id="言語獲得">言語獲得</h1>
<p>確率モデル、というかベイズの定理を利用した言語学・認知科学研究で忘れてはならないのが、言語獲得のモデル化です（ベイジアンモデリングの解説については、<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661310001129" target="_blank" rel="noopener">Griffiths et al. (2010, Trends in Cognitive Sciences)</a>や<a href="https://oecs.mit.edu/pub/lwxmte1p/release/2" target="_blank" rel="noopener">Griffiths (2024, Open Encyclopedia of Cognitive Science)</a>へ）。</p>
<p>\begin{equation}
P(h_i\mid d)=\frac{P(d\mid h_i)P(h_i)}{\sum_{h_j\in\mathcal{H}}P(d\mid h_j)P(h_j)}\propto P(d\mid h_i)P(h_i)
\end{equation}</p>
<p>世の中にはベイズの定理に関する解説で満ち満ちているので適当に書きますが、簡単に説明すると、上式は、データ $d$ を受けたとき、仮説 $h_i\in\mathcal{H}$ をどれだけ支持するか、という確率 $P(h_i\mid d)$ は、その仮説 $h_i$ の事前確率 $P(h_i)$ と、その仮説 $h_i$ をもっていたときにデータ $d$ に出会う尤度（尤もらしさ）$P(d\mid h_i)$ の積でわかりますよ、ということです。</p>
<p>ベイズの定理の嬉しさは、概念上は、生得主義 vs. 経験主義といった二項対立を超えて、言語はどのくらい学習可能か、どのくらいの生得知識が必要か、ということを定量的に評価することができるはず、というところでしょう（と、私は思っています）。
これについて、<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318" target="_blank" rel="noopener">Chater and Manning (2006)</a> で、何だかわかるような気がする説明があるので、引用して紹介します。</p>
<p>以下、<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318" target="_blank" rel="noopener">Chater and Manning (2006)</a> の p.342より：</p>
<blockquote>
<p>Oversimplifying somewhat, suppose that a learner wonders whether to include constraint $C$ in her grammar. $C$ happens, perhaps coincidentally, to fit all the data so far encountered. If the learner does not assume $C$, the probability that each sentence will happen to fit $C$ by chance is $p$. Thus, each sentence obeying $C$ is $1/p$ times more probable, if the constraint is true than if it is not (if we simply rescale the probability of all sentences obeying the constraint). Thus, after $n$ sentences, the probability of the corpus, is $1/p^n$ greater, if the constraint is included. Yet, a more complex grammar will typically have a lower prior probability. If the ratio of priors for grammars with/without the constraint is greater than $1/p^n$, then, by Bayes&rsquo; theorem, the constraint is unlearnable in $n$ items.</p>
</blockquote>
<p>ようは、ある制約 $C$ が生得知識としてあるべきなのか、それとも学習可能なのかは、制約 $C$ がある文法（仮に $G_C$）と制約 $C$ がない文法（仮に $G_{\neg C}$）の事前確率の比 $\frac{P(G_C)}{P(G_{\neg C})}$ と尤度比 $\frac{P(d\mid G_C)}{P(d\mid G_{\neg C})}$ を比べることで、事後分布の比 $\frac{P(G_C\mid d)}{P(G_{\neg C}\mid d)}$ を比べるのと同じこととなり、制約 $C$ が学習可能なのかを判定することができる、ということです。</p>
<p>仮に、$G_{\neg C}$ を想定している人が、偶然にも $G_{C}$ に整合的な文 $s_C$ に出会う確率を $p$ とすると、$P(s_C\mid G_{\neg C}) = p$ です。
$P(s_C\mid G_{C}) = 1$ であるとすると、ある人が $G_{C}$ に整合的な文に $n$ 回出会ったとき、それぞれの文法を想定したときの尤度比は、$\frac{P(s_C\mid G_{C})^n}{P(s_C\mid G_{\neg C})^n} = \frac{1}{p^n}$ です。
このとき、それぞれの文法の事前分布 $P(G_C)$ と $P(G_{\neg C})$ の比が $\frac{1}{p^n}$ 分離れているのかどうかで、制約 $C$ が学習可能なのかどうか決まります。
基本的には、制約が一つ多い文法の方が複雑で事前分布がより小さくなっているはずだが、その小さい事前分布をもってもなお尤度により挽回可能なのか、というところである。</p>
<h3 id="xu-and-tenenbaum-2007-psycho-rev">Xu and Tenenbaum (2007, Psycho Rev)</h3>
<h3 id="perfors-et-al-2011-cognition">Perfors et al. (2011, Cognition)</h3>
<h3 id="abend-et-al-2017-cognition">Abend et al. (2017, Cognition)</h3>
<h3 id="yang-and-piantadosi-2022-pnas">Yang and Piantadosi (2022, PNAS)</h3>
<p>個人的に、<a href="">『自然言語処理』への解説記事</a>でも書いたのですが、学習可能性について考えたいのですが、現在のACL系列の論文で大人気の「とりあえずニューラルモデルでの学習速度を見ました」という研究はさすがにコネクショニストすぎて私自身は乗っかれないと思っているものの、そうはいっても学習可能性については語りたいし、そうしたらニューラルを使うのが「出版において安全」な気がするし…という状況の中で、何とかベイジアンモデリングが使えないかと模索していました。
が、<a href="">Yang (2002)</a>の頃から指摘されているように、</p>
<h1 id="言語進化">言語進化</h1>
<h2 id="iterated-learning-model">Iterated Learning Model</h2>
<ul>
<li>文化進化による構成性の出現
<ul>
<li><a href="https://link.springer.com/article/10.3758/s13423-016-1166-7" target="_blank" rel="noopener">Kirby (2017, Psychon Bull Rev)</a></li>
</ul>
</li>
</ul>

    </div>

    







<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblog%2F250101information%2F&amp;text=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblog%2F250101information%2F&amp;t=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblog%2F250101information%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblog%2F250101information%2F&amp;title=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6%20http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblog%2F250101information%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblog%2F250101information%2F&amp;title=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="http://localhost:1313/github-pages/"><img class="avatar mr-3 avatar-square" src="/github-pages/authors/admin/avatar_hu18040700168886216705.jpeg" alt="Kohei Kajikawa"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="http://localhost:1313/github-pages/">Kohei Kajikawa</a></h5>
      <h6 class="card-subtitle">Master&rsquo;s student at the University of Tokyo (UTokyo).</h6>
      <p class="card-text">My research interests include computational linguistics and computational psycholinguistics.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/github-pages/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/qjqw1223" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/kohei-kaji" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Kohei-Kajikawa" target="_blank" rel="noopener">
        <i class="fab fa-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/github-pages/uploads/CV.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  <p class="powered-by copyright-license-text">
    © 2025 Kohei Kajikawa.
  </p>
  





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/github-pages/js/vendor-bundle.min.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/github-pages/js/wowchemy-headroom.js" type="module"></script>









  
  


<script src="/github-pages/en/js/wowchemy.min.js"></script>



  <script src="/github-pages/js/wowchemy-map.js" type="module"></script>






















</body>
</html>
