<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: March 3, 2025 --><html lang="en-us" >


<head><script src="/github-pages/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=github-pages/livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/github-pages/css/vendor-bundle.min.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/github-pages/css/wowchemy.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/github-pages/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/github-pages/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Kohei Kajikawa" />





  

<meta name="description" content="確率モデル・情報理論を使った心理言語学・計算心理言語学研究について" />



<link rel="alternate" hreflang="en-us" href="http://localhost:1313/github-pages/blogs/250101information/" />
<link rel="canonical" href="http://localhost:1313/github-pages/blogs/250101information/" />



  <link rel="manifest" href="/github-pages/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/github-pages/media/icon_hu10978985619530458252.png" />
<link rel="apple-touch-icon" type="image/png" href="/github-pages/media/icon_hu10391704213710329260.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@GetResearchDev" />
  <meta property="twitter:creator" content="@GetResearchDev" />
<meta property="twitter:image" content="http://localhost:1313/github-pages/media/icon_hu10226378635250344412.png" />



  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Kohei Kajikawa" />
<meta property="og:url" content="http://localhost:1313/github-pages/blogs/250101information/" />
<meta property="og:title" content="確率・情報理論を使った言語研究 | Kohei Kajikawa" />
<meta property="og:description" content="確率モデル・情報理論を使った心理言語学・計算心理言語学研究について" /><meta property="og:image" content="http://localhost:1313/github-pages/media/icon_hu10226378635250344412.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2025-03-03T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2025-03-03T00:00:00&#43;00:00">
  







  




  
  
  

  
  

  


  
  <title>確率・情報理論を使った言語研究 | Kohei Kajikawa</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="811475aff5a35f3f96964efeed0a82de" >

  
  
  
  
  
  
  
  
  
  <script src="/github-pages/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/github-pages/">Kohei Kajikawa</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/github-pages/">Kohei Kajikawa</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#international_conference"><span>International Conference</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#journal"><span>Journal</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#domestic_conference"><span>Domestic Conference</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#blogs"><span>Blogs</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#misc"><span>MISC</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#experience"><span>Experience</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/github-pages/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>確率・情報理論を使った言語研究</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Mar 3, 2025
  </span>
  

  

  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    options: {
      processHtmlClass: "mathjax-process"
    }
  };
</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
<p>確率や情報理論といった数学の道具は、言語使用や言語の構造それ自体を支配している何らかの規則を記述するのに非常に有用であるように思えます。</p>
<p>それは、確率や情報理論といったものは現象に対して理論中立的 (theory-neutral) な記述が可能で、とにかく使い勝手が良い道具だからなのかもしれないし、そもそも人間の言語処理をはじめとした認知活動が何らかの確率的なものだからなのかもしれないし、認知活動自体が、ベイジアンが主張するような「合理的 (rational)」なものだと考えたら合理的でありそう、という直感や経験的事実があるからなのかもしれないし&hellip;。</p>
<p>いずれにせよ、確率や情報理論は、言語処理・言語使用の研究（つまり、Performance の側面の研究）、そしてさらには言語の構造や知識自体の研究（つまり、Competence の側面）をするのに非常に強力な道具です。</p>
<p>近年は、コーパスをはじめとした言語資源の整備、計算機の性能向上やプログラミング言語・各種計算ライブラリの充実、大規模言語モデルのような汎用的な言語処理技術たちの登場により、かなりの精度で言語の情報量が推定できるようになってきたこと、といったさまざまな要因たちが揃ってきて、そしてさらには、確率や情報理論といった道具自体の理論研究や言語研究へ応用するといった方法論が確立されてきたので、これからやれることが大量にあるよ、という状況です。</p>
<p>実際、心理言語学とか、計算言語学、計算心理言語学、認知科学とか言われる分野の研究をみていると（これらの分野がそれぞれどういった範囲を指しているのか正直よくわからないが）、確率や情報理論であふれています。
昨年、2024年夏にオランダのロッテルダムであった CogSci（認知科学の国際会議）に参加したら、これが私にとって初めての海外での国際会議だったわけですが、何とまあ端から端まで情報理論かベイジアンモデリングで割と衝撃を受けました。
そういった状況が良いか置いておいて、とりあえず勉強しなきゃなぁと思わされたし、この波に乗っておきたい、と思えました。ので、その紹介記事です。</p>
<p>このあたりのレビュー論文、本として、個人的参照すべきものたち：</p>
<ul>
<li>確率を取り入れた言語研究は結構有用じゃないでしょうか、という話：
<ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318" target="_blank" rel="noopener">Chater and Manning (2006)</a></li>
</ul>
</li>
<li>言語を含めた認知科学における確率モデル的（合理的）アプローチについて：
<ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306001343" target="_blank" rel="noopener">Tenenbaum et al. (2006)</a></li>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661310001129" target="_blank" rel="noopener">Griffiths et al. (2010)</a></li>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S001002771000291X" target="_blank" rel="noopener">Perfors et al. (2011)</a></li>
<li><a href="https://mitpress.mit.edu/9780262049412/bayesian-models-of-cognition/" target="_blank" rel="noopener">Griffiths et al. (2024)</a></li>
<li><a href="https://oecs.mit.edu/pub/lwxmte1p/release/2" target="_blank" rel="noopener">Griffiths (2024)</a></li>
</ul>
</li>
</ul>
<!-- 余談、言語学者のshannon underestimate問題 -->
<!-- 一方で、特に私のような、大学に文系で入学し、数理的な教育を集中して受けてこなかった人間にとって、確率や情報理論といったことばはそこまで馴染みがないものです。
そしてさらには、Chomskyを読んで言語学が面白いと思った人の中には、確率モデル・情報理論は「工学とかの実用性を追う人には大事なのかもしれないが、言語の本質を捉えるのに寄与しないもの」という認識の人も少なくないと思います（めちゃくちゃ急な決めつけですが、少なくとも私はそうだった）。
[Chomsky (1956)](https://ieeexplore.ieee.org/document/1056813) [Chomsky (1957)](https://www.degruyter.com/document/doi/10.1515/9783112316009/html)には、「」と、
では、Claude Shannonのマルコフモデルを散々叩いていて、

[Chomsky (1965)](https://www.jstor.org/stable/j.ctt17kk81z)には、「」と書いてあるし。 -->
<!-- 人間はそこまで合理的でない？という問題 -->
<h1 id="目次">目次</h1>
<ul>
<li><a href="#%e6%96%87%e5%87%a6%e7%90%86%e7%b3%bb">文処理系</a>
<ul>
<li><a href="#surprisal">Surprisal</a></li>
<li><a href="#lossy-context-surprisal">Lossy-context surprisal</a></li>
<li><a href="#noisy-channel-model">Noisy-channel model</a></li>
</ul>
</li>
<li><a href="#%e5%8a%b9%e7%8e%87%e7%9a%84%e3%81%aa%e3%82%b3%e3%83%9f%e3%83%a5%e3%83%8b%e3%82%b1%e3%83%bc%e3%82%b7%e3%83%a7%e3%83%b3-efficient-communication">効率的なコミュニケーション</a>
<ul>
<li><a href="#%e5%8d%98%e7%b4%94%e6%80%a7%e3%81%a8%e6%83%85%e5%a0%b1%e4%bc%9d%e9%81%94%e6%80%a7%e3%81%ae%e3%83%88%e3%83%ac%e3%83%bc%e3%83%89%e3%82%aa%e3%83%95">単純性と情報伝達性のトレードオフ</a>
<ul>
<li><a href="#ferrer-i-cancho-and-sole-2003-pnas">Ferrer i Cancho and Sole (2003)</a></li>
</ul>
</li>
<li><a href="#rational-speech-act-rsa">Rational Speech Act (RSA)</a></li>
<li><a href="#uniform-information-density-uid">Uniform Information Density (UID)</a></li>
<li><a href="#dependency-length-minimization-dlm">Dependency Length Minimization (DLM)</a></li>
<li><a href="#memory--prediction-trade-off">Memory&ndash;prediction trade-off</a></li>
</ul>
</li>
<li><a href="#%e8%a8%80%e8%aa%9e%e7%8d%b2%e5%be%97">言語獲得</a>
<ul>
<li><a href="#xu-and-tenenbaum-2007-psycho-rev">Xu and Tenenbaum (2007)</a></li>
<li><a href="#perfors-et-al-2011-cognition">Perfors et al. (2011)</a></li>
<li><a href="#abend-et-al-2017-cognition">Abend et al. (2017)</a></li>
</ul>
</li>
<li><a href="#%e8%a8%80%e8%aa%9e%e9%80%b2%e5%8c%96">言語進化</a>
<ul>
<li><a href="#%e7%b9%b0%e3%82%8a%e8%bf%94%e3%81%97%e5%ad%a6%e7%bf%92%e3%83%a2%e3%83%87%e3%83%ab-iterated-learning-model">繰り返し学習モデル (Iterated Learning Model)</a></li>
</ul>
</li>
</ul>
<h1 id="文処理系">文処理系</h1>
<!-- 我々の産出・理解といった言語使用において、確率モデルや情報理論の道具を使うことで割と適切にモデル化できることがわかっている側面が多々あります。 -->
<h2 id="surprisal">Surprisal</h2>
<p>ある単語 $w$ の生起確率 $P(w)$ の 負の対数 $-\log P(w)$ のことをサプライザルと呼びます。
サプライザル理論 (<a href="https://aclanthology.org/N01-1021" target="_blank" rel="noopener">Hale, 2001</a>; <a href="https://www.sciencedirect.com/science/article/pii/S0010027707001436" target="_blank" rel="noopener">Levy, 2008b</a>) では、ある単語の予測のしにくさ（サプライザル）はその単語の処理の難しさに比例する、とします：
\begin{equation}
\text{difficulty}(w)\propto -\log P(w)
\end{equation}</p>
<!-- - ここで、$P(X|Y)$ は、$Y$ だったときの $X$ の条件付き確率を表します。 -->
<ul>
<li>確率値を表すときは大文字 $P$、確率分布を表すときは小文字 $p$ を使うようです。</li>
<li>対数の底はしばしば $2$ で bit単位ですが、底が一致している限りはサプライザル同士の相対関係は変わらないので、底の値自体をサプライザルの定義に組み込む必要は（たぶん）ないです。
<ul>
<li>$\log_2 x = \frac{\log_e x}{\log_e 2} = \frac{\log_e x}{0.30103&hellip;}$ と、底の変更は可能。</li>
</ul>
</li>
</ul>
<p>サプライザルの値 $-\log P(\cdot)$ は、生起確率 $P(\cdot)$ の値が小さければ小さいほどほど大きくなる、という関係になっています。</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blogs/250101information/surprisal_hu3673861848599141302.webp 400w,
               /github-pages/blogs/250101information/surprisal_hu8183784539060068456.webp 760w,
               /github-pages/blogs/250101information/surprisal_hu9282218982883452785.webp 1200w"
               src="/github-pages/blogs/250101information/surprisal_hu3673861848599141302.webp"
               width="376"
               height="394"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>そして、実際に、読み時間 (e.g., <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010027708001741" target="_blank" rel="noopener">Demberg and Keller, 2008</a>; <a href="https://www.sciencedirect.com/science/article/pii/S0010027713000413" target="_blank" rel="noopener">Smith and Levy, 2013</a>; <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00612/118718/Testing-the-Predictions-of-Surprisal-Theory-in-11" target="_blank" rel="noopener">Wilcox et al., 2023</a>; <a href="https://www.pnas.org/doi/10.1073/pnas.2307876121" target="_blank" rel="noopener">Shain et al., 2024</a>)、ERP（<a href="https://bsd.neuroinf.jp/wiki/%E4%BA%8B%E8%B1%A1%E9%96%A2%E9%80%A3%E9%9B%BB%E4%BD%8D" target="_blank" rel="noopener">事象関連電位</a>）(e.g., <a href="https://www.sciencedirect.com/science/article/pii/S0093934X14001515" target="_blank" rel="noopener">Frank et al., 2015</a>; <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0207741" target="_blank" rel="noopener">Brennan and Hale, 2019</a>) や fMRIによるBOLD信号 (e.g., <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177794" target="_blank" rel="noopener">Lopopolo et al., 2017</a>; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0028393219303495?via%3Dihub" target="_blank" rel="noopener">Shain et al., 2020</a>) についても、サプライザルがpredictorとして有効であることが示されています。</p>
<h3 id="サプライザルがとらえているものは何なのか">サプライザルがとらえているものは何なのか？</h3>
<p>では、単語 $w$ の生起確率 $P(w)$ は何で求めることができるのでしょうか。
逐次的な文処理過程について考える場合、単語 $w$ 以前の文脈 $w_1,\dots,w_{n-1}$ が与えられたときの単語 $w$ の生起確率 $P(w\mid w_1,\dots,w_{n-1})$ が求められれば良い、すなわち、言語モデルがあれば良い、ということになります。</p>
<p><a href="https://aclanthology.org/N01-1021" target="_blank" rel="noopener">Hale (2001)</a> では、言語モデルとして、確率的文脈自由文法（Probabilistic Context-Free Grammar, PCFG）によるものを採用しました。
PCFGは、文脈自由文法における各文法規則に生起確率を割り当てたものです。詳しくは、とりあえず最初に参照すべき文献、<a href="https://web.stanford.edu/~jurafsky/slp3/C.pdf" target="_blank" rel="noopener">Jurafsky and Martin (2025, Appendix C)</a> へ。</p>
<p>ある単語 $w$ の確率 $P(w)$ を、それまでの先行文脈 $w_1,\dots,w_{n-1}$ をもとに推定することを考えると、すなわち、$P(w_n\mid w_1,\dots,w_{n-1})$ を考えるとなると、この値は、chain ruleにより、
\begin{equation}
P(w_n\mid w_1,\dots,w_{n-1}) = \frac{P(w_1 \cdots w_{n-1}, w_n)}{P(w_1 \cdots w_{n-1})}
\end{equation}
と分解することができます。</p>
<p>PCFGは、文脈自由性により、ある文字列に対するすべての木構造は互いに排反なので、以下の式が成り立ちます：
\begin{equation}
P(w_1 \cdots w_n) = \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T, w_1 \cdots w_n)
\end{equation}</p>
<p>また、PCFGは木構造と文字列の生成モデルであり、木構造 $T$ に対して文字列 $w_1\cdots w_n$ が一意に定まるので、
\begin{equation}
P(w_1 \cdots w_n\mid T) = 1
\end{equation}
が成り立ちます。</p>
<p>よって、ある文字列 $w_1\cdots w_n$ の確率は、その文字列に対するすべての可能な木構造の確率の和に等しくなります：
\begin{align}
P(w_1 \cdots w_n) &amp;= \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T, w_1 \cdots w_n) \newline
&amp;= \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T)\cdot P(w_1 \cdots w_n\mid T) \newline
&amp;= \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T)
\end{align}</p>
<p>つまり、サプライザル $-\log P(w_n\mid w_1,\dots,w_{n-1})$ は、
\begin{align}
-\log P(w_n\mid w_1,\dots,w_{n-1}) &amp;= -\log\frac{P(w_1 \cdots w_{n-1}, w_n)}{P(w_1 \cdots w_{n-1})} \newline
<!-- &= - \log P(w_1 \cdots w_{n}) - \log P(w_1 \cdots w_{n-1}) \newline -->
&amp;= -\log\sum P(n\text{単語までの木構造}) \newline
&amp;\quad  - \log\sum P(n-1\text{単語までの木構造})
\end{align}
と、$n$ 単語時点での構造に関する情報量と $n-1$ 時点での構造に関する情報量の差、言い換えると、その単語により構造に関してどれだけ情報が得られたのか、を表す値として説明されます（解説として、<a href="https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12196" target="_blank" rel="noopener">Hale (2016)</a> も参照ください）。</p>
<p>Hale (2001) でPCFGが採用されたのは、$n$-gram 言語モデルでは（$n$ がとりわけ大きいわけではなければ）単語間の依存関係を正しく取り扱えないからであり、さらにはそもそも著者自身の興味が文法構造の処理にあったからだと思われるのですが、それにより、シンボリックな文法理論と処理負荷の予測をつなげられています。</p>
<p>これに対し、<a href="https://www.sciencedirect.com/science/article/pii/S0010027707001436" target="_blank" rel="noopener">Levy (2008b)</a> では、特定の構造の形に依存しない形でのサプライザルの解釈を提示しました。</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010027707001436" target="_blank" rel="noopener">Levy (2008b)</a> では、$n$ 時点での単語 $w_n$ のサプライザルを、　$n-1$ 時点までの意味 $T\in\mathcal{T}$ の確率分布から $n$ 時点での意味の確率分布へのKL距離（Kullback–Leibler divergence）、すなわち、単語 $w_n$ の入力により、意味に関する確率分布がどれだけ変わったのかという値として説明しました。
これは、先ほどの Hale (2001) でのサプライザルの導出過程と同じで、$P(w_1,\cdots,w_n\mid T) = 1$ 、という仮定を置いた場合に示すことができます：
\begin{align}
&amp;\text{KL}(p(T\mid w_1\cdots w_{n})\parallel p(T\mid w_1\cdots w_{n-1})) \newline
&amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{p(T\mid w_1\cdots w_n)}{p(T\mid w_1\cdots w_{n-1})} \newline
&amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{\frac{p(w_n\mid T, w_1\cdots w_{n-1})p(T\mid w_1\cdots w_{n-1})}{p(w_n\mid w_1\cdots w_{n-1})}}{p(T\mid w_1\cdots w_{n-1})} \newline
&amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{\frac{p(T\mid w_1\cdots w_{n-1})}{p(w_n\mid w_1\cdots w_{n-1})}}{p(T\mid w_1\cdots w_{n-1})} \newline
&amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{1}{p(w_n\mid w_1\cdots w_{n-1})} \newline
&amp;= \log\frac{1}{p(w_n\mid w_1\cdots w_{n-1})}
\end{align}</p>
<p>* 疑問：$P(w_{1,\cdots,n}\mid T)=1$ という仮定は問題ないのだろうか？真理条件的には同じ意味だが、語順・構造が違う文は作れるし、語彙も「同じ意味っぽい」形が異なる組み合わせはたくさんある。また、Hale (2001) では $T$ で周辺化された文字列に関する周辺分布を、Levy (2008b) では、$T$ に関する確率分布を用いてサプライザルを説明しているが、人間って、こういった分布を全部ちゃんと記憶して扱えるのだろうか？おそらく絶対に分布を端から端までは記憶しておけないが、分布を想定するという理想化により現実とどのくらい差異が生じうるのか？</p>
<h3 id="言語モデルの性能-perplexity-との関係">言語モデルの性能 (perplexity) との関係</h3>
<p>言語モデルの次単語予測性能を表す（単語ごとの）perplexityは、その値が低いほど「テストデータにおける次単語予測が正確である」ことを表すのですが、一時期は、「言語モデルのperplexityが低いほど、そのモデルによって算出したサプライザルの、読み時間の予測精度が高い」ということが主張されていました (<a href="https://aclanthology.org/W18-0102/" target="_blank" rel="noopener">Goodkind and Bicknell, 2018</a>)。
しかし、次第に、<a href="https://aclanthology.org/2023.tacl-1.20/" target="_blank" rel="noopener">Oh and Schuler (2023)</a> や <a href="https://www.pnas.org/doi/10.1073/pnas.2307876121" target="_blank" rel="noopener">Shain et al. (2024)</a> など、「言語モデルの規模が大きくなり、perplexityがさらに低くなっていくと、むしろ読み時間予測精度が下がる」という傾向が観察されているようです。
確かに、人間以上に次単語予測ができるモデルは、人間の反応時間をunderestimateしてしまうでしょう。</p>
<p>* 言語モデル $q$ のperplexityとは、
\begin{equation}
2^{-\frac{1}{|\text{test data}|}\sum_{\text{test data}}\log_2 q(x)}
\end{equation}
で、要は $2$ のテストデータにおける平均サプライザル（クロスエントロピー）乗です。</p>
<h3 id="サプライザルと処理負荷の関係は線形か非線形か">サプライザルと処理負荷の関係は線形か、非線形か</h3>
<p>hoge</p>
<h2 id="lossy-context-surprisal">Lossy-context surprisal</h2>
<p>前項でのサプライザルの定義では、それまでの文脈をフルに使って求められた当該単語の生起確率が用いられていました。
一方で、心理言語学の文処理研究では、昔から、人間は作業記憶に極めて制限があり、記憶にまつわる処理の負荷があることが知られていました。
この<strong>予測に関する処理負荷</strong>と<strong>記憶に関する処理負荷</strong>は、要は文処理に関する2大処理負荷で、ずっと別々に研究が進んでいたのですが、<a href="https://onlinelibrary.wiley.com/doi/10.1111/cogs.12814" target="_blank" rel="noopener">Futrell et al. (2020)</a> では、サプライザルの定義を少しだけ変更した lossy-context surprisal (LCS) を提案し、この2つを統合しました（これはほんとうにすごいと思っています）。
アイディアはものすごいシンプルで、文脈 $c = w_{1,\cdots, n-1}$ をフルに使用せず、文脈 $c$ をもとにした記憶表象 $m$ を用いて、単語 $w$ の生起確率を求める、というものです：
\begin{align}
\text{LCS}(w\mid c) &amp;:= \mathbb{E}_{m\sim p(m\mid c)} [-\log p(w\mid m)] \newline
&amp;= -\sum_m p(m\mid c)\log p(w\mid m)
\end{align}</p>
<p>この定式化は非常に一般的なものであり、例えば記憶表象 $m$ がどのような形なのか、どのようにして生成されるのか、といったことは特に指定されていません。
ということは、この形を明らかにする、という研究は求められるはずで、例えば 単純に文字列としての $c$ と $m$ の関係なのかもしれないし、意味（構造）として $m$ を扱った方が実際の人間がやっていることに近いのかもしれません。
記憶に関する処理については、これまで、<a href="https://www.sciencedirect.com/science/article/abs/pii/S0010027798000341" target="_blank" rel="noopener">Gibson (1998)</a> や <a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_25" target="_blank" rel="noopener">Lewis and Vasishth (2005)</a>、<a href="https://www.sciencedirect.com/science/article/pii/S0010027724000520" target="_blank" rel="noopener">Isono (2024)</a> など、何らかの文法理論を用いてモデル化がなされてきており、人間の記憶単位（チャンク）は文法構造を考えると良さそうかも、という状況です。
なので、これらを組み合わせることはできるし、そして人間の文処理のモデル化として有効でしょう、ということは期待できます。
一方で、「こうしたモデル化をしないと説明できないこと」みたいなものを私は思いついていないので、特にやるモチベーションはまだないです。
少なくとも、Hale (2001) や Levy (2008b) が考えていたような定式化において、分布全体を把握できているのは理想化のしすぎに思えるので、それが少し緩和される、というのは良いことかもしれません。</p>
<!-- ### resource-rational LCS
hoge -->
<h2 id="noisy-channel-model">Noisy-channel model</h2>
<p>人間の文処理研究において、多くの場合、人間は見聞きした文を正確に知覚できていることが前提とされていたりしますが、もちろん常にそんなことはなく、実際には聞き手は、話し手の発話を一部聞き逃したり、誤って聞き取っている場合もあります。
しかしながら、だからといって聞き逃して終わり、というわけではなく、聞き手は何やかんやうまく話し手の意図を推定できたりしています。
このような人間の処理過程を、<a href="https://aclanthology.org/D08-1025/" target="_blank" rel="noopener">Levy (2008a)</a> や <a href="https://www.pnas.org/doi/10.1073/pnas.1216438110" target="_blank" rel="noopener">Gibson et al. (2013)</a> は、noisy-channel model（雑音のある通信路モデル）を用いてモデル化しました。</p>
<p>ここでは簡単に Gibson et al. (2013) による表記を用いて紹介します。
\begin{equation}
s_i \overbrace{\longrightarrow}^{\text{noisy channel}} s_p
\end{equation}
話し手が意図した発話 $s_i$ が、聞き手が実際に知覚した発話 $s_p$ をもとに推定される確率 $P(s_i\mid s_p)$ は、
\begin{equation}
P(s_i\mid s_p)=\frac{P(s_p\mid s_i)P(s_i)}{\sum_{s_j\in\mathcal{S}}P(s_p\mid s_j)P(s_j)}\propto P(s_p\mid s_i)P(s_i)
\end{equation}
で求められます。</p>
<p>ようは、$P(s_p\mid s_i)$ という noise が入る確率と、そもそもありえる発話 $s_i$ の確率 $P(s_i)$ が分かれば、聞き手は話し手の意図を推定できる、ということです。
そして、人間はこの確率モデルにおいて最も確率が高い $s_i$ を合理的に推定する、ということが示されています。</p>
<p><a href="https://oecs.mit.edu/pub/y1uhdz0y/release/1" target="_blank" rel="noopener">Ferreira and Ferreira (2024)</a> では、noisy-channel modelがgood-enough modelに似ている、と説明していますが、むしろnoisy-channel modelはちゃんと予測がたつ理論、という意味ではより強力なものだとは思います。</p>
<p>個人的には、Competence $c$ とPerformance $p$ の間の関係も、まさにnoisy channelで考えることができる、と思っていています。
\begin{equation}
c \overbrace{\longrightarrow}^{P(c\mid p)} p
\end{equation}</p>
<!-- 我々は $p$ しか観測できないですが、$P(p\mid c)$ や $P(c)$ について考えることができます。 -->
<p>例えば、$P(c)$ はオッカムの剃刀的な、シンプルな理論ほど高い確率を付与する、というふうにして表現できますし、$P(p\mid c)$ は、competence $c$ によって performance $p$ をどれだけ説明できるのか、ということで、まさに理論言語学者や心理言語学者が、容認性判断や読み時間、神経活動データなどをもとに判断していることでしょう。概念的には、
\begin{equation}
P(p) = \sum_c P(p\mid c)P(c)
\end{equation}
の $P(p)$ と実際の観測との誤差が最小になるような $P(p\mid c)$ を探す、ということをしているはずです。</p>
<p>そして、$P(p\mid c)$ や $P(c)$ を定めることができれば、$P(c\mid p)$ が推測できます：
\begin{equation}
P(c\mid p)=\frac{P(p\mid c)P(c)}{\sum_{c\in\mathcal{C}}P(p\mid c)P(c)}\propto \underbrace{P(p\mid c)}_{\text{現象への説明}}\overbrace{P(c)}^{\text{文法のシンプルさ}}
\end{equation}</p>
<p>つまり、$P(c\mid p)$ は、competence自体の単純さとそれによる説明力の高さのトレードオフにより求められます。
そして、最も適切な理論 $\hat{c}$ は、事後確率が最大となるもの、とすることができます：
\begin{equation}
\hat{c}=\arg\max_{\hat{c}\in C} P(p\mid c)P(c)
\end{equation}</p>
<p>尤度と事前確率をそれぞれ「記述的妥当性」と「説明的妥当性」として読むと、これはまさに <a href="https://www.jstor.org/stable/j.ctt17kk81z" target="_blank" rel="noopener">Chomsky (1965)</a> による competence-performance の区別における議論でしょう。
記述的妥当性と説明的妥当性のトレードオフは、 <a href="http://mitwpl.mit.edu/catalog/mwpl77" target="_blank" rel="noopener">Berwick (2015)</a> が最小記述長 (minimum description length) を用いて同様の議論をしています。</p>
<p>ただ、このnoisy-channel model は非常に一般的なことを述べているに過ぎないので、Chomsky による competence-performance をより広く解釈できます。
50-60年代は容認性判断くらいしか $P(p\mid c)$ の議論をする場所がなかったので、Chomsky の議論は文法現象の「記述」がメインでしたが、心理言語学研究の進展により、$P(p\mid c)$ に対して、読み時間や脳活動データを使った検証が可能です。</p>
<p>また、<a href="https://www.sciencedirect.com/science/article/pii/S0010027715000815" target="_blank" rel="noopener">Kirby et al. (2015)</a> での文法の文化進化モデルも、尤度を「意図の伝達成功確率」と、事前確率を「文法の学習可能性」として扱っているもの、と見ることができると思います。</p>
<p>とはいっても、Chomsky の50-60年代の直感は、いまになって読み返してみても、（Shannon の研究を必要以上に叩いているようにみえるところ以外は）非常に有用なものが多いように思えます。
ちゃんと整理すると、$P(c),\ P(p\mid c),\ P(c\mid p)$ それぞれについて考えようとする、ということがまっとうな Chomskian だと思っているのですが、どうでしょうか。
そういう意味で、個人的には、formal な文法理論を用いた文処理の研究 ($P(p\mid c)$) と、$P(c\mid p)$ に直接取り組む効率的なコミュニケーション研究（後述）をやっています。学習可能性 ($P(c)$) に関する研究もしてみたいとは思っています。</p>
<p>* もしくは、$c$ と $p$ の間に noise が入りえない、プロの言語学者ならば noise なしで自身の $c$ にアクセスできる、と考えるのも、それが実現可能かはおいておいて、非常に合理的な考え方であるとは思います。そういう考え方に基づくと、$P(p\mid c)$ なんてものは考える必要はなくなります。</p>
<h1 id="効率的なコミュニケーション-efficient-communication">効率的なコミュニケーション (Efficient Communication)</h1>
<p>確率や情報理論により、言語使用や人間のコミュニケーションにおける cost/benefit を心理的に妥当な指標で評価できるようになってきました。
また、計算機上では、「現実には存在しないがありえたかもしれない仮想的な言語」を作り出すことができるので、言語に見られる構造・規則が言語使用において明らかに有利なものであるのか、といったことを検証することが可能です。</p>
<p><strong>効率的なコミュニケーション仮説</strong> (Efficient Communication Hypothesis) では、言語構造が効率的なコミュニケーションを実現するように形作られてきた、と考え、言語に存在する普遍性 (universalities) や 強い統計的傾向 (strong statistical tendencies) が<strong>なぜ</strong>あるのか、ということを言語使用に帰して説明しようとしています。</p>
<!-- 近年、この「効率性」を厳密に定義し、心理的に「妥当」に定量化する方法論が確立されてきたことで、研究が進みつつあります。 -->
<p>おすすめレビュー論文、本：</p>
<ul>
<li><a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.126" target="_blank" rel="noopener">Jaeger and Tily (2011)</a>
<!-- - 心理言語学研究からはじめて typology にまで話を広げましょう、という主張をするレビュー論文 -->
</li>
<li><a href="https://doi.org/10.1146/annurev-linguistics-011817-045406" target="_blank" rel="noopener">Kemp et al. (2018)</a>
<!-- - 特に lexicon (semantic categorization) に関する研究を牽引してきた著者たちによる Annual Review of Linguistics のレビュー論文 -->
</li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S1364661319300580" target="_blank" rel="noopener">Gibson et al. (2019)</a></li>
<li><a href="https://www.frontiersin.org/articles/10.3389/fcomm.2022.657725/full" target="_blank" rel="noopener">Futrell and Hahn (2022)</a></li>
<li><a href="https://www.cambridge.org/core/books/communicative-efficiency/F5AA238FB82B9739592CFAC62BF89708" target="_blank" rel="noopener">Levshina (2022)</a></li>
</ul>
<p>* なお、ここでの「コミュニケーション」とは、文化・社会的要因等を考慮したり、ジェスチャーなどの非言語的情報を考慮する、といったようなレベルにまではまだ達しておらず、より単純に単語や句、文の伝達を指します。
言語学や心理学、認知科学で扱われてきた、文レベルを超えたコミュニケーションについては、まだまだこれからです。</p>
<h2 id="単純性と情報伝達性のトレードオフ">単純性と情報伝達性のトレードオフ</h2>
<p>コミュニケーションが効率的であるとは、人間の認知能力の制約のもと、「意図・情報の伝達が最大化されている一方、産出や理解といった使用のコストが最小化されている状況」のことを指す、という表現を私は使います。
つまり、できる限り informative でありかつできる限り simple である状況が効率的であります。</p>
<p>この情報伝達性 (<em>informativeness</em>) と単純性 (<em>simplicity</em>) にはトレードオフ関係があり、つまり、どちらか一方だけを高めるともう一方は低くなってしまう、という関係であり、自然言語は、このトレードオフのもとで（ほとんど）最適解である側面が数々観察されています。</p>
<p>「最適解」とは、お気持ちレベルの説明すると、「片方の軸における値をより良い方に持っていくと、どうしてももう片方の軸における値が悪い方に行ってしまう点」のことを指します。
そういった点は1つに定まるわけではなく、下の図のように曲線を描くはずです。
この最適解のことをパレート解と、そしてパレート解を結んでできる曲線のことをパレートフロンティアといいます。</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blogs/250101information/imageoftradeoff_hu4786304544351349382.webp 400w,
               /github-pages/blogs/250101information/imageoftradeoff_hu3424281975557398783.webp 760w,
               /github-pages/blogs/250101information/imageoftradeoff_hu10939622207393665217.webp 1200w"
               src="/github-pages/blogs/250101information/imageoftradeoff_hu4786304544351349382.webp"
               width="760"
               height="609"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>* <a href="">『自然言語処理』の学会記事 (to appear)</a> でも書いたのですが、<em>simplicity</em> と <em>informativeness</em> という用語は専門用語として固まってきてしまっているようなのですが、その意味するところが伝わりにくい表現であります。
私自身は <em>informative</em> という単語の語感がそもそもわからないので何とも言い難いですが、そもそも「コミュニケーションの効率性」をどういった概念・シナリオで考えるのか、そしてそれをどういった表現で言い表すのか、といったことの整理は、今後時間をかけてなされていくものなのでしょうかね。トレードオフの概念をもっとも抽象的に（それゆえに正確に？）表すとすると、<em>complexity/accuracy</em> とかでしょうか。それか、<em>cost/benefit</em> とかでしょうか。そして、こうした抽象的な概念を、人間の言語処理の問題として表現するとなると、<a href="https://www.sciencedirect.com/science/article/pii/S0010027711002496" target="_blank" rel="noopener">Piantadosi et al. (2012)</a> で使われていた、<em>ease/clarity</em> がいいですよね。とは思っています。</p>
<p>2つの相反する圧力のもとでの最適化、として言語を説明する研究は、<a href="https://psycnet.apa.org/record/1995-97959-000" target="_blank" rel="noopener">Hawkins (1994)</a> や <a href="https://benjamins.com/catalog/la.132.04has?srsltid=AfmBOopYpsq--9vlEPxMDdbyNompli6t4Jw79TmooUgYmoaNU1l7YsCG" target="_blank" rel="noopener">Haspelmath (2008)</a> など、言語学者の間でも主張されてきたものですが、効率的なコミュニケーション研究は、それを理論中立的な情報理論のことばを使って書き下すことにより、定量的な研究を実現するだけでなく、それをスケールさせることができた、という点が大きな進展だったのだと思っています。
特に語彙 (wordformやsemantic categorization) や、最近では文レベルについても研究が進んできています：</p>
<p>語彙に関して：</p>
<ul>
<li>親族名称: <a href="https://www.science.org/doi/abs/10.1126/science.1218811" target="_blank" rel="noopener">Kemp and Regier (2012)</a></li>
<li>色: <a href="https://www.pnas.org/doi/abs/10.1073/pnas.0610341104" target="_blank" rel="noopener">Regier et al. (2007)</a>, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118346136.ch11" target="_blank" rel="noopener">Regier et al. (2015)</a>, <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1619666114" target="_blank" rel="noopener">Gibson et al. (2017)</a>, <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1800521115" target="_blank" rel="noopener">Zaslavsky et al. (2018)</a></li>
<li>数: <a href="https://doi.org/10.1162/opmi%5c_a%5c_00034" target="_blank" rel="noopener">Xu et al. (2020)</a>, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13424" target="_blank" rel="noopener">Denic and Szymanik (2024)</a></li>
<li>文法標識 (number, tense, evidentiality): <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2025993118" target="_blank" rel="noopener">Mollica et al. (2021)</a></li>
<li>量化子: <a href="https://www.mdpi.com/1099-4300/23/10/1335" target="_blank" rel="noopener">Steinert-Threlkeld (2021)</a></li>
<li>人称代名詞: <a href="https://escholarship.org/uc/item/2sj4t8m3" target="_blank" rel="noopener">Zaslavsky et al. (2021)</a></li>
<li>不定代名詞: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13142" target="_blank" rel="noopener">Denic et al. (2022)</a></li>
<li>Boolean connectives: <a href="https://doi.org/10.1162/ling%5c_a%5c_00461" target="_blank" rel="noopener">Uegaki (2022)</a></li>
<li>Spatial demonstratives: <a href="https://www.sciencedirect.com/science/article/pii/S0010027723001397" target="_blank" rel="noopener">Chen et al. (2023)</a></li>
<li>単語の長さ: <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1012551108" target="_blank" rel="noopener">Piantadosi et al. (2011)</a>, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12689" target="_blank" rel="noopener">Mahowald et al. (2018)</a>, <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2406971121" target="_blank" rel="noopener">Xu et al. (2024)</a></li>
<li>Zipf則: <a href="https://www.pnas.org/doi/abs/10.1073/pnas.0335980100" target="_blank" rel="noopener">Ferrer i Cancho and Sole (2003)</a>, <a href="https://doi.org/10.1140/epjb/e2005-00340-y" target="_blank" rel="noopener">Ferrer i Cancho (2005)</a></li>
<li>Zipf&rsquo;s meaning-frequency law: <a href="https://www.sciencedirect.com/science/article/pii/S0010027711002496" target="_blank" rel="noopener">Piantadosi et al. (2012)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0010027722000828" target="_blank" rel="noopener">Trott and Bergen (2022)</a></li>
<li>&hellip;</li>
</ul>
<p>文法に関して：</p>
<ul>
<li>合成性（構成性）: <a href="https://www.sciencedirect.com/science/article/pii/S0010027715000815" target="_blank" rel="noopener">Kirby et al. (2015)</a>, <a href="https://arxiv.org/abs/2405.12109" target="_blank" rel="noopener">Futrell and Hahn (2024)</a></li>
<li>Greenbergの語順普遍: <a href="https://www.pnas.org/doi/10.1073/pnas.1910923117" target="_blank" rel="noopener">Hahn et al. (2020)</a></li>
<li>等位接続における構造依存性: <a href="https://aclanthology.org/2024.conll-1.23/" target="_blank" rel="noopener">Kajikawa et al. (2024)</a></li>
</ul>
<p>効率的なコミュニケーション研究について、実際に情報理論の道具を使ってどう仮説を検証するか、代表的な研究を紹介しながら説明します。</p>
<h3 id="ferrer-i-cancho-and-sole-2003httpswwwpnasorgdoiabs101073pnas0335980100"><a href="https://www.pnas.org/doi/abs/10.1073/pnas.0335980100" target="_blank" rel="noopener">Ferrer i Cancho and Sole (2003)</a></h3>
<p><a href="https://www.pnas.org/doi/abs/10.1073/pnas.0335980100" target="_blank" rel="noopener">Ferrer i Cancho and Sole (2003)</a> では、世界中の言語に見られるZipf則 (Zipf&rsquo;s law) (<a href="https://www.routledge.com/The-Psycho-Biology-Of-Language-AN-INTRODUCTION-TO-DYNAMIC-PHILOLOGY/ZipfGeorgeKingsley/p/book/9781138875098?srsltid=AfmBOopeIeJMNPikd6dTWUgyGseCgJeRgqiKdyvD5JXpZ5Qxcuzrgx3m" target="_blank" rel="noopener">Zipf, 1936</a>; <a href="https://psycnet.apa.org/record/1950-00412-000" target="_blank" rel="noopener">1949</a>) が、効率的なコミュニケーションの実現の結果として生まれるものなのかを検証するため、コミュニケーションを効率的にしたときにZipf則が立ち現れるのかシミュレーション実験をしました。
この研究は、効率的なコミュニケーション研究の先駆けであるので紹介します。</p>
<p>まず、前提として、Zipf則とは、単語頻度に関する経験則で、全体で$k$番目に多く使用される単語の頻度 $f(k)$ は、$f(k)=C\cdot k^{-\alpha}$ と冪乗則に従う、というものです。要は、よく使われる単語はめちゃくちゃ使われるし、珍しい単語は本当に珍しい、というものです。
$C$ は比例定数で、$\alpha$ は <a href="https://psycnet.apa.org/record/1950-00412-000" target="_blank" rel="noopener">Zipf (1949)</a> では $1$ です。
両辺に対数を適用すると、$\log f(k) = -\alpha\log k + \log C$ と線形な関係になります。</p>
<p>実際の頻度と頻度ランクの関係を見たら、式の意味はすぐにわかります。
ちょうど手元に<a href="https://github.com/UniversalDependencies/UD_Japanese-BCCWJ" target="_blank" rel="noopener">UD_Japanese-BCCWJ</a>のv2.10、国語研長単位 (LUW) 分割があったので、これの単語頻度と頻度ランクを数えてみました。
全部で57,109文、995,632単語（長単位）で、<em>x</em>軸を頻度ランク、<em>y</em>軸を実際の頻度としてプロットしたのが左図、そして両軸に $\log_{10}$ を適用してプロットしたものが右図です。</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blogs/250101information/bccwj_zipf_hu6585653125791012963.webp 400w,
               /github-pages/blogs/250101information/bccwj_zipf_hu9883852135442870022.webp 760w,
               /github-pages/blogs/250101information/bccwj_zipf_hu10125593117757427291.webp 1200w"
               src="/github-pages/blogs/250101information/bccwj_zipf_hu6585653125791012963.webp"
               width="760"
               height="378"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>右図については、最小二乗法での回帰直線も引いてみました。
$\alpha = 1$ とはなっていないようですが、ある程度はZipf則に従っているようにみえます。</p>
<p>* 直線から逸脱している部分があることや、傾きの係数が $0.94$ であることを「Zipf則に従っている」とみるか、は結構重要な問題な気はします。
詳しくは <a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" target="_blank" rel="noopener">Piantadosi (2014)</a> をお読みください。</p>
<p>では、（一旦単語分布はZipf則に従っているとして）なぜ単語分布はZipf則に従うのでしょうか。
<a href="https://psycnet.apa.org/record/1950-00412-000" target="_blank" rel="noopener">Zipf (1949)</a> は、言語は、省エネでありたいという話し手の要求と、意図・情報の復元が容易でありたいという聞き手の要求のトレードオフのもとで形作られているのだという仮説を提案しました。
具体例として、言語を「単語」と「意味」の対応関係と考えてみましょう。
すべての意味をたった一つの単語で表現する言語があったら、話し手は覚えるべき単語が一つだけになるため、話者の負担は最小限になります。
しかし、このような言語では、単語がどの意味を指すのか曖昧になり、聞き手が話者の意図を理解するのに困難が生じます。
一方で、すべての単語が明確に異なる意味を表現するシステムでは、聞き手の理解（意図推定）は容易になりますが、話し手の負担は増加します。
Zipf は、言語はこうした話し手と聞き手の相反する要求のバランスによって形作られているのだと主張しました。</p>
<p>では、それは本当か？と計算機上で実験してみたのが、Ferrer i Cancho氏 と Sole氏 の研究です。</p>
<h4 id="モデル">モデル</h4>
<p>彼らは、上述した Zipf の説明を検証するため、信号（単語）と意味の対応関係について、話し手と聞き手双方のコストがトレードオフのもとで最小となるような関係になるまで進化アルゴリズムを使って推定する、ということを行いました。</p>
<p>$n$ 個のシグナル $\mathcal{S} = {s_1,&hellip;,s_i,&hellip;,s_n}$ と
$m$ 個の意味（objects of reference）$\mathcal{R} = {r_1,&hellip;,r_i,&hellip;,r_m}$ を考えたとき、それらの間の関係をバイナリ行列 $\mathbf{A} = {a_{ij}}\ (1\leq i\leq n,\ 1\leq j\leq m)$ で表します。</p>
<p>バイナリ行列は、すべて $0$ か $1$ で構成されている行列で、ここでは、ある $i$ 番目のシグナル $s_i$ が $j$ 番目の意味 $r_j$ を表している場合、行列内の要素 $a_{ij}$ は $1$ と、そうでないならば $0$ と表現します。</p>
<p>いま、synonym（同義語）があるなら、シグナルと意味の確率分布の関係は以下になります：
\begin{equation}
P(s_i) = \sum_j P(s_i,r_j)
\end{equation}</p>
<p>なお、$P(r_i) = 1/m$ と仮定。</p>
<p>ベイズの定理より、
\begin{equation}
P(s_i,r_j) = P(r_j)P(s_i\mid r_j)
\end{equation}
であり、$P(s_i\mid r_j)$ は、
\begin{equation}
P(s_i\mid r_j) = a_{ij}\frac{1}{\omega_j}.
\end{equation}
で定義します。
$\omega_i = \sum_j a_{ij}$ は意味 $r_j$ における同義語の数です。</p>
<p>代入すると、
\begin{equation}
P(s_i,r_j) = a_{ij}\frac{P(r_j)}{\omega_j}
\end{equation}</p>
<p>話し手と聞き手のコストは、それぞれ平均情報量（エントロピー）で定義します：
\begin{align}
H(S) &amp;= -\sum_i^n P(s_i)\log P(s_i) \newline
H(R\mid S) &amp;= -\sum_i^n\sum_j^m P(s_i,r_j)\log P(r_j\mid s_i)
\end{align}</p>
<p>そして最後に、話し手と聞き手のコストのトレードオフは、それぞれの加重和で表現します：
\begin{equation}
\Omega(\lambda)=\lambda H(S) + (1-\lambda)H(R\mid S)
\end{equation}
ここで、$0\leq\lambda\leq 1$ で、話し手と聞き手のコストのどちらを重視するかを表します。</p>
<p>この $\lambda$ の値を変えながら、各 $\lambda$ において、$\Omega(\lambda)$ を最小化するようなバイナリ行列 $\mathbf{A}$ を進化アルゴリズムで求める、ということをします。</p>
<h4 id="再現コードと結果">再現コードと結果</h4>
<p>非常に単純なモデルなので、再現実装してみました（いくつか簡略化しています）。
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blogs/250101information/freq_rank_hu1559019237969630005.webp 400w,
               /github-pages/blogs/250101information/freq_rank_hu10479040455501117085.webp 760w,
               /github-pages/blogs/250101information/freq_rank_hu526095839200649385.webp 1200w"
               src="/github-pages/blogs/250101information/freq_rank_hu1559019237969630005.webp"
               width="691"
               height="760"
               loading="lazy" data-zoomable /></div>
  </div></figure>

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blogs/250101information/log_freq_rank_hu2101583012652647821.webp 400w,
               /github-pages/blogs/250101information/log_freq_rank_hu9408570637108809242.webp 760w,
               /github-pages/blogs/250101information/log_freq_rank_hu4437273928768977922.webp 1200w"
               src="/github-pages/blogs/250101information/log_freq_rank_hu2101583012652647821.webp"
               width="691"
               height="760"
               loading="lazy" data-zoomable /></div>
  </div></figure>

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /github-pages/blogs/250101information/mi_lexicon_size_hu12214877015639816266.webp 400w,
               /github-pages/blogs/250101information/mi_lexicon_size_hu16297562656130702712.webp 760w,
               /github-pages/blogs/250101information/mi_lexicon_size_hu9567190417810425306.webp 1200w"
               src="/github-pages/blogs/250101information/mi_lexicon_size_hu12214877015639816266.webp"
               width="760"
               height="456"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>最適化の結果として、$\lambda = 0.41$ あたりで、Zipf則に近い分布が得られることがわかりました。</p>
<p>もちろん、目的関数が心理的に妥当ではない (<a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" target="_blank" rel="noopener">Piantadosi (2014)</a>) という批判はあります。
具体的には、通信コードとしての複雑さ（＝エントロピー）が、人間にとっての複雑さと一致しているのかは誰も示していない、ということと、すべての意味が等確率で現れるという前提は明らかに現実に則していない、ということです。
前者は、「平均サプライザル」とみれば良いのでは、とは思うのですが、後者については問題で、確かに</p>
<!-- や、そもそも Zipf則は統計側の性質であって、言語はただそれに従っているだけでは ([田中 (2021)](https://www.utp.or.jp/book/b559376.html)) という主張もあります。 -->
<h3 id="zaslavsky-et-al-2018httpswwwpnasorgdoiabs101073pnas1800521115"><a href="https://www.pnas.org/doi/abs/10.1073/pnas.1800521115" target="_blank" rel="noopener">Zaslavsky et al. (2018)</a></h3>
<p>hoge</p>
<!-- ### トレードオフを検証する方法論
#### 反事実的 (counterfactual) 言語の設計

#### Information Bottleneck -->
<h2 id="rational-speech-act-rsa">Rational Speech Act (RSA)</h2>
<p>Rational Speech Act (RSA) モデル (<a href="https://www.science.org/doi/10.1126/science.1218633" target="_blank" rel="noopener">(Frank and Goodman, 2012)</a>) は、その名の通り、「<strong>話し手</strong>の合理的な発話」をモデル化したモデルです。
話し手が合理的であるとは、「話し手は、必ずしも全部を言ってくれるわけではないが（parsimoniousであるが）、何らかの意図を伝えようとできる限りinformativeな発話をする」ということです。</p>
<p>こうした状況はGrice以来想定されてきたわけですが、RSAではその話し手の合理性を確率変数に対するutility関数の形で表現し、聞き手の語用論的推論 (pragmatic reasoning) はベイズ推定 (Beyesian inference) で定式化しました。</p>
<p>話者の合理性を表すutility関数は、さまざまな亜種はあるものの、多くの場合、聞き手にとってinformativeである一方、自分にとってコストが低い発話、という、informativenessとaccessibility (cost) のトレードオフを表現したものが使われます。</p>
<p>具体的には、意味 $m\in\mathcal{M}$ と発話 $u\in\mathcal{U}$ について、utility関数 $U(m,u)$ は、
\begin{equation}
U(m,u) = \log P_L(m\mid u) - C(u),
\end{equation}
ここで、$\log P_L(m\mid u)$ は聞き手にとってのinformativeさを、$C(u)\geq 0$ は発話それ自体のコストを表します。</p>
<p>そして、実際の話し手の発話の確率は、このutility関数の値をもとに確率的に決まると考えると、ソフトマックス関数を通して求めることができます：
\begin{equation}
P_{S_t}(u\mid m) = \frac{e^{\alpha U_{t-1}(u,m)}}{\sum e^{\alpha U_{t-1}(u,m)}} \propto e^{\alpha U_{t-1}(u,m)},
\end{equation}
ここで、$\alpha$ は話し手がutilityにどれだけ重みを置いているかを表します。</p>
<p>こうして話し手の合理的な発話がモデル化できると、聞き手の語用論的推論は、ベイズ推論によって表現することができます：
\begin{equation}
P_{L_t}(m\mid u) \propto P_{S_t}(u\mid m)P(m)
\end{equation}</p>
<p>なお、添え字の $t$ や $t-1$ は、合理的な話し手が再帰的な思考の内部で想定した、話し手および聞き手モデルの埋め込みのレベルを指します。
要は、合理的な話し手は、「合理的な話し手ならこう言うだろう、ということを合理的な聞き手は推定するんだろうな、だからこういおうかな&hellip;」という推論を再帰的に繰り返し続けることができ、毎度ひとつ前の埋め込みでの聞き手を想定したうえで発話選択をする、ということを繰り返している、ということです。</p>
<p>ただし、もちろん、この再帰的な話し手と聞き手の関係は無限に考えることができるが、それだと推論がいつまでたっても終わらないので、通常は、1回埋め込んだところに、&ldquo;literal listener&rdquo; $P_{Lit}$ を想定します。</p>
<p>literal listner は、語用論的推論は行わず、文字通り、意味の真偽を判断します：
\begin{equation}
P_{Lit}(m\mid u) \propto \delta_{[[u]]}(m) P(m),
\end{equation}
ここで、$\delta_{[[u]]}(\cdot)$ は、真偽値の $0, 1$ を返す関数です。
これにより、RSA frameworkにlexical semanticsやcompositional semanticsを組み込むこともできる、という主張です。</p>
<p>もう一度流れを整理すると（埋め込みが1回の場合）、
$t$ における合理的な話し手の発話は、
\begin{equation}
U(m,u) = \log P_{L_{t-1}}(m\mid u) - C(u)
\end{equation}
というutility関数をもとに、
\begin{equation}
P_{S_t}(u\mid m) \propto e^{\alpha U_{t-1}(u,m)}
\end{equation}
でモデル化することができます。
ここで、utility関数内の $P_{L_{t-1}}$ をliteral listnerと考えると、
\begin{equation}
P_{Lit}(m\mid u) \propto \delta_{[[u]]}(m) P(m)
\end{equation}
で $P_{L_{t-1}}$ をモデル化できます。</p>
<p>そうして、$t$ における聞き手の語用論的推論は、
\begin{equation}
P_{L_t}(m\mid u) \propto P_{S_t}(u\mid m)P(m)
\end{equation}
で求めることができます。</p>
<p>RSA 関連でとりあえず参照すべきもの：</p>
<ul>
<li><a href="https://www.science.org/doi/10.1126/science.1218633" target="_blank" rel="noopener">Frank and Goodman (2012)</a>
<ul>
<li>RSAが提案された最初の論文</li>
</ul>
</li>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S136466131630122X" target="_blank" rel="noopener">Goodman and Frank (2016)</a>
<ul>
<li>RSAの提案者によるレビュー論文</li>
</ul>
</li>
<li><a href="https://www.jstage.jst.go.jp/article/jjsai/31/1/31_91/_article/-char/ja/" target="_blank" rel="noopener">折田 (2016)</a>
<ul>
<li>著者自身によるRSAを用いた指示表現選択の研究の解説</li>
</ul>
</li>
<li><a href="https://www.problang.org/" target="_blank" rel="noopener">Scontras et al. (2016?)</a>
<ul>
<li>ESSLLI 2016での資料をもとにした解説記事</li>
</ul>
</li>
<li><a href="https://www.annualreviews.org/content/journals/10.1146/annurev-linguistics-031220-010811" target="_blank" rel="noopener">Degen (2023)</a>
<ul>
<li>2023年版の、Annual Review of Linguistics でのレビュー論文</li>
</ul>
</li>
</ul>
<h2 id="uniform-information-density-uid">Uniform Information Density (UID)</h2>
<p>hoge</p>
<!-- - operationalizationが複数あるが、どれが最も良いのか不明。
- "Uniformity"の定義があやふや -->
<h2 id="dependency-length-minimization-dlm">Dependency Length Minimization (DLM)</h2>
<p>hoge</p>
<h2 id="memory--prediction-trade-off">Memory&ndash;prediction trade-off</h2>
<p>hoge</p>
<h1 id="言語獲得">言語獲得</h1>
<p>確率モデル、というかベイズの定理を利用した言語学・認知科学研究で忘れてはならないのが、言語獲得のモデル化です（ベイジアンモデリングの解説については、<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661310001129" target="_blank" rel="noopener">Griffiths et al. (2010)</a> や <a href="https://oecs.mit.edu/pub/lwxmte1p/release/2" target="_blank" rel="noopener">Griffiths (2024)</a> へ）。</p>
<p>\begin{equation}
P(h_i\mid d)=\frac{P(d\mid h_i)P(h_i)}{\sum_{h_j\in\mathcal{H}}P(d\mid h_j)P(h_j)}\propto P(d\mid h_i)P(h_i)
\end{equation}</p>
<p>世の中にはベイズの定理に関する解説で満ち満ちているので適当に書きますが、簡単に説明すると、上式は、データ $d$ を受けたとき、仮説 $h_i\in\mathcal{H}$ をどれだけ支持するか、という確率 $P(h_i\mid d)$ は、その仮説 $h_i$ の事前確率 $P(h_i)$ と、その仮説 $h_i$ をもっていたときにデータ $d$ に出会う尤度（尤もらしさ）$P(d\mid h_i)$ の積でわかりますよ、ということです。</p>
<p>ベイズの定理の嬉しさは、概念上は、生得主義 vs. 経験主義といった二項対立を超えて、言語はどのくらい学習可能か、どのくらいの生得知識 (inductive bias) が必要か、ということを定量的に評価することができるはず、というところでしょう、と、私は思っています。
これについて、<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318" target="_blank" rel="noopener">Chater and Manning (2006)</a> で説明があるので、引用して紹介します。</p>
<p>以下、<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318" target="_blank" rel="noopener">Chater and Manning (2006)</a> の p.342より：</p>
<blockquote>
<p>Oversimplifying somewhat, suppose that a learner wonders whether to include constraint $C$ in her grammar. $C$ happens, perhaps coincidentally, to fit all the data so far encountered. If the learner does not assume $C$, the probability that each sentence will happen to fit $C$ by chance is $p$. Thus, each sentence obeying $C$ is $1/p$ times more probable, if the constraint is true than if it is not (if we simply rescale the probability of all sentences obeying the constraint). Thus, after $n$ sentences, the probability of the corpus, is $1/p^n$ greater, if the constraint is included. Yet, a more complex grammar will typically have a lower prior probability. If the ratio of priors for grammars with/without the constraint is greater than $1/p^n$, then, by Bayes&rsquo; theorem, the constraint is unlearnable in $n$ items.</p>
</blockquote>
<p>ようは、ある制約 $C$ が生得知識としてあるべきなのか、それとも学習可能なのかは、制約 $C$ がある文法（仮に $G_C$）と制約 $C$ がない文法（仮に $G_{\neg C}$）の事前確率の比 $\frac{P(G_C)}{P(G_{\neg C})}$ と尤度比 $\frac{P(d\mid G_C)}{P(d\mid G_{\neg C})}$ を比べることで、事後分布の比 $\frac{P(G_C\mid d)}{P(G_{\neg C}\mid d)}$ を比べるのと同じこととなり、制約 $C$ が学習可能なのかを判定することができる、ということです。</p>
<p>仮に、$G_{\neg C}$ を想定している人が、偶然にも $G_{C}$ に整合的な文 $s_C$ に出会う確率を $p$ とすると、$P(s_C\mid G_{\neg C}) = p$ です。
$P(s_C\mid G_{C}) = 1$ であるとすると、ある人が $G_{C}$ に整合的な文に $n$ 回出会ったとき、それぞれの文法を想定したときの尤度比は、$\frac{P(s_C\mid G_{C})^n}{P(s_C\mid G_{\neg C})^n} = \frac{1}{p^n}$ です。
このとき、それぞれの文法の事前確率 $P(G_C)$ と $P(G_{\neg C})$ の比が $\frac{1}{p^n}$ 分離れているのかどうかで、制約 $C$ が学習可能なのかどうか決まります。
基本的には、制約が一つ多い文法の方が複雑で事前確率がより小さくなっているはずだが、その小さい事前確率をもってもなお尤度により挽回可能なのか、ということです。</p>
<h3 id="xu-and-tenenbaum-2007">Xu and Tenenbaum (2007)</h3>
<h3 id="perfors-et-al-2011">Perfors et al. (2011)</h3>
<h3 id="abend-et-al-2017">Abend et al. (2017)</h3>
<h3 id="yang-and-piantadosi-2022">Yang and Piantadosi (2022)</h3>
<!-- 個人的に、[『自然言語処理』への解説記事]()でも書いたのですが、学習可能性について考えたいのですが、現在のACL系列の論文で大人気の「とりあえずニューラルモデルでの学習速度を見ました」という研究はさすがにコネクショニストすぎて私自身は乗っかれないと思っているものの、そうはいっても学習可能性については語りたいし、そうしたらニューラルを使うのが「出版において安全」な気がするし…という状況の中で、何とかベイジアンモデリングが使えないかと模索していました。

[Yang (2004)](https://www.sciencedirect.com/science/article/pii/S1364661304002104?via%3Dihub)の頃から指摘されているように、 -->
<h1 id="言語進化">言語進化</h1>
<h2 id="繰り返し学習モデル-iterated-learning-model">繰り返し学習モデル (Iterated Learning Model)</h2>
<!-- - 文化進化による構成性の出現
    - [Kirby (2017)](https://link.springer.com/article/10.3758/s13423-016-1166-7) -->

    </div>

    







<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblogs%2F250101information%2F&amp;text=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblogs%2F250101information%2F&amp;t=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblogs%2F250101information%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblogs%2F250101information%2F&amp;title=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6%20http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblogs%2F250101information%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http%3A%2F%2Flocalhost%3A1313%2Fgithub-pages%2Fblogs%2F250101information%2F&amp;title=%E7%A2%BA%E7%8E%87%E3%83%BB%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E8%A8%80%E8%AA%9E%E7%A0%94%E7%A9%B6" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="http://localhost:1313/github-pages/"><img class="avatar mr-3 avatar-square" src="/github-pages/authors/admin/avatar_hu18040700168886216705.jpeg" alt="Kohei Kajikawa"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="http://localhost:1313/github-pages/">Kohei Kajikawa</a></h5>
      <h6 class="card-subtitle">Master&rsquo;s student at the University of Tokyo (UTokyo).</h6>
      <p class="card-text">My research interests include computational linguistics and computational psycholinguistics.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/github-pages/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/qjqw1223" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/kohei-kaji" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Kohei-Kajikawa" target="_blank" rel="noopener">
        <i class="fab fa-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/github-pages/uploads/CV.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  <p class="powered-by copyright-license-text">
    © 2025 Kohei Kajikawa.
  </p>
  





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/github-pages/js/vendor-bundle.min.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/github-pages/js/wowchemy-headroom.js" type="module"></script>









  
  


<script src="/github-pages/en/js/wowchemy.min.js"></script>



  <script src="/github-pages/js/wowchemy-map.js" type="module"></script>






















</body>
</html>
