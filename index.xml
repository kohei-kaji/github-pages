<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kohei Kajikawa</title>
    <link>https://kohei-kaji.github.io/github-pages/</link>
      <atom:link href="https://kohei-kaji.github.io/github-pages/index.xml" rel="self" type="application/rss+xml" />
    <description>Kohei Kajikawa</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><copyright>© 2023 Kohei Kajikawa</copyright><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kohei-kaji.github.io/github-pages/media/icon_hu10226378635250344412.png</url>
      <title>Kohei Kajikawa</title>
      <link>https://kohei-kaji.github.io/github-pages/</link>
    </image>
    
    <item>
      <title>計算心理言語学の新地平開拓の試み：効率的なコミュニケーション仮説の検証</title>
      <link>https://kohei-kaji.github.io/github-pages/misc/nlp-2025/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/misc/nlp-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CCGによる日本語脳波データのモデリング</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/isono-etal-2025-nlp/</link>
      <pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/isono-etal-2025-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>統語情報は脳情報デコーディングに寄与するのか？</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/akama-etal-2025-nlp/</link>
      <pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/akama-etal-2025-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>アテンションが記憶想起の認知モデルたりうるならば、記憶の表現としては何が妥当か？</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/yoshida-etal-2025-nlp/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/yoshida-etal-2025-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>自然言語における冪則と統語構造の関係の再考</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/nakaishi-etal-2025-nlp/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/nakaishi-etal-2025-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>認知負荷の最適化戦略としての自由語順と項省略</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/kajikawa-etal-2025-nlp/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/kajikawa-etal-2025-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>確率・情報理論を使った言語研究</title>
      <link>https://kohei-kaji.github.io/github-pages/blogs/250303information/</link>
      <pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/blogs/250303information/</guid>
      <description>&lt;script&gt;
  MathJax = {
    tex: {
      inlineMath: [[&#39;$&#39;, &#39;$&#39;]],
      displayMath: [[&#39;$$&#39;, &#39;$$&#39;], [&#39;\\[&#39;, &#39;\\]&#39;]]
    },
    options: {
      processHtmlClass: &#34;mathjax-process&#34;
    }
  };
&lt;/script&gt;
&lt;script async src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;確率や情報理論といった数学の道具は、言語使用や言語の構造それ自体を支配している何らかの規則を記述するのに非常に有用であるように思えます。&lt;/p&gt;
&lt;p&gt;それは、確率や情報理論といったものは現象に対して理論中立的 (theory-neutral) な記述が可能で、とにかく使い勝手が良い道具だからなのかもしれないし、そもそも人間の言語処理をはじめとした認知活動が何らかの確率的なものだからなのかもしれないし、認知活動自体が、ベイジアンが主張するような「合理的 (rational)」なものだと考えたら合理的でありそう、という直感や経験的事実があるからなのかもしれないし&amp;hellip;。&lt;/p&gt;
&lt;p&gt;いずれにせよ、確率や情報理論は、言語処理・言語使用の研究（つまり、Performance の側面の研究）、そしてさらには言語の構造や知識自体の研究（つまり、Competence の側面）をするのに非常に強力な道具です。&lt;/p&gt;
&lt;p&gt;近年は、コーパスをはじめとした言語資源の整備、計算機の性能向上やプログラミング言語・各種計算ライブラリの充実、大規模言語モデルのような汎用的な言語処理技術たちの登場によりかなりの精度で言語の情報量が推定できるようになってきたこと、といったさまざまな要因たちが揃ってきて、そしてさらには、確率や情報理論といった道具自体の理論研究や言語研究へ応用するといった方法論が確立されてきたので、これからできることが大量にあるよ、という状況です。&lt;/p&gt;
&lt;p&gt;実際、心理言語学とか、計算言語学、計算心理言語学、認知科学とか言われる分野の研究をみていると（これらの分野がそれぞれどういった範囲を指しているのか正直よくわからないが）、確率や情報理論であふれています。
昨年、2024年夏にオランダのロッテルダムであった CogSci（認知科学の国際会議）に参加したら、これが私にとって初めての海外での国際会議だったわけですが、何とまあ端から端まで情報理論かベイジアンモデリングで割と衝撃を受けました。
そういった状況が良いか置いておいて、とりあえず勉強しなきゃなぁと思わされたし、この波に乗っておきたい、と思えました。ので、その紹介記事です。&lt;/p&gt;
&lt;p&gt;このあたりのレビュー論文、本として、個人的参照すべきものたち：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;確率を取り入れた言語研究は結構有用じゃないでしょうか、という話：
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chater and Manning (2006)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;言語を含めた認知科学における確率モデル的（合理的）アプローチについて：
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661306001343&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tenenbaum et al. (2006)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661310001129&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Griffiths et al. (2010)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S001002771000291X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perfors et al. (2011)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mitpress.mit.edu/9780262049412/bayesian-models-of-cognition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Griffiths et al. (2024)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://oecs.mit.edu/pub/lwxmte1p/release/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Griffiths (2024)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 余談、言語学者のshannon underestimate問題 --&gt;
&lt;!-- 一方で、特に私のような、大学に文系で入学し、数理的な教育を集中して受けてこなかった人間にとって、確率や情報理論といったことばはそこまで馴染みがないものです。
そしてさらには、Chomskyを読んで言語学が面白いと思った人の中には、確率モデル・情報理論は「工学とかの実用性を追う人には大事なのかもしれないが、言語の本質を捉えるのに寄与しないもの」という認識の人も少なくないと思います（めちゃくちゃ急な決めつけですが、少なくとも私はそうだった）。
[Chomsky (1956)](https://ieeexplore.ieee.org/document/1056813) [Chomsky (1957)](https://www.degruyter.com/document/doi/10.1515/9783112316009/html)には、「」と、
では、Claude Shannonのマルコフモデルを散々叩いていて、

[Chomsky (1965)](https://www.jstor.org/stable/j.ctt17kk81z)には、「」と書いてあるし。 --&gt;
&lt;!-- 人間はそこまで合理的でない？という問題 --&gt;
&lt;h1 id=&#34;目次&#34;&gt;目次&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#%e6%96%87%e5%87%a6%e7%90%86%e7%b3%bb&#34;&gt;文処理系&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#surprisal&#34;&gt;Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lossy-context-surprisal&#34;&gt;Lossy-context surprisal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#noisy-channel-model&#34;&gt;Noisy-channel model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%e5%8a%b9%e7%8e%87%e7%9a%84%e3%81%aa%e3%82%b3%e3%83%9f%e3%83%a5%e3%83%8b%e3%82%b1%e3%83%bc%e3%82%b7%e3%83%a7%e3%83%b3-efficient-communication&#34;&gt;効率的なコミュニケーション&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#%e5%8d%98%e7%b4%94%e6%80%a7%e3%81%a8%e6%83%85%e5%a0%b1%e4%bc%9d%e9%81%94%e6%80%a7%e3%81%ae%e3%83%88%e3%83%ac%e3%83%bc%e3%83%89%e3%82%aa%e3%83%95&#34;&gt;単純性と情報伝達性のトレードオフ&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ferrer-i-cancho-and-sole-2003&#34;&gt;Ferrer i Cancho and Sole (2003)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rational-speech-act-rsa&#34;&gt;Rational Speech Act (RSA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#uniform-information-density-uid&#34;&gt;Uniform Information Density (UID)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dependency-length-minimization-dlm&#34;&gt;Dependency Length Minimization (DLM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory--prediction-trade-off&#34;&gt;Memory&amp;ndash;prediction trade-off&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%e8%a8%80%e8%aa%9e%e7%8d%b2%e5%be%97&#34;&gt;言語獲得&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#xu-and-tenenbaum-2007-psycho-rev&#34;&gt;Xu and Tenenbaum (2007)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#perfors-et-al-2011-cognition&#34;&gt;Perfors et al. (2011)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#abend-et-al-2017-cognition&#34;&gt;Abend et al. (2017)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%e8%a8%80%e8%aa%9e%e9%80%b2%e5%8c%96&#34;&gt;言語進化&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#%e7%b9%b0%e3%82%8a%e8%bf%94%e3%81%97%e5%ad%a6%e7%bf%92%e3%83%a2%e3%83%87%e3%83%ab-iterated-learning-model&#34;&gt;繰り返し学習モデル (Iterated Learning Model)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;文処理系&#34;&gt;文処理系&lt;/h1&gt;
&lt;!-- 我々の産出・理解といった言語使用において、確率モデルや情報理論の道具を使うことで割と適切にモデル化できることがわかっている側面が多々あります。 --&gt;
&lt;h2 id=&#34;surprisal&#34;&gt;Surprisal&lt;/h2&gt;
&lt;p&gt;ある単語 $w$ の生起確率 $P(w)$ の 負の対数 $-\log P(w)$ のことを単語 $w$ のサプライザルと呼びます。
サプライザル理論 (&lt;a href=&#34;https://aclanthology.org/N01-1021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hale, 2001&lt;/a&gt;; &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027707001436&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Levy, 2008b&lt;/a&gt;) では、ある単語の予測のしにくさ（サプライザル）はその単語の処理の難しさに比例する、とします：
\begin{equation}
\text{difficulty}(w)\propto -\log P(w)
\end{equation}&lt;/p&gt;
&lt;!-- - ここで、$P(X|Y)$ は、$Y$ だったときの $X$ の条件付き確率を表します。 --&gt;
&lt;ul&gt;
&lt;li&gt;確率値を表すときは大文字 $P$、確率分布を表すときは小文字 $p$ を使うようです。&lt;/li&gt;
&lt;li&gt;対数の底はしばしば $2$ で bit単位ですが、底が一致している限りはサプライザル同士の相対関係は変わらないので、底の値自体をサプライザルの定義に組み込む必要は（たぶん）ないです。
&lt;ul&gt;
&lt;li&gt;$\log_2 x = \frac{\log_e x}{\log_e 2} = \frac{\log_e x}{0.30103&amp;hellip;}$ と、底の変更は可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;サプライザルの値 $-\log P(\cdot)$ は、生起確率 $P(\cdot)$ の値が小さければ小さいほどほど大きくなる、という関係になっています。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/250303information/surp_hu16772009528392312929.webp 400w,
               /github-pages/blogs/250303information/surp_hu936059329883402758.webp 760w,
               /github-pages/blogs/250303information/surp_hu2938042842656091789.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/250303information/surp_hu16772009528392312929.webp&#34;
               width=&#34;760&#34;
               height=&#34;722&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;そして、実際に、読み時間 (e.g., &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0010027708001741&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demberg and Keller, 2008&lt;/a&gt;; &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027713000413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Smith and Levy, 2013&lt;/a&gt;; &lt;a href=&#34;https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00612/118718/Testing-the-Predictions-of-Surprisal-Theory-in-11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wilcox et al., 2023&lt;/a&gt;; &lt;a href=&#34;https://www.pnas.org/doi/10.1073/pnas.2307876121&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shain et al., 2024&lt;/a&gt;) や、ERP（&lt;a href=&#34;https://bsd.neuroinf.jp/wiki/%E4%BA%8B%E8%B1%A1%E9%96%A2%E9%80%A3%E9%9B%BB%E4%BD%8D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;事象関連電位&lt;/a&gt;）(e.g., &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0093934X14001515&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frank et al., 2015&lt;/a&gt;; &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0207741&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brennan and Hale, 2019&lt;/a&gt;)、 fMRIによるBOLD信号 (e.g., &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177794&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lopopolo et al., 2017&lt;/a&gt;; &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0028393219303495?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shain et al., 2020&lt;/a&gt;) について、サプライザルが predictor として有効であることが示されています。&lt;/p&gt;
&lt;h3 id=&#34;サプライザルがとらえているものは何なのか&#34;&gt;サプライザルがとらえているものは何なのか？&lt;/h3&gt;
&lt;p&gt;では、単語 $w$ の生起確率 $P(w)$ は何で求めることができるのでしょうか。
逐次的な文処理過程について考える場合、単語 $w$ 以前の文脈 $w_1,\dots,w_{n-1}$ が与えられたときの単語 $w$ の生起確率 $P(w\mid w_1,\dots,w_{n-1})$ が求められれば良い、すなわち、言語モデルがあれば良い、ということになります。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://aclanthology.org/N01-1021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hale (2001)&lt;/a&gt; では、言語モデルとして、確率的文脈自由文法（Probabilistic Context-Free Grammar, PCFG）によるものを採用しました。
PCFGは、文脈自由文法における各文法規則に生起確率を割り当てたものです。詳しくは、自然言語処理系においてとりあえず最初に参照すべき文献である &lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/C.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jurafsky and Martin (2025, Appendix C)&lt;/a&gt; を参照ください。&lt;/p&gt;
&lt;p&gt;$P(w_n\mid w_1,\dots,w_{n-1})$ は、条件付き確率の定義により、
\begin{equation}
P(w_n\mid w_1,\dots,w_{n-1}) = \frac{P(w_1 \cdots w_{n-1}, w_n)}{P(w_1 \cdots w_{n-1})}
\end{equation}
と分解することができます。&lt;/p&gt;
&lt;p&gt;PCFGは、文脈自由性により、ある文字列に対する可能なすべての木構造は互いに排反なので、以下が成り立ちます：
\begin{equation}
P(w_1 \cdots w_n) = \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T, w_1 \cdots w_n)
\end{equation}&lt;/p&gt;
&lt;p&gt;また、PCFGは木構造と文字列の生成モデルであり、木構造 $T$ に対して文字列 $w_1\cdots w_n$ が一意に定まるので、
\begin{equation}
P(w_1 \cdots w_n\mid T) = 1
\end{equation}
が成り立ちます。&lt;/p&gt;
&lt;p&gt;よって、ある文字列 $w_1\cdots w_n$ の確率は、その文字列に対するすべての可能な木構造の確率の和に等しくなります：
\begin{align}
P(w_1 \cdots w_n) &amp;amp;= \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T, w_1 \cdots w_n) \newline
&amp;amp;= \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T)\cdot P(w_1 \cdots w_n\mid T) \newline
&amp;amp;= \sum_{T\in\mathcal{T}(w_1 \cdots w_n)} P(T)
\end{align}&lt;/p&gt;
&lt;p&gt;つまり、サプライザル $-\log P(w_n\mid w_1,\dots,w_{n-1})$ は、
\begin{align}
-\log P(w_n\mid w_1,\dots,w_{n-1}) &amp;amp;= -\log\frac{P(w_1 \cdots w_{n-1}, w_n)}{P(w_1 \cdots w_{n-1})} \newline
&amp;amp;= -\log\sum P(n\text{単語までの木構造}) \newline
&amp;amp;\quad  - \log\sum P(n-1\text{単語までの木構造})
\end{align}
と、$n$ 単語時点での構造に関する情報量と $n-1$ 時点での構造に関する情報量の差、言い換えると、その単語により構造に関してどれだけ情報が得られたのか、を表す値として説明されます（解説として、&lt;a href=&#34;https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12196&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hale (2016)&lt;/a&gt; も参照ください）。&lt;/p&gt;
&lt;p&gt;Hale (2001) でPCFGが採用されたのは、$n$-gram 言語モデルでは（$n$ がとりわけ大きいわけではなければ）単語間の依存関係を正しく取り扱えないからであり、さらにはそもそも著者自身の興味が文法構造の処理にあったからだと思われるのですが、それにより、シンボリックで離散的な文法理論を、連続的で柔軟な情報理論を通して、処理負荷の予測につなげられています。&lt;/p&gt;
&lt;p&gt;* そのため、&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hale (2006)&lt;/a&gt; では、PCFG ではなく、linguistically-motivated な Minimalist Grammar を用いてモデリングを行っています。構造への確率が付与できさえすれば良いので、そういった拡張が容易にできます。&lt;/p&gt;
&lt;p&gt;これに対し、&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027707001436&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Levy (2008b)&lt;/a&gt; では、特定の構造・意味の形を前提としない形でのサプライザルの解釈を提示しました。&lt;/p&gt;
&lt;p&gt;アイディアは非常にシンプルで、$n$ 時点での単語 $w_n$ のサプライザルを、　$n-1$ 時点までの意味 $T\in\mathcal{T}$ の確率分布から $n$ 時点での意味の確率分布へのKL距離（Kullback–Leibler divergence）、すなわち、単語 $w_n$ の入力により、意味に関する確率分布がどれだけ変化するのかを示す値である、と示しました。
これは、先ほどの Hale (2001) でのサプライザルの導出過程と同じで、$P(w_1,\cdots,w_n\mid T) = 1$ 、という仮定を置いた場合に示すことができます：
\begin{align}
&amp;amp;D_{\textit{KL}}(p(T\mid w_1\cdots w_{n})\parallel p(T\mid w_1\cdots w_{n-1})) \newline
&amp;amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{p(T\mid w_1\cdots w_n)}{p(T\mid w_1\cdots w_{n-1})} \newline
&amp;amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{\frac{p(w_n\mid T, w_1\cdots w_{n-1})p(T\mid w_1\cdots w_{n-1})}{p(w_n\mid w_1\cdots w_{n-1})}}{p(T\mid w_1\cdots w_{n-1})} \newline
&amp;amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{\frac{p(T\mid w_1\cdots w_{n-1})}{p(w_n\mid w_1\cdots w_{n-1})}}{p(T\mid w_1\cdots w_{n-1})} \newline
&amp;amp;= \sum_{T\in\mathcal{T}} p(T, w_1\cdots w_n)\log\frac{1}{p(w_n\mid w_1\cdots w_{n-1})} \newline
&amp;amp;= \log\frac{1}{p(w_n\mid w_1\cdots w_{n-1})} \newline
&amp;amp;= -\log p(w_n\mid w_1\cdots w_{n-1})
\end{align}&lt;/p&gt;
&lt;p&gt;言語モデルが妥当に単語の確率分布を推定できるのならば、その言語モデルが計算するサプライザルは、背後の意味・構造に関する分布を考慮に入れられているよ、ということが示されているのは非常に面白いところです。&lt;/p&gt;
&lt;p&gt;しかしながら、ではなぜ現在の大規模言語モデルによるサプライザルが人間の読み時間・神経活動を（かなり説明はできているが）説明しきれていないのでしょうか。
おそらくは、人間の作業記憶は大きくない、という制約や、確率分布等の計算をすること自体のコストが考慮されていないからでしょう。
Hale (2001) では $T$ で周辺化された文字列に関する周辺分布を、Levy (2008b) では、$T$ に関する確率分布を用いてサプライザルを説明していますが、人間が、こういった分布を全部ちゃんと記憶して扱えるのかは謎です。おそらく絶対に分布を端から端までは記憶していないでしょう。次に調べるべきところは、分布を想定するという理想化により現実とどのくらい差異が生じうるのか、ということでしょう。
また、（これは些細であるのかどうかの判断もついていないのですが）$P(w_{1,\cdots,n}\mid T)=1$ という仮定は問題ないのでしょうか。真理条件的には同じ意味だが、語順・構造が違う文は作れるし、語彙も「同じ意味っぽい」形が異なる組み合わせはたくさんあるが、とは思います。&lt;/p&gt;
&lt;h3 id=&#34;言語モデルの性能-perplexity-との関係&#34;&gt;言語モデルの性能 (perplexity) との関係&lt;/h3&gt;
&lt;p&gt;言語モデルの次単語予測性能を表す（単語ごとの）perplexityは、その値が低いほど「テストデータにおける次単語予測が正確である」ことを表すのですが、一時期は、「言語モデルのperplexityが低いほど、そのモデルによって算出したサプライザルの、読み時間の予測精度が高い」ということが主張されていました &lt;a href=&#34;https://aclanthology.org/W18-0102/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Goodkind and Bicknell, 2018)&lt;/a&gt;。
しかし、次第に、&lt;a href=&#34;https://aclanthology.org/2023.tacl-1.20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oh and Schuler (2023)&lt;/a&gt; や &lt;a href=&#34;https://www.pnas.org/doi/10.1073/pnas.2307876121&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shain et al. (2024)&lt;/a&gt; など、「言語モデルの規模が大きくなり、perplexityがさらに低くなっていくと、むしろ読み時間予測精度が下がる」という傾向が観察されているようです。
確かに、人間以上に次単語予測ができるモデルは、人間の反応時間をunderestimateしてしまうでしょう。とはいっても、大規模言語モデルのサプライザルは人間の反応についてかなりの程度説明できているのは確かです。&lt;/p&gt;
&lt;p&gt;* 言語モデル $q$ のperplexityとは、
\begin{equation}
b^{-\frac{1}{|\text{test data}|}\sum_{\text{test data}}\log_b q(x)}
\end{equation}
で、要は $b$ のテストデータにおける平均サプライザル（クロスエントロピー）乗です。$b$ は底 (base) で、一貫していればなんでもいいです。&lt;/p&gt;
&lt;h3 id=&#34;サプライザルと処理負荷の関係は線形か非線形か&#34;&gt;サプライザルと処理負荷の関係は線形か、非線形か&lt;/h3&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;h2 id=&#34;lossy-context-surprisal&#34;&gt;Lossy-context surprisal&lt;/h2&gt;
&lt;p&gt;前項でのサプライザルの定義では、それまでの文脈をフルに使って求められた当該単語の生起確率が用いられていました。
一方で、心理言語学の文処理研究では、昔から、人間は作業記憶に極めて制限があり、記憶にまつわる処理の負荷があることが知られていました。
この&lt;strong&gt;予測に関する処理負荷&lt;/strong&gt;と&lt;strong&gt;記憶に関する処理負荷&lt;/strong&gt;は、要は文処理に関する2大処理負荷で、ずっと別々に研究が進んでいたのですが、&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1111/cogs.12814&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Futrell et al. (2020)&lt;/a&gt; では、サプライザルの定義を少しだけ変更した lossy-context surprisal (LCS) を提案し、この2つを統合しました（これはほんとうにすごいと思っています）。
アイディアはものすごいシンプルで、文脈 $c = w_{1,\cdots, n-1}$ をフルに使用せず、文脈 $c$ をもとにした記憶表象 $m$ を用いて、単語 $w$ の生起確率を求める、というものです：
\begin{align}
\text{LCS}(w\mid c) &amp;amp;:= \mathbb{E}_{m\sim p(m\mid c)} [-\log p(w\mid m)] \newline
&amp;amp;= -\sum_m p(m\mid c)\log p(w\mid m)
\end{align}&lt;/p&gt;
&lt;p&gt;この定式化は非常に一般的なものであり、例えば記憶表象 $m$ がどのような形なのか、どのようにして生成されるのか、といったことは特に指定されていません。
ということは、この形を明らかにする、という研究は求められるはずで、例えば 単純に文字列としての $c$ と $m$ の関係なのかもしれないし、意味（構造）として $m$ を扱った方が実際の人間がやっていることに近いのかもしれません。
記憶に関する処理については、これまで、&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0010027798000341&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gibson (1998)&lt;/a&gt; や &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_25&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lewis and Vasishth (2005)&lt;/a&gt;、&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027724000520&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Isono (2024)&lt;/a&gt; など、何らかの文法理論を用いてモデル化がなされてきており、人間の記憶単位（チャンク）は文法構造を考えると良さそうかも、という状況です。
なので、これらを組み合わせることはできるし、そして人間の文処理のモデル化として有効でしょう、ということは期待できます。
一方で、「こうしたモデル化をしないと説明できないこと」みたいなものを私は思いついていないので、特にやるモチベーションはまだないです。
少なくとも、Hale (2001) や Levy (2008b) が考えていたような定式化において、分布全体を把握できているのは理想化のしすぎに思えるので、それが少し緩和される、というのは良いことかもしれません。&lt;/p&gt;
&lt;!-- ### resource-rational LCS
hoge --&gt;
&lt;h2 id=&#34;noisy-channel-model&#34;&gt;Noisy-channel model&lt;/h2&gt;
&lt;p&gt;人間の文処理研究において、多くの場合、人間は見聞きした文を正確に知覚できていることが前提とされていたりしますが、もちろん常にそんなことはなく、実際には聞き手は、話し手の発話を一部聞き逃したり、誤って聞き取っている場合もあります。
しかしながら、だからといって聞き逃して終わり、というわけではなく、聞き手は何やかんやうまく話し手の意図を推定できたりしています。
このような人間の処理過程を、&lt;a href=&#34;https://aclanthology.org/D08-1025/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Levy (2008a)&lt;/a&gt; や &lt;a href=&#34;https://www.pnas.org/doi/10.1073/pnas.1216438110&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gibson et al. (2013)&lt;/a&gt; は、noisy-channel model（雑音のある通信路モデル）を用いてモデル化しました。&lt;/p&gt;
&lt;p&gt;ここでは簡単に Gibson et al. (2013) による表記を用いて紹介します。
\begin{equation}
s_i \overbrace{\longrightarrow}^{\text{noisy channel}} s_p
\end{equation}
話し手が意図した発話 $s_i$ が、聞き手が実際に知覚した発話 $s_p$ をもとに推定される確率 $P(s_i\mid s_p)$ は、
\begin{equation}
P(s_i\mid s_p)=\frac{P(s_p\mid s_i)P(s_i)}{\sum_{s_j\in\mathcal{S}}P(s_p\mid s_j)P(s_j)}\propto P(s_p\mid s_i)P(s_i)
\end{equation}
で求められます。&lt;/p&gt;
&lt;p&gt;ようは、$P(s_p\mid s_i)$ という noise が入る確率と、そもそもありえる発話 $s_i$ の確率 $P(s_i)$ のバランスをとって、 $s_i$ を合理的に推定する、ということです。
Gibson et al. (2013) の例を使うと、&lt;em&gt;The mother gave the candle the daughter.&lt;/em&gt; という文は、&lt;em&gt;the candle&lt;/em&gt; に &lt;em&gt;the daughter&lt;/em&gt; を与えたという意味的に不自然な文（つまり、$P(s_p)$ は低い）が、名詞同士の間に &lt;em&gt;to&lt;/em&gt; を入れ忘れた可能性は高そう（$s_i$ が &lt;em&gt;The mother gave the candle to the daughter.&lt;/em&gt; と考えたとき、$P(s_p\mid s_i)$ の確率は高そう）なので、おそらく多くの人は、文字通りではない意味 &lt;em&gt;The mother gave the candle to the daughter.&lt;/em&gt; で解釈しそう。
一方で、&lt;em&gt;The girl was kicked by the ball.&lt;/em&gt; は、意味的に不自然で $P(s_p)$ の確率は低そうなのだが、他の意味的に妥当な文を探してきても、$P(s_p\mid s_i)$ が $P(s_p)$ を超えることはあまりなさそう（これは事前確率と尤度をどう計算するか次第ではあるが）なので、文字通りの解釈が取られやすい、と考えられます。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://oecs.mit.edu/pub/y1uhdz0y/release/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ferreira and Ferreira (2024)&lt;/a&gt; では、noisy-channel model が good-enough model に似ている、と説明していますが、むしろ noisy-channel model はちゃんと予測がたつ理論、という意味ではより強力なものだとは思います。&lt;/p&gt;
&lt;p&gt;個人的には、Competence $c$ とPerformance $p$ の間の関係も、まさにnoisy channelで考えることができる、と思っていています。
\begin{equation}
c \overbrace{\longrightarrow}^{P(c\mid p)} p
\end{equation}&lt;/p&gt;
&lt;!-- 我々は $p$ しか観測できないですが、$P(p\mid c)$ や $P(c)$ について考えることができます。 --&gt;
&lt;p&gt;例えば、$P(c)$ はオッカムの剃刀的な、シンプルな理論ほど高い確率を付与する、というふうにして表現できますし、$P(p\mid c)$ は、competence $c$ によって performance $p$ をどれだけ説明できるのか、ということで、まさに理論言語学者や心理言語学者が、容認性判断や読み時間、神経活動データなどをもとに判断していることでしょう。概念的には、
\begin{equation}
P(p) = \sum_c P(p\mid c)P(c)
\end{equation}
の $P(p)$ と実際の観測との誤差が最小になるような $P(p\mid c)$ を探す、ということをしているはずです。&lt;/p&gt;
&lt;p&gt;そして、$P(p\mid c)$ や $P(c)$ を定めることができれば、$P(c\mid p)$ が推測できます：
\begin{equation}
P(c\mid p)=\frac{P(p\mid c)P(c)}{\sum_{c\in\mathcal{C}}P(p\mid c)P(c)}\propto \underbrace{P(p\mid c)}_{\text{現象への説明}}\overbrace{P(c)}^{\text{文法のシンプルさ}}
\end{equation}&lt;/p&gt;
&lt;p&gt;つまり、$P(c\mid p)$ は、competence自体の単純さとそれによる説明力の高さのトレードオフにより求められます。
そして、最も適切な理論 $\hat{c}$ は、事後確率が最大となるもの、とすることができます：
\begin{equation}
\hat{c}=\arg\max_{\hat{c}\in C} P(p\mid c)P(c)
\end{equation}&lt;/p&gt;
&lt;p&gt;尤度と事前確率をそれぞれ「記述的妥当性」と「説明的妥当性」として読むと、これはまさに &lt;a href=&#34;https://www.jstor.org/stable/j.ctt17kk81z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chomsky (1965)&lt;/a&gt; による competence-performance の区別における議論でしょう。
記述的妥当性と説明的妥当性のトレードオフは、 &lt;a href=&#34;http://mitwpl.mit.edu/catalog/mwpl77&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Berwick (2015)&lt;/a&gt; が最小記述長 (minimum description length) を用いて同様の議論をしています。&lt;/p&gt;
&lt;p&gt;ただ、このnoisy-channel model は非常に一般的なことを述べているに過ぎないので、Chomsky による competence-performance をより広く解釈できます。
50-60年代は容認性判断くらいしか $P(p\mid c)$ の議論をする場所がなかったので、Chomsky の議論は文法現象の「記述」がメインでしたが、心理言語学研究の進展により、$P(p\mid c)$ に対して、読み時間や脳活動データを使った検証が可能です。&lt;/p&gt;
&lt;p&gt;また、&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027715000815&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kirby et al. (2015)&lt;/a&gt; での文法の文化進化モデルも、尤度を「意図の伝達成功確率」と、事前確率を「文法の学習可能性」として扱っているもの、と見ることができると思います。&lt;/p&gt;
&lt;p&gt;とはいっても、Chomsky の50-60年代の直感は、いまになって読み返してみても、（Shannon の研究を必要以上に叩いているようにみえるところ以外は）非常に有用なものが多いように思えます。
ちゃんと整理すると、$P(c),\ P(p\mid c),\ P(c\mid p)$ それぞれについて考えようとする、ということがまっとうな Chomskian だと思っているのですが、どうでしょうか。
そういう意味で、個人的には、formal な文法理論を用いた文処理の研究 ($P(p\mid c)$) と、$P(c\mid p)$ に直接取り組む効率的なコミュニケーション研究（後述）をやっています。学習可能性 ($P(c)$) に関する研究もしてみたいとは思っています。&lt;/p&gt;
&lt;p&gt;* もしくは、$c$ と $p$ の間に noise が入りえない、プロの言語学者ならば noise なしで自身の $c$ にアクセスできる、と考えるのも、それが実現可能かはおいておいて、筋は通っているとは思います。そういう考え方に基づくと、$P(p\mid c)$ なんてものは考える必要はなくなります（では、心理言語学研究は何をやっていることになるのでしょうか）。&lt;/p&gt;
&lt;h1 id=&#34;効率的なコミュニケーション-efficient-communication&#34;&gt;効率的なコミュニケーション (Efficient Communication)&lt;/h1&gt;
&lt;p&gt;確率や情報理論により、言語使用や人間のコミュニケーションにおける cost/benefit を心理的に妥当な指標で評価できるようになってきました。
また、計算機上では、「現実には存在しないがありえたかもしれない仮想的な言語」を作り出すことができるので、言語に見られる構造・規則が言語使用において明らかに有利なものであるのか、といったことを検証することが可能です。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;効率的なコミュニケーション仮説&lt;/strong&gt; (Efficient Communication Hypothesis) では、言語構造が効率的なコミュニケーションを実現するように形作られてきた、と考え、言語に存在する普遍性 (universalities) や 強い統計的傾向 (strong statistical tendencies) が&lt;strong&gt;なぜ&lt;/strong&gt;あるのか、ということを言語使用に帰して説明しようとしています。&lt;/p&gt;
&lt;!-- 近年、この「効率性」を厳密に定義し、心理的に「妥当」に定量化する方法論が確立されてきたことで、研究が進みつつあります。 --&gt;
&lt;p&gt;おすすめレビュー論文、本：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.126&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jaeger and Tily (2011)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;心理言語学研究からはじめて typology にまで話を広げましょう、というレビュー論文&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1146/annurev-linguistics-011817-045406&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kemp et al. (2018)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;特に lexicon (semantic categorization) に関する研究を牽引してきた著者たちによる Annual Review of Linguistics のレビュー論文&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1364661319300580&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gibson et al. (2019)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;心理言語学の立場から、単語の長さや色の語彙、語順について処理の効率性の観点で説明してきた著者らによるレビュー論文。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fcomm.2022.657725/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Futrell and Hahn (2022)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Gibson et al. (2019) に加え、さらに情報理論それ自体の説明やその応用について解説しているレビュー論文。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cambridge.org/core/books/communicative-efficiency/F5AA238FB82B9739592CFAC62BF89708&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Levshina (2022)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;個別具体的な言語現象にそれぞれ注目して言語の効率性について論じた本。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;* なお、ここでの「コミュニケーション」とは、文化・社会的要因等を考慮したり、ジェスチャーなどの非言語的情報を考慮する、といったようなレベルにまではまだ達しておらず、より単純に単語や句、文の伝達を指します。
言語学や心理学、認知科学で扱われてきた、文レベルを超えたコミュニケーションについては、まだまだこれからです。&lt;/p&gt;
&lt;h2 id=&#34;単純性と情報伝達性のトレードオフ&#34;&gt;単純性と情報伝達性のトレードオフ&lt;/h2&gt;
&lt;p&gt;コミュニケーションが効率的であるとは、人間の認知能力の制約のもと、「意図・情報の伝達が最大化されている一方、産出や理解といった使用のコストが最小化されている状況」のことを指す、という表現を私は使います。
つまり、できる限り informative でありかつできる限り simple である状況が効率的であります。&lt;/p&gt;
&lt;p&gt;この情報伝達性 (&lt;em&gt;informativeness&lt;/em&gt;) と単純性 (&lt;em&gt;simplicity&lt;/em&gt;) にはトレードオフ関係があり、つまり、どちらか一方だけを高めるともう一方は低くなってしまう、という関係であり、自然言語は、このトレードオフのもとで（ほとんど）最適解である側面が数々観察されています。&lt;/p&gt;
&lt;p&gt;「最適解」とは、お気持ちレベルの説明すると、「片方の軸における値をより良い方に持っていくと、どうしてももう片方の軸における値が悪い方に行ってしまう点」のことを指します。
そういった点は1つに定まるわけではなく、下の図のように曲線を描くはずです。
この最適解のことをパレート解と、そしてパレート解を結んでできる曲線のことをパレートフロンティアといいます。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/250303information/imageoftradeoff_hu4786304544351349382.webp 400w,
               /github-pages/blogs/250303information/imageoftradeoff_hu3424281975557398783.webp 760w,
               /github-pages/blogs/250303information/imageoftradeoff_hu10939622207393665217.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/250303information/imageoftradeoff_hu4786304544351349382.webp&#34;
               width=&#34;760&#34;
               height=&#34;609&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;* &lt;a href=&#34;&#34;&gt;『自然言語処理』の学会記事 (to appear)&lt;/a&gt; でも書いたのですが、&lt;em&gt;simplicity&lt;/em&gt; と &lt;em&gt;informativeness&lt;/em&gt; という用語は専門用語として固まってきてしまっているようなのですが、その意味するところが伝わりにくい表現であります。
私自身は &lt;em&gt;informative&lt;/em&gt; という単語の語感がそもそもわからないので何とも言い難いですが、そもそも「コミュニケーションの効率性」をどういった概念・シナリオで考えるのか、そしてそれをどういった表現で言い表すのか、といったことの整理は、今後時間をかけてなされていくものなのでしょうかね。トレードオフの概念をもっとも抽象的に（それゆえに正確に？）表すとすると、&lt;em&gt;complexity/accuracy&lt;/em&gt; とかでしょうか。それか、&lt;em&gt;cost/benefit&lt;/em&gt; とか、&lt;em&gt;cost/reward&lt;/em&gt; とかでしょうか。そして、こうした抽象的な概念を、人間の言語処理の問題として表現するとなると、&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027711002496&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Piantadosi et al. (2012)&lt;/a&gt; で使われていた、&lt;em&gt;ease/clarity&lt;/em&gt; がいいですよね。とは思っています。&lt;/p&gt;
&lt;p&gt;2つの相反する圧力のもとでの最適化、として言語を説明する研究は、&lt;a href=&#34;https://psycnet.apa.org/record/1950-00412-000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zipf (1949)&lt;/a&gt; や &lt;a href=&#34;https://psycnet.apa.org/record/1995-97959-000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hawkins (1994)&lt;/a&gt;、&lt;a href=&#34;https://benjamins.com/catalog/la.132.04has?srsltid=AfmBOopYpsq--9vlEPxMDdbyNompli6t4Jw79TmooUgYmoaNU1l7YsCG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haspelmath (2008)&lt;/a&gt; など、言語学者の間でも主張されてきたものですが、効率的なコミュニケーション研究は、それを理論中立的な情報理論のことばを使って書き下すことにより、定量的な研究を実現するだけでなく、その適用先・規模を拡大させることができた、という点が大きな進展だったのだと思っています。
特に語彙 (wordformやsemantic categorization) や、最近では文レベルについても研究が進んできています：&lt;/p&gt;
&lt;p&gt;語彙に関して：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;親族名称: &lt;a href=&#34;https://www.science.org/doi/abs/10.1126/science.1218811&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kemp and Regier (2012)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;色: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.0610341104&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regier et al. (2007)&lt;/a&gt;, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118346136.ch11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regier et al. (2015)&lt;/a&gt;, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1619666114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gibson et al. (2017)&lt;/a&gt;, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1800521115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zaslavsky et al. (2018)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;数: &lt;a href=&#34;https://doi.org/10.1162/opmi%5c_a%5c_00034&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Xu et al. (2020)&lt;/a&gt;, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13424&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denic and Szymanik (2024)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;文法標識 (number, tense, evidentiality): &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.2025993118&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mollica et al. (2021)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;量化子: &lt;a href=&#34;https://www.mdpi.com/1099-4300/23/10/1335&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steinert-Threlkeld (2021)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人称代名詞: &lt;a href=&#34;https://escholarship.org/uc/item/2sj4t8m3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zaslavsky et al. (2021)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;不定代名詞: &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denic et al. (2022)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Boolean connectives: &lt;a href=&#34;https://doi.org/10.1162/ling%5c_a%5c_00461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Uegaki (2022)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spatial demonstratives: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027723001397&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chen et al. (2023)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;単語の長さ: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1012551108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Piantadosi et al. (2011)&lt;/a&gt;, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12689&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mahowald et al. (2018)&lt;/a&gt;, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.2406971121&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Xu et al. (2024)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zipf則: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.0335980100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ferrer i Cancho and Sole (2003)&lt;/a&gt;, &lt;a href=&#34;https://doi.org/10.1140/epjb/e2005-00340-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ferrer i Cancho (2005)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zipf&amp;rsquo;s meaning-frequency law: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027711002496&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Piantadosi et al. (2012)&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027722000828&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trott and Bergen (2022)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;文法に関して：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合成性（構成性）: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027715000815&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kirby et al. (2015)&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2405.12109&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Futrell and Hahn (2024)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Greenbergの語順普遍: &lt;a href=&#34;https://www.pnas.org/doi/10.1073/pnas.1910923117&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hahn et al. (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;等位接続における構造依存性: &lt;a href=&#34;https://aclanthology.org/2024.conll-1.23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kajikawa et al. (2024)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;効率的なコミュニケーション研究について、実際に情報理論の道具を使ってどう仮説を検証するか、代表的な研究を紹介しながら説明します。&lt;/p&gt;
&lt;h3 id=&#34;ferrer-i-cancho-and-sole-2003&#34;&gt;Ferrer i Cancho and Sole (2003)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.0335980100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ferrer i Cancho and Sole (2003)&lt;/a&gt; では、世界中の言語に見られるZipf則 (Zipf&amp;rsquo;s law) (&lt;a href=&#34;https://www.routledge.com/The-Psycho-Biology-Of-Language-AN-INTRODUCTION-TO-DYNAMIC-PHILOLOGY/ZipfGeorgeKingsley/p/book/9781138875098?srsltid=AfmBOopeIeJMNPikd6dTWUgyGseCgJeRgqiKdyvD5JXpZ5Qxcuzrgx3m&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zipf, 1936&lt;/a&gt;; &lt;a href=&#34;https://psycnet.apa.org/record/1950-00412-000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1949&lt;/a&gt;) が、効率的なコミュニケーションの実現の結果として生まれるものなのかを検証するため、コミュニケーションを効率的にしたときにZipf則が立ち現れるのかシミュレーション実験をしました。
この研究は、効率的なコミュニケーション研究の先駆けであるので紹介します。&lt;/p&gt;
&lt;p&gt;まず、前提として、Zipf則とは、単語頻度に関する経験則で、全体で$k$番目に多く使用される単語の頻度 $f(k)$ は、$f(k)=C\cdot k^{-\alpha}$ と冪乗則に従う、というものです。要は、よく使われる単語はめちゃくちゃ使われるし、珍しい単語は本当に珍しい、というものです。
$C$ は比例定数で、$\alpha$ は &lt;a href=&#34;https://psycnet.apa.org/record/1950-00412-000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zipf (1949)&lt;/a&gt; では $1$ です。
両辺に対数を適用すると、$\log f(k) = -\alpha\log k + \log C$ と線形な関係になります。&lt;/p&gt;
&lt;p&gt;実際の頻度と頻度ランクの関係を見たら、式の意味はすぐにわかります。
ちょうど手元に&lt;a href=&#34;https://github.com/UniversalDependencies/UD_Japanese-BCCWJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UD_Japanese-BCCWJ&lt;/a&gt;のv2.10、国語研長単位 (LUW) 分割があったので、これの単語頻度と頻度ランクを数えてみました。
全部で57,109文、995,632単語（長単位）で、&lt;em&gt;x&lt;/em&gt;軸を頻度ランク、&lt;em&gt;y&lt;/em&gt;軸を実際の頻度としてプロットしたのが左図、そして両軸に $\log_{10}$ を適用してプロットしたものが右図です。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/250303information/bccwj_zipf_hu6585653125791012963.webp 400w,
               /github-pages/blogs/250303information/bccwj_zipf_hu9883852135442870022.webp 760w,
               /github-pages/blogs/250303information/bccwj_zipf_hu10125593117757427291.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/250303information/bccwj_zipf_hu6585653125791012963.webp&#34;
               width=&#34;760&#34;
               height=&#34;378&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;右図については、最小二乗法での回帰直線も引いてみました。
$\alpha = 1$ とはなっていないようですが、ある程度はZipf則に従っているようにみえます。&lt;/p&gt;
&lt;p&gt;* 直線から逸脱している部分があることや、傾きの係数が $0.94$ であることを「Zipf則に従っている」とみるか、は結構重要な問題な気はします。
詳しくは &lt;a href=&#34;https://link.springer.com/article/10.3758/s13423-014-0585-6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Piantadosi (2014)&lt;/a&gt; をお読みください。&lt;/p&gt;
&lt;p&gt;では、（一旦単語分布はZipf則に従っているとして）なぜ単語分布はZipf則に従うのでしょうか。
&lt;a href=&#34;https://psycnet.apa.org/record/1950-00412-000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zipf (1949)&lt;/a&gt; は、言語は、省エネでありたいという話し手の要求と、意図・情報の復元が容易でありたいという聞き手の要求のトレードオフのもとで形作られているのだという仮説を提案しました。
具体例として、言語を「単語」と「意味」の対応関係と考えてみましょう。
すべての意味をたった一つの単語で表現する言語があったら、話し手は覚えるべき単語が一つだけになるため、話者の負担は最小限になります。
しかし、このような言語では、単語がどの意味を指すのか曖昧になり、聞き手が話者の意図を理解するのに困難が生じます。
一方で、すべての単語が明確に異なる意味を表現するシステムでは、聞き手の理解（意図推定）は容易になりますが、話し手の負担は増加します。
Zipf は、言語はこうした話し手と聞き手の相反する要求のバランスによって形作られているのだと主張しました。&lt;/p&gt;
&lt;p&gt;では、それは本当か？と計算機上で実験してみたのが、Ferrer i Cancho氏 と Sole氏 の研究です。&lt;/p&gt;
&lt;h4 id=&#34;モデル&#34;&gt;モデル&lt;/h4&gt;
&lt;p&gt;彼らは、上述した Zipf の説明を検証するため、信号（単語）と意味の対応関係について、話し手と聞き手双方のコストがトレードオフのもとで最小となるような関係になるまで進化アルゴリズムを使って推定する、ということを行いました。&lt;/p&gt;
&lt;p&gt;$n$ 個のシグナル $\mathcal{S} = {s_1,&amp;hellip;,s_i,&amp;hellip;,s_n}$ と
$m$ 個の意味（objects of reference）$\mathcal{R} = {r_1,&amp;hellip;,r_i,&amp;hellip;,r_m}$ を考えたとき、それらの間の関係をバイナリ行列 $\mathbf{A} = {a_{ij}}\ (1\leq i\leq n,\ 1\leq j\leq m)$ で表します。&lt;/p&gt;
&lt;p&gt;バイナリ行列は、すべて $0$ か $1$ で構成されている行列で、ここでは、ある $i$ 番目のシグナル $s_i$ が $j$ 番目の意味 $r_j$ を表している場合、行列内の要素 $a_{ij}$ は $1$ と、そうでないならば $0$ と表現します。&lt;/p&gt;
&lt;p&gt;いま、synonym（同義語）があるなら、シグナルと意味の確率分布の関係は以下になります：
\begin{equation}
P(s_i) = \sum_j P(s_i,r_j)
\end{equation}&lt;/p&gt;
&lt;p&gt;なお、$P(r_i) = 1/m$ と仮定。&lt;/p&gt;
&lt;p&gt;ベイズの定理より、
\begin{equation}
P(s_i,r_j) = P(r_j)P(s_i\mid r_j)
\end{equation}
であり、$P(s_i\mid r_j)$ は、
\begin{equation}
P(s_i\mid r_j) = a_{ij}\frac{1}{\omega_j}.
\end{equation}
で定義します。
$\omega_i = \sum_j a_{ij}$ は意味 $r_j$ における同義語の数です。&lt;/p&gt;
&lt;p&gt;代入すると、
\begin{equation}
P(s_i,r_j) = a_{ij}\frac{P(r_j)}{\omega_j}
\end{equation}&lt;/p&gt;
&lt;p&gt;話し手と聞き手のコストは、それぞれ平均情報量（エントロピー）で定義します：
\begin{align}
H(S) &amp;amp;= -\sum_i^n P(s_i)\log P(s_i) \newline
H(R\mid S) &amp;amp;= -\sum_i^n\sum_j^m P(s_i,r_j)\log P(r_j\mid s_i)
\end{align}&lt;/p&gt;
&lt;p&gt;そして最後に、話し手と聞き手のコストのトレードオフは、それぞれの加重和で表現します：
\begin{equation}
\Omega(\lambda)=\lambda H(S) + (1-\lambda)H(R\mid S)
\end{equation}
ここで、$0\leq\lambda\leq 1$ で、話し手と聞き手のコストのどちらを重視するかを表します。&lt;/p&gt;
&lt;p&gt;この $\lambda$ の値を変えながら、各 $\lambda$ において、$\Omega(\lambda)$ を最小化するようなバイナリ行列 $\mathbf{A}$ を進化アルゴリズムで求める、ということをします。&lt;/p&gt;
&lt;h4 id=&#34;再現コードと結果&#34;&gt;再現コードと結果&lt;/h4&gt;
&lt;p&gt;非常に単純なモデルなので、再現実装してみました（いくつか簡略化しています）。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/250303information/freq_rank_hu1559019237969630005.webp 400w,
               /github-pages/blogs/250303information/freq_rank_hu10479040455501117085.webp 760w,
               /github-pages/blogs/250303information/freq_rank_hu526095839200649385.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/250303information/freq_rank_hu1559019237969630005.webp&#34;
               width=&#34;691&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/250303information/log_freq_rank_hu2101583012652647821.webp 400w,
               /github-pages/blogs/250303information/log_freq_rank_hu9408570637108809242.webp 760w,
               /github-pages/blogs/250303information/log_freq_rank_hu4437273928768977922.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/250303information/log_freq_rank_hu2101583012652647821.webp&#34;
               width=&#34;691&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/250303information/mi_lexicon_size_hu12214877015639816266.webp 400w,
               /github-pages/blogs/250303information/mi_lexicon_size_hu16297562656130702712.webp 760w,
               /github-pages/blogs/250303information/mi_lexicon_size_hu9567190417810425306.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/250303information/mi_lexicon_size_hu12214877015639816266.webp&#34;
               width=&#34;760&#34;
               height=&#34;456&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;!-- https://neilkidd.gitlab.io/hugo-academic-lean/post/jupyter/ --&gt;
&lt;p&gt;最適化の結果として、$\lambda = 0.41$ あたりで、Zipf則に近い分布が得られることがわかりました。&lt;/p&gt;
&lt;p&gt;もちろん、目的関数が心理的に妥当ではないという批判はあります &lt;a href=&#34;https://link.springer.com/article/10.3758/s13423-014-0585-6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Piantadosi; 2014)&lt;/a&gt;。
具体的には、通信コードとしての複雑さ（＝エントロピー）が、人間の発話にとっての複雑さと一致しているのかは誰も示していない、ということと、すべての意味が等確率で現れるという前提は明らかに現実に則していない、ということです。
後者については、&lt;a href=&#34;https://link.springer.com/article/10.1140/epjb/e2005-00340-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ferrer i Cancho (2005)&lt;/a&gt; で提案されているように、意味の分布が一定ではない、と想定した上で、条件付きエントロピー $H(R\mid S)$ を負の相互情報量 $-I(S; R)$ に置き換えると少しは良くなるかもしれません。&lt;/p&gt;
&lt;p&gt;負の相互情報量は、
\begin{equation}
-I(S; R) = H(R\mid S) - H(R)
\end{equation}
であり、これまでの式は $H(R)$ が定数であることを想定したものだった、と考えられます。&lt;/p&gt;
&lt;!-- や、そもそも Zipf則は統計側の性質であって、言語はただそれに従っているだけでは ([田中 (2021)](https://www.utp.or.jp/book/b559376.html)) という主張もあります。 --&gt;
&lt;h3 id=&#34;zaslavsky-et-al-2018&#34;&gt;Zaslavsky et al. (2018)&lt;/h3&gt;
&lt;p&gt;コミュニケーションにおける最適解を求めるのに、Ferrer i Cancho and Sole (2003) のように進化アルゴリズムのような探索的な方法をとる、ということ以外に、&lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1800521115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zaslavsky et al. (2018)&lt;/a&gt; は、情報ボトルネック (information bottleneck) を用いて理論的な限界を求める、という方法論を提案しました。&lt;/p&gt;
&lt;p&gt;形や意味の確率分布さえちゃんと定められれば、使える方法なので、極めて強力です（特に語彙レベルだと、形と意味の集合を合理的に想定することができるので非常に有用です。文レベルだと、集合や分布を決める、というのにおいてまだ良い方法論が見つけられていない気がします）。&lt;/p&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;h2 id=&#34;rational-speech-act-rsa&#34;&gt;Rational Speech Act (RSA)&lt;/h2&gt;
&lt;p&gt;Rational Speech Act (RSA) モデル &lt;a href=&#34;https://www.science.org/doi/10.1126/science.1218633&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Frank and Goodman, 2012)&lt;/a&gt; は、その名の通り、「&lt;strong&gt;話し手&lt;/strong&gt;の合理的な発話」をモデル化したモデルです。
話し手が合理的であるとは、「話し手は、必ずしも全部を言ってくれるわけではないが（parsimoniousであるが）、何らかの意図を伝えようとできる限りinformativeな発話をする」ということです。&lt;/p&gt;
&lt;p&gt;こうした状況はGrice以来想定されてきたわけですが、RSAではその話し手の合理性を確率変数に対するutility関数の形で表現し、聞き手の語用論的推論 (pragmatic reasoning) はベイズ推定 (Beyesian inference) で定式化しました。&lt;/p&gt;
&lt;p&gt;話者の合理性を表すutility関数は、さまざまな亜種はあるものの、多くの場合、聞き手にとってinformativeである一方、自分にとってコストが低い発話、という、informativenessとaccessibility (cost) のトレードオフを表現したものが使われます。&lt;/p&gt;
&lt;p&gt;具体的には、意味 $m\in\mathcal{M}$ と発話 $u\in\mathcal{U}$ について、utility関数 $U(m,u)$ は、
\begin{equation}
U(m,u) = \log P_L(m\mid u) - C(u),
\end{equation}
ここで、$\log P_L(m\mid u)$ は聞き手にとってのinformativeさを、$C(u)\geq 0$ は発話それ自体のコストを表します。&lt;/p&gt;
&lt;p&gt;そして、実際の話し手の発話の確率は、このutility関数の値をもとに確率的に決まると考えると、ソフトマックス関数を通して求めることができます：
\begin{equation}
P_{S_t}(u\mid m) = \frac{e^{\alpha U_{t-1}(u,m)}}{\sum e^{\alpha U_{t-1}(u,m)}} \propto e^{\alpha U_{t-1}(u,m)},
\end{equation}
ここで、$\alpha$ は話し手がutilityにどれだけ重みを置いているかを表します。&lt;/p&gt;
&lt;p&gt;こうして話し手の合理的な発話がモデル化できると、聞き手の語用論的推論は、ベイズ推論によって表現することができます：
\begin{equation}
P_{L_t}(m\mid u) \propto P_{S_t}(u\mid m)P(m)
\end{equation}&lt;/p&gt;
&lt;p&gt;なお、添え字の $t$ や $t-1$ は、合理的な話し手が再帰的な思考の内部で想定した、話し手および聞き手モデルの埋め込みのレベルを指します。
要は、合理的な話し手は、「合理的な話し手ならこう言うだろう、ということを合理的な聞き手は推定するんだろうな、だからこういおうかな&amp;hellip;」という推論を再帰的に繰り返し続けることができ、毎度ひとつ前の埋め込みでの聞き手を想定したうえで発話選択をする、ということを繰り返している、ということです。&lt;/p&gt;
&lt;p&gt;ただし、もちろん、この再帰的な話し手と聞き手の関係は無限に考えることができるが、それだと推論がいつまでたっても終わらないので、通常は、1回埋め込んだところに、&amp;ldquo;literal listener&amp;rdquo; $P_{Lit}$ を想定します。&lt;/p&gt;
&lt;p&gt;literal listner は、語用論的推論は行わず、文字通り、意味の真偽を判断します：
\begin{equation}
P_{Lit}(m\mid u) \propto \delta_{[[u]]}(m) P(m),
\end{equation}
ここで、$\delta_{[[u]]}(\cdot)$ は、真偽値の $0, 1$ を返す関数です。
これにより、RSA frameworkにlexical semanticsやcompositional semanticsを組み込むこともできる、という主張です。&lt;/p&gt;
&lt;p&gt;もう一度流れを整理すると（埋め込みが1回の場合）、
$t$ における合理的な話し手の発話は、
\begin{equation}
U(m,u) = \log P_{L_{t-1}}(m\mid u) - C(u)
\end{equation}
というutility関数をもとに、
\begin{equation}
P_{S_t}(u\mid m) \propto e^{\alpha U_{t-1}(u,m)}
\end{equation}
でモデル化することができます。
ここで、utility関数内の $P_{L_{t-1}}$ をliteral listnerと考えると、
\begin{equation}
P_{Lit}(m\mid u) \propto \delta_{[[u]]}(m) P(m)
\end{equation}
で $P_{L_{t-1}}$ をモデル化できます。&lt;/p&gt;
&lt;p&gt;そうして、$t$ における聞き手の語用論的推論は、
\begin{equation}
P_{L_t}(m\mid u) \propto P_{S_t}(u\mid m)P(m)
\end{equation}
で求めることができます。&lt;/p&gt;
&lt;p&gt;RSA 関連でとりあえず参照すべきもの：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.science.org/doi/10.1126/science.1218633&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frank and Goodman (2012)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;RSAが提案された最初の論文&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S136466131630122X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodman and Frank (2016)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;RSAの提案者によるレビュー論文&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/jjsai/31/1/31_91/_article/-char/ja/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;折田 (2016)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;著者自身によるRSAを用いた指示表現選択の研究の解説&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.09867&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scontras et al. (2021)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;著者らによる ESSLLI 2016 の授業資料をもとにした解説記事も：&lt;a href=&#34;https://www.problang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scontras et al. (2016?)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.annualreviews.org/content/journals/10.1146/annurev-linguistics-031220-010811&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Degen (2023)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;2023年版の、Annual Review of Linguistics でのレビュー論文&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;uniform-information-density-uid&#34;&gt;Uniform Information Density (UID)&lt;/h2&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;!-- - operationalizationが複数あるが、どれが最も良いのか不明。
- &#34;Uniformity&#34;の定義があやふや --&gt;
&lt;!-- https://www.gakushuin.ac.jp/~881791/spm/2017/slides/tanaka.pdf --&gt;
&lt;!-- https://www.ieice-hbkb.org/files/01/01gun_01hen_03.pdf --&gt;
&lt;h2 id=&#34;dependency-length-minimization-dlm&#34;&gt;Dependency Length Minimization (DLM)&lt;/h2&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;h2 id=&#34;memory--prediction-trade-off&#34;&gt;Memory&amp;ndash;prediction trade-off&lt;/h2&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;h1 id=&#34;言語獲得&#34;&gt;言語獲得&lt;/h1&gt;
&lt;p&gt;確率モデル、というかベイズの定理を利用した言語学・認知科学研究で忘れてはならないのが、言語獲得のモデル化です（ベイジアンモデリングの解説については、&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661310001129&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Griffiths et al. (2010)&lt;/a&gt; や &lt;a href=&#34;https://oecs.mit.edu/pub/lwxmte1p/release/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Griffiths (2024)&lt;/a&gt; へ）。&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(h_i\mid d)=\frac{P(d\mid h_i)P(h_i)}{\sum_{h_j\in\mathcal{H}}P(d\mid h_j)P(h_j)}\propto P(d\mid h_i)P(h_i)
\end{equation}&lt;/p&gt;
&lt;p&gt;世の中にはベイズの定理に関する解説で満ち満ちているので適当に書きますが、簡単に説明すると、上式は、データ $d$ を受けたとき、仮説 $h_i\in\mathcal{H}$ をどれだけ支持するか、という確率 $P(h_i\mid d)$ は、その仮説 $h_i$ の事前確率 $P(h_i)$ と、その仮説 $h_i$ をもっていたときにデータ $d$ に出会う尤度（尤もらしさ）$P(d\mid h_i)$ の積でわかりますよ、ということです。&lt;/p&gt;
&lt;p&gt;ベイズの定理の嬉しさは、概念上は、生得主義 vs. 経験主義といった二項対立を超えて、言語はどのくらい学習可能か、どのくらいの生得知識 (inductive bias) が必要か、ということを定量的に評価することができるはず、というところでしょう、と、私は思っています。
これについて、&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chater and Manning (2006)&lt;/a&gt; で説明があるので、引用して紹介します。&lt;/p&gt;
&lt;p&gt;以下、&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661306001318&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chater and Manning (2006)&lt;/a&gt; の p.342より：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Oversimplifying somewhat, suppose that a learner wonders whether to include constraint $C$ in her grammar. $C$ happens, perhaps coincidentally, to fit all the data so far encountered. If the learner does not assume $C$, the probability that each sentence will happen to fit $C$ by chance is $p$. Thus, each sentence obeying $C$ is $1/p$ times more probable, if the constraint is true than if it is not (if we simply rescale the probability of all sentences obeying the constraint). Thus, after $n$ sentences, the probability of the corpus, is $1/p^n$ greater, if the constraint is included. Yet, a more complex grammar will typically have a lower prior probability. If the ratio of priors for grammars with/without the constraint is greater than $1/p^n$, then, by Bayes&amp;rsquo; theorem, the constraint is unlearnable in $n$ items.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ようは、ある制約 $C$ が生得知識としてあるべきなのか、それとも学習可能なのかは、制約 $C$ がある文法（仮に $G_C$）と制約 $C$ がない文法（仮に $G_{\neg C}$）の事前確率の比 $\frac{P(G_C)}{P(G_{\neg C})}$ と尤度比 $\frac{P(d\mid G_C)}{P(d\mid G_{\neg C})}$ を比べることで、事後分布の比 $\frac{P(G_C\mid d)}{P(G_{\neg C}\mid d)}$ を比べるのと同じこととなり、制約 $C$ が学習可能なのかを判定することができる、ということです。&lt;/p&gt;
&lt;p&gt;仮に、$G_{\neg C}$ を想定している人が、偶然にも $G_{C}$ に整合的な文 $s_C$ に出会う確率を $p$ とすると、$P(s_C\mid G_{\neg C}) = p$ です。
$P(s_C\mid G_{C}) = 1$ であるとすると、ある人が $G_{C}$ に整合的な文に $n$ 回出会ったとき、それぞれの文法を想定したときの尤度比は、$\frac{P(s_C\mid G_{C})^n}{P(s_C\mid G_{\neg C})^n} = \frac{1}{p^n}$ です。
このとき、それぞれの文法の事前確率 $P(G_C)$ と $P(G_{\neg C})$ の比が $\frac{1}{p^n}$ 分離れているのかどうかで、制約 $C$ が学習可能なのかどうか決まります。
基本的には、制約が一つ多い文法の方が複雑で事前確率がより小さくなっているはずだが、その小さい事前確率をもってもなお尤度により挽回可能なのか、ということです。&lt;/p&gt;
&lt;h3 id=&#34;xu-and-tenenbaum-2007&#34;&gt;Xu and Tenenbaum (2007)&lt;/h3&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;h3 id=&#34;perfors-et-al-2011&#34;&gt;Perfors et al. (2011)&lt;/h3&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;h3 id=&#34;abend-et-al-2017&#34;&gt;Abend et al. (2017)&lt;/h3&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;h3 id=&#34;yang-and-piantadosi-2022&#34;&gt;Yang and Piantadosi (2022)&lt;/h3&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;!-- 個人的に、[『自然言語処理』への解説記事]()でも書いたのですが、学習可能性について考えたいのですが、現在のACL系列の論文で大人気の「とりあえずニューラルモデルでの学習速度を見ました」という研究はさすがにコネクショニストすぎて私自身は乗っかれないと思っているものの、そうはいっても学習可能性については語りたいし、そうしたらニューラルを使うのが「出版において安全」な気がするし…という状況の中で、何とかベイジアンモデリングが使えないかと模索していました。

[Yang (2004)](https://www.sciencedirect.com/science/article/pii/S1364661304002104?via%3Dihub)の頃から指摘されているように、 --&gt;
&lt;h1 id=&#34;言語進化&#34;&gt;言語進化&lt;/h1&gt;
&lt;h2 id=&#34;繰り返し学習モデル-iterated-learning-model&#34;&gt;繰り返し学習モデル (Iterated Learning Model)&lt;/h2&gt;
&lt;p&gt;hoge&lt;/p&gt;
&lt;!-- - 文化進化による構成性の出現
    - [Kirby (2017)](https://link.springer.com/article/10.3758/s13423-016-1166-7) --&gt;
</description>
    </item>
    
    <item>
      <title>言語の起源はコミュニケーションか?：計算心理言語学の観点から</title>
      <link>https://kohei-kaji.github.io/github-pages/misc/%E8%A8%80%E8%AA%9E%E5%AD%A6%E3%83%95%E3%82%A7%E3%82%B9-2025/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/misc/%E8%A8%80%E8%AA%9E%E5%AD%A6%E3%83%95%E3%82%A7%E3%82%B9-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Is Structure Dependence Shaped for Efficient Communication? A Case Study on Coordination</title>
      <link>https://kohei-kaji.github.io/github-pages/talks/nlp-colloquium/</link>
      <pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/talks/nlp-colloquium/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Is Struncure Dependence Shaped for Efficient Communication? A Case Study on Coordination</title>
      <link>https://kohei-kaji.github.io/github-pages/international_conference/kajikawa-etal-2024-conll/</link>
      <pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/international_conference/kajikawa-etal-2024-conll/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dissociating Syntactic Operations via Composition Count</title>
      <link>https://kohei-kaji.github.io/github-pages/international_conference/kajikawa-etal-2024-cogsci/</link>
      <pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/international_conference/kajikawa-etal-2024-cogsci/</guid>
      <description></description>
    </item>
    
    <item>
      <title>日本語大規模読み時間コーパスにおける記憶の負荷のCCGによるモデリング</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/isono-etal-2024-lsj/</link>
      <pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/isono-etal-2024-lsj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>弱文脈依存文法にまつわる理論言語学のはなし</title>
      <link>https://kohei-kaji.github.io/github-pages/blogs/240322mcsg/</link>
      <pubDate>Fri, 22 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/blogs/240322mcsg/</guid>
      <description>&lt;script&gt;
  MathJax = {
    tex: {
      inlineMath: [[&#39;$&#39;, &#39;$&#39;]],
      displayMath: [[&#39;$$&#39;, &#39;$$&#39;], [&#39;\\[&#39;, &#39;\\]&#39;]]
    },
    options: {
      processHtmlClass: &#34;mathjax-process&#34;
    }
  };
&lt;/script&gt;
&lt;script async src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js&#34;&gt;&lt;/script&gt;
&lt;!-- 何らかの現象を包括的に記述し、それを一般化しようとする際、そうした現象がそもそもどれだけ「複雑」なのかを考えるのは極めて重要です。 --&gt;
&lt;p&gt;&amp;lt;工事中&amp;gt;&lt;/p&gt;
&lt;p&gt;自然言語の形態論・統語論を考えるとき、それらがどの程度「複雑」なのかを考えることは、言語理論を考えるのにも、人間の文産出・理解について考えるのにも、そして言語進化について考えるのにも極めて重要だと思います。
そして、自然言語の文法の「複雑さ」（以降、適当に「文法」といってしまいます）について、形式言語の世界では「生成力」という概念での整理がなされており、言語学研究としてきちんと扱いたいと個人的に思っています。&lt;/p&gt;
&lt;h2 id=&#34;生成力とは&#34;&gt;生成力とは？&lt;/h2&gt;
&lt;p&gt;「生成力（generative capacity）」とは、文法の「複雑さ」の概念です。
ここでの複雑さは、その文法が作る（生成する）ことのできる文字列もしくは木構造の種類をもとに定義されます。
ここで、文字列に関する生成力を「弱生成力 (weak generative capacity)」と、木構造に関する生成力を「強生成力 (strong generative capacity)」と呼びます (&lt;a href=&#34;https://www.jstor.org/stable/j.ctt17kk81z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chomsky, 1965&lt;/a&gt;; &lt;a href=&#34;https://www.iwanami.co.jp/book/b280256.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;福井・辻子, 2017; 日本語訳&lt;/a&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;弱生成力&lt;/strong&gt;は、ある文法が作ることのできる文字列（単語の配列）の集合に関する概念で、文法の弱生成力が等しいかつ語彙が等しい言語同士では、作ることのできる文字列の集合は完全に一致します。
つまり、弱生成力の議論において、文法とは、「容認可能な文字列のみを受理し、容認不可能な文字列は排除する装置」のことを意味します。
&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%81%E3%83%A7%E3%83%A0%E3%82%B9%E3%82%AD%E3%83%BC%E9%9A%8E%E5%B1%A4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;チョムスキー階層&lt;/a&gt;（下図。図は&lt;a href=&#34;https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0077&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jager and Rogers (2012)&lt;/a&gt;より。）という概念は、弱生成力のクラス分けに関するものです（&lt;a href=&#34;https://ieeexplore.ieee.org/document/1056813&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chomsky (1956)&lt;/a&gt;; &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0049237X08720238&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chomsky and Schutzenberger (1963)&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/240322mcsg/chom_hierarchy_hu8374134136788068203.webp 400w,
               /github-pages/blogs/240322mcsg/chom_hierarchy_hu9174748545354879079.webp 760w,
               /github-pages/blogs/240322mcsg/chom_hierarchy_hu1918991218216860271.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/240322mcsg/chom_hierarchy_hu8374134136788068203.webp&#34;
               width=&#34;760&#34;
               height=&#34;592&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;自然言語の弱生成力の議論に関しては、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0077&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jager and Rogers (2012)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://timhunter.humspace.ucla.edu/papers/blackwell-chomsky-hierarchy.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hunter (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rlevy.github.io/9.19-syllabus/syllabus.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Roger Levyの計算心理言語学の授業のWeek 7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;あたりが良かったです。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;強生成力&lt;/strong&gt;は、ある文法が作ることのできる木構造の集合に関する概念であり、Chomsky自身は弱生成力ではなく強生成力を研究の対象とするべし、としました。
木構造は（ほぼすべての理論において）意味や韻律の理論と直接関わるので、単に容認可能な文字列を識別する装置を考えるのでは不十分だ、ということでしょう。
ただ、強生成力それ自体に明確な定義があるわけではなく、そのために理論ニュートラルな議論が難しくなっている、というのが現状です（形式言語理論の世界だと、単純にある言語クラスにおいて作ることのできる木構造の集合について考えれば良く、本来ならば、理論言語学の言語理論についても同様のことができるはずですが、残念ながら多くの言語理論は、その木構造の集合を定義することができるほど厳密ではないように思えます。もちろん、以下で紹介する範疇文法理論やMinimalist Grammarなどの形式性と記述的妥当性の両面をちゃんと追い求めている文法理論たちを除いて）。
私が知らないだけかもしれませんが、Chomskyを中心とした主流生成文法においても、強生成力の議論がなされている、というイメージはないです。&lt;/p&gt;
&lt;p&gt;もちろん、強生成力についての議論はまったくないわけではなく、最近では、CCGとTAGが弱生成力だけではなく、強生成力においても等価である、と主張されています (&lt;a href=&#34;https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00393/106789/Strong-Equivalence-of-TAG-and-CCG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Schiffer and Maletti, 2021&lt;/a&gt;)。
（生成力が同一となると、最後はどちらの derivation step がより妥当か、という話になる。）&lt;/p&gt;
&lt;h2 id=&#34;文脈自由文法-context-free-grammar&#34;&gt;文脈自由文法 (Context-Free Grammar)&lt;/h2&gt;
&lt;p&gt;自然言語の構造について考えるとき、
文脈自由文法とは、$(\Gamma, \Sigma, S, R)$ の4つ組で定義されます。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\Gamma$: 非終端記号（非端末記号）の有限集合&lt;/li&gt;
&lt;li&gt;$\Sigma$: 終端記号（端末記号）の有限集合で、$\Gamma\cap \Sigma=\varnothing$&lt;/li&gt;
&lt;li&gt;$\mathit{S}\in \Gamma$: 開始記号&lt;/li&gt;
&lt;li&gt;$R$: 規則（生成規則）の有限集合。各規則は、$\alpha\rightarrow\beta$ の形をとる。ここで、$\alpha\in\Gamma$ は&lt;strong&gt;ちょうど1つ&lt;/strong&gt;の非終端記号、$\beta$ は $(\Gamma\cup\Sigma)^*$ の任意の列（空列を含む）であり、$\rightarrow$ は左側の要素から右側の要素への書き換えを意味します。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;「文脈自由」とは、書き換え対象はただ1つの非終端記号だけであり、他の要素から影響を受ける（書き換えに文脈が存在する）というわけではないことに由来します。
これにより、ネストした構造を表現することができるので、たとえば四則演算はまさに文脈自由文法でしょう。&lt;/p&gt;
&lt;!-- 
たとえば、次のような4つ組による文脈自由文法 $G$ を考えてみましょう：
1.  $\{ \mathit{N}, \mathit{V}, \mathit{Det}, \mathit{NP}, \mathit{VP}, \mathit{S} \}\in\Gamma$
2. $\{ \text{the, rat, cat, chased} \}\in\Sigma$
3. $\mathit{S}$
4.  - $\mathit{S}\rightarrow\mathit{NP}\ \mathit{VP}$
    - $\mathit{VP}\rightarrow\mathit{V}\ \mathit{NP}$
    - $\mathit{NP}\rightarrow\mathit{Det}\ \mathit{N}$
    - $\mathit{Det}\rightarrow\text{the}$
    - $\mathit{N}\rightarrow\text{rat}$
    - $\mathit{N}\rightarrow\text{cat}$
    - $\mathit{V}\rightarrow\text{chased}$

とすると、文法 $G$ が生成する言語 $\mathcal{L}(G)$ には、
    `(S (NP (Det the) (N rat)) (VP (V chased) (NP (Det the) (N cat))))` や
    `(S (NP (Det the) (N cat)) (VP (V chased) (NP (Det the) (N rat))))`
    が含まれるようになります。 --&gt;
&lt;h3 id=&#34;反復補題&#34;&gt;反復補題&lt;/h3&gt;
&lt;p&gt;ある言語が文脈自由&lt;strong&gt;ではない&lt;/strong&gt;ことは、以下の（文脈自由文法の）反復補題を使うことで示すことができる：&lt;/p&gt;
&lt;p&gt;文脈自由言語 $L$ には、長さが $m$ 以上の文字列 $w$ があり、これは適当な $w, v, x, y, z\in\Sigma^*$ によって $w=uvxyz$ で表すことができる。ここで、この $w$ は以下の3つの条件を満たす：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;任意の $i\geq 0$ に対して、$uv^ixy^iz\in L$&lt;/li&gt;
&lt;li&gt;$|vy|\geq 1$&lt;/li&gt;
&lt;li&gt;$|vxy|\leq m$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例として、$L = {a^nb^nc^n\mid n\geq 0}$ が文脈自由言語であるのかこの反復補題を用いて考えてみましょう。&lt;/p&gt;
&lt;h3 id=&#34;他の文法形式との等価性&#34;&gt;他の文法形式との等価性&lt;/h3&gt;
&lt;h4 id=&#34;projective-dependency-tree&#34;&gt;projective dependency tree&lt;/h4&gt;
&lt;p&gt;依存構造木において、依存関係同士が「交差」しないものを &lt;code&gt;projective&lt;/code&gt; なtreeであるといいます。
文脈自由文法で表現できる依存関係は、projectiveな依存関係にとどまることが知られています。&lt;/p&gt;
&lt;h4 id=&#34;ab-grammar&#34;&gt;AB grammar&lt;/h4&gt;
&lt;p&gt;範疇文法 (Categorial Grammar) において、関数合成しかない範疇文法は、文脈自由文法とその弱生成力が等価です。&lt;/p&gt;
&lt;h2 id=&#34;自然言語は文脈自由文法で扱えるのか&#34;&gt;自然言語は文脈自由文法で扱えるのか？&lt;/h2&gt;
&lt;p&gt;では、自然言語の文法は、文脈自由文法で表現することはできるのでしょうか。&lt;/p&gt;
&lt;p&gt;ここで、しょうもないですが大事な注意として、「文脈自由文法で表現することはできるのか？」という問いは、&lt;a href=&#34;https://www.degruyter.com/document/doi/10.1515/9783112316009/html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chomsky (1957)&lt;/a&gt; で指摘されているように、「文法が有限個」という前提を置いた上でのものです。
というのも、観測したすべての自然言語の文を「記述」したいと思ったとき、高々それらの文と同数の文法規則さえ用意することができれば、それは、文脈自由文法どころか有限状態オートマトンで「記述」したことになります。&lt;/p&gt;
&lt;p&gt;たとえば、projectiveな依存構造と文脈自由文法が等価ですが、依存構造が交差するとき、すなわち、non-projectiveであるとき、その構造は文脈自由文法では記述できません。
では、依存構造が交差する状況は自然言語に存在するのでしょうか？&lt;/p&gt;
&lt;h3 id=&#34;respectively読み&#34;&gt;respectively読み&lt;/h3&gt;
&lt;p&gt;こうした議論の中で、最初に（？）議論の俎上に上がったのが「respectively読み」でしょう。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/240322mcsg/respectively_hu7808455904040875432.webp 400w,
               /github-pages/blogs/240322mcsg/respectively_hu11395988369503543859.webp 760w,
               /github-pages/blogs/240322mcsg/respectively_hu9084051997729566803.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/240322mcsg/respectively_hu7808455904040875432.webp&#34;
               width=&#34;586&#34;
               height=&#34;162&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

図で示すように、日本語でも「それぞれ」をはさんで、主語と述語の依存関係が交差し（文字列で表すと、$a_1 b_1 a_2 b_2$ のようなコピー言語になり）、まさに文脈自由文法では記述的ないものになります。&lt;/p&gt;
&lt;p&gt;この現象は割とわかりやすいためか、今でも時々自然言語の文脈自由文法による記述の非妥当性を示す例として紹介されがちなのですが、実は、&lt;a href=&#34;&#34;&gt;Pullum and Gazdar (1982)&lt;/a&gt; により否定されています。
彼らの主張は至極単純で、respectively読みは単に統語の問題ではなく意味の問題でしょう、というものです。&lt;/p&gt;
&lt;h3 id=&#34;extraction&#34;&gt;Extraction&lt;/h3&gt;
&lt;p&gt;一方で、extraposition from NPやheavy NP shift、scramblingといった構文は、分析の仕方にも依りますが、非常に文脈依存性を感じるところではないでしょうか。
Extraposition from NPとは、たとえば
&lt;em&gt;The man fell into the pit who had been chased by dogs.&lt;/em&gt;
のような文で、ここで、&lt;em&gt;The man&lt;/em&gt; とそこにかかる関係節 &lt;em&gt;who &amp;hellip;&lt;/em&gt; の間に動詞句 &lt;em&gt;fell into the pit&lt;/em&gt; が入っている構造です。
Scramblingは、日本語でよくある、
&lt;em&gt;花子に太郎が会った。&lt;/em&gt; のような、名詞句の語順が通常と入れ替わった状態のものです。&lt;/p&gt;
&lt;p&gt;こうしたものも、Gazdar (1981) や Gazdar et al. (1985) によるGeneralized Context-Free Grammarにより記述可能であることが示されています。&lt;/p&gt;
&lt;h3 id=&#34;cross-serial-dependency&#34;&gt;Cross-serial dependency&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/BF00630917&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shieber (1985)&lt;/a&gt; により、Swiss-German（ドイツ語のスイス方言）の従属節中には、統語的にcross-serial dependencies（連続交差依存）が成り立ち、これにより自然言語には文脈自由文法では作れない表現が存在することが示されました。&lt;/p&gt;
&lt;p&gt;cross-serial dependenciesとは、以下のような（ここでは連続する動詞と対応する主格の項）の依存関係が交差しあっている状況です（文は &lt;a href=&#34;https://link.springer.com/article/10.1007/BF00630917&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shieber (1985)&lt;/a&gt; より）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/240322mcsg/swiss-german_hu7034314326232825233.webp 400w,
               /github-pages/blogs/240322mcsg/swiss-german_hu12724652800119912262.webp 760w,
               /github-pages/blogs/240322mcsg/swiss-german_hu869190537413510075.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/240322mcsg/swiss-german_hu7034314326232825233.webp&#34;
               width=&#34;760&#34;
               height=&#34;186&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;非常に雑にいうと、依存関係が交差しないネストした（入れ子構造になっている）関係だと文脈自由文法で表現できるのですが、連続交差依存は文脈自由文法では表現できません。
たとえば、高校生の頃、英語を読むときに句や節ごとに括弧でかこんで読んでいた人は多いと思うのですが（そうですよね？）、いま考えてみると、これは英語を入れ子構造として、すなわち文脈自由レベルの文法として解析していたことになります。
もしSwiss-Germanを勉強することになっていたら、このような読み方はできなかったはずです。&lt;/p&gt;
&lt;p&gt;cross-serial dependenciesはそうあるわけではなく、上記の文と同じ意味を日本語で表そうとすると、以下のように、依存関係は交差せず、入れ子構造になります。
cross-serial dependencies自体は確かに頻度は多くはないですが &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0378437117310580?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(e.g., Ferrer i Cancho et al., 2018)&lt;/a&gt;、存在しないわけではないので、自然言語の文法を文脈自由文法で済ますのは不十分であるといえます。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/240322mcsg/japanese_hu15470679320395784030.webp 400w,
               /github-pages/blogs/240322mcsg/japanese_hu2570914269836168450.webp 760w,
               /github-pages/blogs/240322mcsg/japanese_hu17957990151208278955.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/240322mcsg/japanese_hu15470679320395784030.webp&#34;
               width=&#34;760&#34;
               height=&#34;243&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;ちなみに、同時期に &lt;a href=&#34;https://link.springer.com/article/10.1007/BF00630918&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Culy (1985)&lt;/a&gt; にて、Bambaraを用いて、形態的にも文脈自由文法で表現できない事例が存在することが示されています。&lt;br&gt;
241212追記：Bambaraの例は音調的な振る舞いに依存しており、形態論の問題なのか？、という議論があるようで、結局、アイヌ語にて形態論レベルでのbeyond context-freenessが示されたようです &lt;a href=&#34;https://aclanthology.org/2024.tacl-1.36/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Sanuma and Aizawa, 2024)&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;cross-serial dependency は日本語にあるのでしょうか。&lt;/p&gt;
&lt;p&gt;また、&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2805_4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stabler (2004)&lt;/a&gt; では、英語においてもcross-serial dependenciesは存在している主張されています（画像は&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2805_4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stabler (2004; p.701)&lt;/a&gt; より）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /github-pages/blogs/240322mcsg/stabler_2004_hu16409881544769631867.webp 400w,
               /github-pages/blogs/240322mcsg/stabler_2004_hu5559447429778830936.webp 760w,
               /github-pages/blogs/240322mcsg/stabler_2004_hu13786223649734498953.webp 1200w&#34;
               src=&#34;https://kohei-kaji.github.io/github-pages/github-pages/blogs/240322mcsg/stabler_2004_hu16409881544769631867.webp&#34;
               width=&#34;760&#34;
               height=&#34;108&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;また、正規言語で自然言語を語るのはさすがに厳しそうに思われますが、文脈自由文法は実際に自然言語の記述に広く使われています。
例えば、&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC99T42&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Penn Treebank&lt;/a&gt; (&lt;a href=&#34;https://aclanthology.org/J93-2004/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marcus et al., 1993&lt;/a&gt;) はまさに文脈自由文法によるアノテーションであり、多言語に拡張されているところを見る限り、（実用上）大きな問題は生じていないようです。
同様に、&lt;a href=&#34;https://universaldependencies.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Universal Dependencies (UD)&lt;/a&gt; (&lt;a href=&#34;https://aclanthology.org/2020.lrec-1.497/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nirve et al., 2020&lt;/a&gt;) においても、（依存関係が交差していない木をprojective、交差している木をnon-projectiveというのですが、）他言語においてもほとんどの木がprojectiveであると報告されています。つまり、UDコーパスのほとんどは文脈自由文法で記述できているということです。&lt;/p&gt;
&lt;h2 id=&#34;自然言語は弱文脈依存言語なのか&#34;&gt;自然言語は弱文脈依存言語なのか？&lt;/h2&gt;
&lt;p&gt;用語や歴史の説明は抜きにして、結論から言うと、自然言語の文法が弱文脈依存文法（Mildly Context-Sensitive Grammar; MCSG）であるということは明確に示されているわけではありません。
ただ、少なくとも以下の2つの事実から、「自然言語の文法は弱文脈依存文法である」という言説には一定の合意が得られています。そのため、しばしば弱文脈依存仮説（MCS Hypothesis）などとも呼ばれています。&lt;br&gt;
なお、mildly context sensitiveという用語・概念は &lt;a href=&#34;https://www.cambridge.org/core/books/abs/natural-language-parsing/tree-adjoining-grammars-how-much-contextsensitivity-is-required-to-provide-reasonable-structural-descriptions/81BFD6DAC6B0CB24A3042A06E964F2E1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joshi (1985)&lt;/a&gt; が初出です。&lt;/p&gt;
&lt;h3 id=&#34;1-文脈自由文法では表現できない構文が自然言語に存在する&#34;&gt;1. 文脈自由文法では表現できない構文が自然言語に存在する。&lt;/h3&gt;
&lt;h3 id=&#34;2-数多くの独立に提案された文法理論が弱文脈依存文法である&#34;&gt;2. 数多くの（独立に提案された）文法理論が、弱文脈依存文法である。&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0022000075800195&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joshi et al. (1975)&lt;/a&gt; によるTree Adjoining Grammar（TAG; 木接合文法）をはじめ、多くの文法理論が、特に80-90年代にかけて、計算・数理言語学の分野で提案されましたが、そのほとんどが最終的に文脈自由文法と文脈依存文法の間にいることが示されました。
これは、多くの計算・数理言語学者の間である種の「合意」がとれた状況とも言えるでしょう。&lt;/p&gt;
&lt;p&gt;これについては、&lt;a href=&#34;https://academic.oup.com/book/26119/chapter-abstract/194151165?redirectedFrom=fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stabler (2013)&lt;/a&gt; にてわかりやすくレビューされていますので、少々長いですが引用して提示しておきます。&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Stabler (2013); p.4より&lt;/summary&gt;
In particular, a very significant computational consensus was identified by Joshi (1985) in his hypothesis that human languages are both strongly and weakly mildly context sensitive (MCS). While any empirical test of this hypothesis still depends on a network of theoretical assumptions, the claim is so fundamental that it can be connected to many diverse traditions in grammar. To say that language is &#34;strongly and weakly&#34; MCS is to say that MCS grammars can both define the sentences of human languages (weak adequacy) and also provide the structures of those languages (strong adequacy). Joshi’s original definition of MCS grammars was partly informal, so there are now various precise versions of his claim. One is that human languages are defined by tree adjoining grammars (TAGs) or closely related grammars, and another theoretically weaker (and hence empirically stronger) position is that human language are definable by the more expressive (set local) multi-component TAGs or closely related grammars. The most remarkable thing about this claim came out of the innocent-sounding phrase &#34;or closely related grammars,&#34; because it was discovered that a wide range of independently proposed grammar formalisms falls under that description. In particular, a series of papers beginning in the 1980’s and 1990’s established the following inclusion relations among the languages defined by various kinds of grammars, across traditions:
CFG ⊂ CCG = TAG ⊂ MCTAG = ACG = MCFG = MG ⊂ CSG
&lt;/details&gt;
&lt;p&gt;上記引用にて、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CFGはContext-Free Grammar（文脈自由文法）&lt;/li&gt;
&lt;li&gt;CCGはCombinatory Categorial Grammar（組合せ範疇文法; &lt;a href=&#34;https://link.springer.com/article/10.1007/BF00360804&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ades and Steedman (1982)&lt;/a&gt;; &lt;a href=&#34;https://mitpress.mit.edu/9780262691932/surface-structure-and-interpretation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman (1996)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;TAGはTree Adjoining Grammar（木接合文法; &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0022000075800195&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joshi et al. (1975)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;MCTAGはMulti-Component Tree Adjoining Grammar（&lt;a href=&#34;https://benjamins.com/catalog/z.35.07jos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joshi (1987)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;ACGはAbstract Categorial Grammar（&lt;a href=&#34;https://aclanthology.org/P01-1033/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;de Groote (2001)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;MCFGはMultiple Context Free Grammar（多重文脈自由文法; &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/030439759190374B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seki et al. (1991)&lt;/a&gt;）
&lt;ul&gt;
&lt;li&gt;MCFGはLCFRS（Linear Context Free Rewriting System; &lt;a href=&#34;https://dl.acm.org/doi/10.3115/981175.981190&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vijay-Shanker et al. (1987)&lt;/a&gt;）と同一。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MGはMinimalist Grammar（&lt;a href=&#34;https://link.springer.com/chapter/10.1007/BFb0052152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stabler (1997)&lt;/a&gt;; &lt;a href=&#34;https://academic.oup.com/edited-volume/38634/chapter/335332383&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stabler (2011)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;CSGはContext Sensitive Grammar（文脈依存文法）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を指します（定訳があるものは定訳を、また、その文法が提案されている代表的な論文情報を付与しています）。&lt;/p&gt;
&lt;p&gt;また、このほかにも、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Indexed Grammar（LIG; &lt;a href=&#34;https://cir.nii.ac.jp/crid/1571698599593629312&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pollard (1984)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;Head Grammar（HG; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-94-009-1337-0_3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gazdar (1985)&lt;/a&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;も、弱文脈依存文法の仲間であることが示されています (&lt;a href=&#34;https://link.springer.com/article/10.1007/BF01191624&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vijay-Shanker and Weir, 1994&lt;/a&gt;)。&lt;/p&gt;
&lt;p&gt;ただ、ここで面白いのは、弱文脈依存文法は一枚岩ではなく、上記の &lt;a href=&#34;https://academic.oup.com/book/26119/chapter-abstract/194151165?redirectedFrom=fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stabler (2013)&lt;/a&gt; の引用の最後の関係式で表されているように、2つのクラスに分類できます。
具体的に、CCG, TAG, LIG, HGと、MCTAG, ACG, MCFG, LCFRS, MGです（MGにも複数の亜種があります）。&lt;/p&gt;
&lt;p&gt;その後の議論については、たとえば &lt;a href=&#34;https://aclanthology.org/2021.cl-1.2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanojevic and Steedman (2020)&lt;/a&gt; や &lt;a href=&#34;https://benjamins.com/catalog/elt.00033.fra&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frank and Hunter (2021)&lt;/a&gt; なども。&lt;/p&gt;
&lt;p&gt;もちろん、各文法理論内でも、「どのような操作を仮定するか」で揺れはあるので、上記の関係が必ずしも成り立つとは限りません。たとえばCCGは、slash-typingを導入することで（＝組合せ規則の適用に制限をかけることで）、TAGよりも弱生成力が低くなることも示されています (&lt;a href=&#34;https://direct.mit.edu/coli/article/41/2/215/1507/Lexicalization-and-Generative-Power-in-CCG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kuhlmann et al., 2015&lt;/a&gt;)。&lt;/p&gt;
&lt;p&gt;また、Head-driven Phrase Structure Grammar (HPSG; &lt;a href=&#34;https://press.uchicago.edu/ucp/books/book/chicago/H/bo3618318.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pollard and Sag (1994)&lt;/a&gt;) やType Logical Grammar (TLG; &lt;a href=&#34;https://plato.stanford.edu/entries/typelogical-grammar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;適切な引用がわからないのでSEPへ&lt;/a&gt;) のような、チューリング完全な文法理論もあります。&lt;/p&gt;
&lt;h3 id=&#34;多重文脈自由文法-multiple-context-free-grammar&#34;&gt;多重文脈自由文法 (Multiple Context-Free Grammar)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;参考：&lt;a href=&#34;https://alexc17.github.io/static/pdfs/mcfgsforlinguists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clark (2014)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;弱文脈依存文法の心理的妥当性&#34;&gt;弱文脈依存文法の心理的妥当性&lt;/h2&gt;
&lt;p&gt;上記のような議論ほど有名ではありませんが、最近では、計算心理言語学の分野においても、弱文脈依存文法 (MCSG) の文脈自由文法 (CFG) に対する優位性は主張されています。
たとえば、&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0093934X15300687&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brennan et al. (2016)&lt;/a&gt; や &lt;a href=&#34;https://academic.oup.com/book/34998/chapter-abstract/298696095?redirectedFrom=fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Li and Hale (2019)&lt;/a&gt; はMGとCFGを比較して、&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1111/cogs.13312&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanojevic et al. (2023)&lt;/a&gt; はCCGとCFGを比較して、それぞれMCSGであるMG, CCGの方が、CFGよりも適切に、人が物語を聞いている間の&lt;a href=&#34;https://bsd.neuroinf.jp/wiki/%E6%A9%9F%E8%83%BD%E7%9A%84%E7%A3%81%E6%B0%97%E5%85%B1%E9%B3%B4%E7%94%BB%E5%83%8F%E6%B3%95#BOLD%E4%BF%A1%E5%8F%B7%E3%81%AE%E7%99%BA%E8%A6%8B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BOLD信号（Blood Oxygen Level Dependent signals; 脳活動を反映した信号）&lt;/a&gt;を予測できることを示しました。&lt;/p&gt;
&lt;p&gt;もちろん、&lt;a href=&#34;https://www.annualreviews.org/content/journals/10.1146/annurev-linguistics-051421-020803&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hale et al. (2022)&lt;/a&gt; で指摘されているように、こうした結果は必ずしもMCSGが &lt;em&gt;uniquely the right theory of human grammar&lt;/em&gt; (&lt;a href=&#34;https://www.annualreviews.org/content/journals/10.1146/annurev-linguistics-051421-020803&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hale et al., 2022; p.12&lt;/a&gt;) であるということを意味しているわけではありませんが、MCSGが、人間の文処理に関して、CFGでは説明できていないところを説明できているのは確かです。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに：&lt;/h2&gt;
&lt;p&gt;個人的には、CCGやMGによる人間の文処理のモデリング研究から計算言語学・計算心理言語学の勉強を始めた人間なので、弱文脈依存仮説は「記述研究で示されているのならそういうもの」と理解して、それから「ではなぜ自然言語は弱文脈依存なのか？」ということを考えています。が、言語を使用する際には（プッシュダウン）オートマトンっぽく使っているなぁ、正規言語〜文脈自由文法っぽく理解しているなぁという直感や、cross-serialは必ずしも処理が難しいとは限らない &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01690968608404677&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Bach et al., 1986)&lt;/a&gt; という事実、non-projectivityは処理のしやすさである程度説明できるかもしれないが、すべてではない、というモデリング研究の進展 &lt;a href=&#34;https://aclanthology.org/2022.cl-2.5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Yadav et al., 2022)&lt;/a&gt; を見ると、もっと使用の観点から文法の複雑さについて考えないといけないよなぁとか、そもそもここでの「文法の複雑さ」はどれだけ意味のある見方なのかなぁなんて思うときはあります。&lt;/p&gt;
&lt;p&gt;というのも、生成力は「最悪のケース」に関する議論である一方、人間の逐次的な処理における「複雑さ」は人間の記憶や予測における平均的な認知負荷を捉えており、長距離依存などはある程度複雑な文法による解析が必要ですが、たとえば日本語の助動詞などは正規言語でも解析可能である、という事実を考えると、自然言語の文法に関する議論を必ずしも最悪のケースから考えないでもいいよな、もっと平均的な処理負荷の観点から再検討すべきだよな、とは直感的に考えています（といったことは、&lt;a href=&#34;&#34;&gt;『自然言語処理』の学会記事 (to appear)&lt;/a&gt; にもちょっと書きました）。
そういう意味で、認知のベイジアンモデリング (Bayesian modeling of cognition) や、情報理論によるコミュニケーションのモデル化における合理性 (rationality) に魅力を感じています。
ので、博士課程ではゆっくり時間をかけて考えながら取り組んでみたいな、とは（現時点では）考えています。&lt;/p&gt;
&lt;!-- CCGのFunction CompositionとかType Raisingは記述のために必要なのではなく、逐次的な構造構築のために必要で、その結果として言語はMCSレベルにいるのでは？みたいな。 --&gt;</description>
    </item>
    
    <item>
      <title>統語変形はコミュニケーションから創発するのか？</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/kajikawa-etal-2024-nlp/</link>
      <pubDate>Wed, 13 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/kajikawa-etal-2024-nlp/</guid>
      <description>&lt;!-- 
「統語変形」といった（言語を文脈自由以上にする）強力な統語操作が**なぜ**自然言語に存在するのか？という問いへの説明を目指すという目的意識は我ながら非常に面白いと思っていますし、今後もこの方向性で進もうと思っています。\
おそらく委員特別賞をいただけたのも、この目的意識が「面白い」と認識していただけたからだと思っており、この点が評価されたのは素直に嬉しく思います。\
一方で、手法に関してはよろしくないと考えています。特にここに関して学会の場でフィードバックをいただきたく言語処理学会に参加しました。\
結果として、多くの方から有益なコメント・アドバイスを頂けたので、大満足です。\
今後も言語処理学会に何かしらを出し続けようと強く思える会でした（来年以降の自分へ）。 --&gt;
</description>
    </item>
    
    <item>
      <title>統語変換は文化進化から生じるのか？：等位接続構造での検討</title>
      <link>https://kohei-kaji.github.io/github-pages/misc/%E8%A8%80%E8%AA%9E%E5%AD%A6%E3%83%95%E3%82%A7%E3%82%B9-2024/</link>
      <pubDate>Sat, 20 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/misc/%E8%A8%80%E8%AA%9E%E5%AD%A6%E3%83%95%E3%82%A7%E3%82%B9-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>統語変換はコミュニケーションから創発するのか？</title>
      <link>https://kohei-kaji.github.io/github-pages/misc/ninjalsalon-2024/</link>
      <pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/misc/ninjalsalon-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>自然言語の統語構造における相互情報量の解析と数理モデル化</title>
      <link>https://kohei-kaji.github.io/github-pages/misc/mims-2023/</link>
      <pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/misc/mims-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analyzing Japanese Cleft Construction in Combinatory Categorial Grammar</title>
      <link>https://kohei-kaji.github.io/github-pages/international_conference/kajikawa-2023-lenls/kajikawa-2023/</link>
      <pubDate>Sun, 19 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/international_conference/kajikawa-2023-lenls/kajikawa-2023/</guid>
      <description>&lt;!-- 日本語のcleft（分裂文・断裂文）において、focus（焦点）位置に絶対にガ格名詞句を置くことができない（たとえば、　*花子を呼んだのは太郎がだ*　は非文。）が、他の格ならOK（*花子が呼んだのは太郎をだ*　はOK）、という事実を、CCGの統語論で解決することを提案した論文です。
具体的に、Kratzer (1996) によるconstructivistのアプローチを参考にした格付与のシステムをCCGで形式化しています。
これにより、日本語においてガ格だけが他の格と絶対的に異なる挙動をするという事実（ガ格名詞句は長距離かき混ぜできない、ガ格名詞句はsmall clause内におけない）も説明できることを示しています。 --&gt;</description>
    </item>
    
    <item>
      <title>CCGによる日本語文処理のモデリング</title>
      <link>https://kohei-kaji.github.io/github-pages/misc/ewfl8-2023/</link>
      <pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/misc/ewfl8-2023/</guid>
      <description>&lt;!--

---
title: CCGによる日本語文処理のモデリング
summary: 梶川康平 / Encouraging Workshop on Formal Linguistics 8 (EWFL8) / 東京大学 駒場キャンパス 18号館
date: &#34;2023-03-29&#34;

# Optional external URL for project (replaces project detail page).
external_link: &#34;https://ewflling.com/&#34;

url_code: &#34;&#34;
url_pdf: &#34;&#34;
url_slides: &#34;https://www.dropbox.com/s/jfbo2nfz0ur4sse/230329_EWFL_Kajikawa.pdf?dl=0&#34;
url_video: &#34;&#34;

--- --&gt;
</description>
    </item>
    
    <item>
      <title>CCGによる日本語文処理のモデリング</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/kajikawa-etal-2023-nlp/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/kajikawa-etal-2023-nlp/</guid>
      <description>&lt;!-- CCG（組合せ範疇文法）を使って、日本語の逐次的な文処理をモデリングした初の研究です。\
(i) ある地点 (文節) の読み時間を、その時点で作ることのできるCCGの二分木の数で予測可能なこと、(ii) parsing strategyとして英語で有効だと示されているReveal operation (Stanojevic et al., 2019; 2020; 2021) が、日本語では必ずしも妥当ではない、ということを主に主張したものです。\
卒論の内容だったので書けることを4ページに詰め込んだつもりだったが、そのためにわかりやすさを犠牲にしていたことがこの原稿の反省点です。 --&gt;
</description>
    </item>
    
    <item>
      <title>極小主義に動機づけられた統語的教示に基づく言語モデル</title>
      <link>https://kohei-kaji.github.io/github-pages/domestic_conference/isono-etal-2023-nlp/</link>
      <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/domestic_conference/isono-etal-2023-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CCGに関する情報集</title>
      <link>https://kohei-kaji.github.io/github-pages/blogs/230112ccg/</link>
      <pubDate>Thu, 12 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/blogs/230112ccg/</guid>
      <description>&lt;p&gt;生成文法理論の１つであるCCG（組合せ範疇文法）を、私は文法理論としても、そして文処理のモデリングの道具としても非常に有用なものであると考えていますが、残念ながら（分野によっては）人口に膾炙しているとは言い難い状況ですので、CCGの広まりを願い、ここに文献リストや参照したら良いであろう情報を記します。&lt;/p&gt;
&lt;h1 id=&#34;文献案内&#34;&gt;文献案内&lt;/h1&gt;
&lt;h3 id=&#34;さらっと雰囲気を掴みたい場合&#34;&gt;さらっと雰囲気を掴みたい場合&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/E.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jurafsky and Martin (to appear)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;NLPの教科書として有名なDan Jurafsky氏とJames Martin氏による &lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech and Language Processing&lt;/a&gt; のDraftのAppendixの１つ。&lt;/li&gt;
&lt;li&gt;最低限の組合せ規則と、CCGによるチャートパーザが簡潔に説明されている。&lt;/li&gt;
&lt;li&gt;言語学の方でも、前半だけ読むと、CCGにどういう文法操作があるのか・どういう構造を作るのか何となく掴みやすいのではないか。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homepages.inf.ed.ac.uk/steedman/papers/ccg/moravcsik2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman (2022)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Mark Steedman氏による、網羅的なCCGの解説論文。本をギュッと圧縮したイメージ。&lt;/li&gt;
&lt;li&gt;CCG分析で特徴的な構文や関連トピックについて最低限の紙幅で説明されてる。&lt;/li&gt;
&lt;li&gt;私ははじめ、これと、ここで引用されている文献を行き来して勉強した。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.10044&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clark (2021, arXiv)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/J07-4004/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C&amp;amp;C parser&lt;/a&gt;のStephen Clark氏によるCCGのparsingに関する論文。&lt;/li&gt;
&lt;li&gt;後半部分は当時最新のCCG parser作りに関する話だが、前半は、CCGの理論やCCGのparsingの概説・歴史解説として読める。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;理論言語学関係&#34;&gt;理論言語学関係&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;統語・意味の理論としてのCCGを理解したい場合に参照すべき文献たちです。&lt;/li&gt;
&lt;li&gt;その他、CCGには音韻の理論としての顔もありますが、そのあたりは詳しくないので含めていません（&lt;a href=&#34;https://mitpress.mit.edu/9780262692687/the-syntactic-process/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman (2000)&lt;/a&gt; にはそのあたりも載っています）。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mitpress.mit.edu/9780262691932/surface-structure-and-interpretation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman (1996)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;CCG本の最初。&lt;/li&gt;
&lt;li&gt;LI Monographs。100ページ強。&lt;/li&gt;
&lt;li&gt;CCGで、英語を網羅的に記述できることを示すべく書かれたもの。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mitpress.mit.edu/9780262692687/the-syntactic-process/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman (2000)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Steedman (1996) の強化版。&lt;/li&gt;
&lt;li&gt;若干の理論的改訂あり。&lt;/li&gt;
&lt;li&gt;Google Scholarでは出版年が誤って2001となっており、ちらほら Steedman (2001) として論文中に登場してしまう。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://era.ed.ac.uk/handle/1842/562&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baldridge (2002)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Jason Baldridge氏による博論。&lt;/li&gt;
&lt;li&gt;slash typingを導入し、言語理論としてのCCGを大きく進化させた。
&lt;ul&gt;
&lt;li&gt;slash type については、&lt;a href=&#34;https://aclanthology.org/E03-1036/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baldridge and Kruijff (2003; EACL)&lt;/a&gt;も参照。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Steedman (2000) までと違い、等位接続を文法規則 ではなくconjunctのカテゴリ X\X/X で導出。&lt;/li&gt;
&lt;li&gt;博論なので、解説としても有用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.9640.jp/book_view/?468&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;戸次 (2010)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;日本語CCG本。&lt;/li&gt;
&lt;li&gt;日本語文法の論文としても面白い。
&lt;ul&gt;
&lt;li&gt;特に活用体系や、量化子周りの話。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/nihongonokenkyu/7/3/7_KJ00007729621/_pdf/-char/ja&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;書評 (矢田部, 2011)&lt;/a&gt; も面白い。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1002/9781444395037.ch5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman and Baldridge (2011)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;非常に簡潔にまとまったCCGの解説論文。&lt;/li&gt;
&lt;li&gt;そのため、Steedman (1996)や(2000)と並んでしばしば引用されている。&lt;/li&gt;
&lt;li&gt;（CCGを引用したいくらいなら、本を読むのは大変なので、こちらを読んで引用した方が良さそう。）&lt;/li&gt;
&lt;li&gt;Non-Transformational Syntaxという本の1チャプター。
&lt;ul&gt;
&lt;li&gt;この本はほかの章も良い。Sag and Wasowの章が好き。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://direct.mit.edu/ling/article-abstract/doi/10.1162/ling_a_00521/117700/On-Internal-Merge?redirectedFrom=PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman (2023)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;ほとんどミニマリストに向けて書いてあるLIの論文。
&lt;ul&gt;
&lt;li&gt;ミニマリストの理論を踏まえ、「CCGならもっと簡潔に書けますよ」と主張したもの。著者の穏健な姿勢が窺える。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ミニマリストがCCGをはじめる際には一番わかりやすいと思われる（自分がミニマリストではないのでわからないが）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homepages.inf.ed.ac.uk/steedman/papers/ccg/book2DRAFT160622.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steedman (to appear)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Mark Steedman氏によるCCG本のドラフト。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;機械の文処理関係&#34;&gt;機械の文処理関係&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ここでの機械の文処理とは、文を入力とし、確率的に計算した構文木を返してくれる構文解析器等のことです。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/J07-4004/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clark and Curran (2007; CL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://web.archive.org/web/20160318193242/http://svn.ask.it.usyd.edu.au/trac/candc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C&amp;amp;C parser&lt;/a&gt;として有名。&lt;/li&gt;
&lt;li&gt;CCGbankを使った最初のwide-coverage parser。&lt;/li&gt;
&lt;li&gt;いまだに現役だったりする。Supertaggerの評価用としてよく見る。
&lt;ul&gt;
&lt;li&gt;Supertagとは、CCG含め、lexicalized theory of grammarの語彙範疇のこと。&lt;/li&gt;
&lt;li&gt;通常のPOS tagよりも情報が豊富なので、supertagと呼ばれる。そして、supertagが決まればほとんど文の構造は決まるので、supertaggingは&lt;code&gt;almost parsing&lt;/code&gt;であるとも言われている &lt;a href=&#34;https://aclanthology.org/J99-2004/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Bangalore and Joshi, 1999; CL)&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/J07-3004/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hockenmaier and Steedman (2007; CL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;英語CCGbank。
&lt;ul&gt;
&lt;li&gt;空白なし小文字b&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Penn Treebank (WSJ) からの自動変換。　&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/D14-1107/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lewis and Steedman (2014; EMNLP)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mikelewis0/easyccg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EasyCCG&lt;/a&gt; として有名。&lt;/li&gt;
&lt;li&gt;深層学習 + A* search。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/P13-1103/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Uematsu et al. (2013; ACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;日本語CCGBank。
&lt;ul&gt;
&lt;li&gt;空白なし大文字B&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;係り受けコーパスである京都大学テキストコーパス（毎日新聞）からの自動変換。&lt;/li&gt;
&lt;li&gt;日本語CCGBankの続きとしては、以下のようなものがあります。
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2020.lrec-1.639/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubota et al. (2020; LREC)&lt;/a&gt; による &lt;a href=&#34;https://github.com/ABCTreebank/ABCTreebank&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ABCTreebank&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.compling.jp/keyaki/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Keyaki Treebank&lt;/a&gt; からの自動変換&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2024.eacl-srw.14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tomita et al. (2024; EACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ABCTreebank/ABCTreebank&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ABCTreebank&lt;/a&gt; と &lt;a href=&#34;https://github.com/DaisukeBekki/lightblue&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lightblue&lt;/a&gt; による日本語CCGBankの再構築&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/P16-4018/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Noji and Miyao (2016; ACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;日本語CCG parserの &lt;a href=&#34;https://github.com/mynlp/jigg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jigg&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclweb.org/anthology/P/P16/P16-4015/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Martinez-Gomez et al. (2016; ACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;意味解析システムの &lt;a href=&#34;https://github.com/mynlp/ccg2lambda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ccg2lambda&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-662-53826-5_4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bekki and Kawazoe (2016; LNTCS)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;日本語CCG parserの &lt;a href=&#34;https://github.com/DaisukeBekki/lightblue&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lightblue&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/P17-1026/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yoshikawa et al. (2017; ACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;日本語CCG parserの &lt;a href=&#34;https://github.com/masashi-y/depccg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;depccg&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;人間の文処理関係&#34;&gt;人間の文処理関係&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/BF00360804&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ades and Steedman (1982; Linguist Philos)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;一番最初のCCG論文。&lt;/li&gt;
&lt;li&gt;当初より逐次的な文処理を意図して作っていることが明確で良い。&lt;/li&gt;
&lt;li&gt;notationは今とところどころ異なる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/W12-4623/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demberg (2012; TAG+)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;CCGの逐次的な構造構築に関して、統語論・心理言語学の知見から（否定的に）述べられている。&lt;/li&gt;
&lt;li&gt;CCGではfull incremental parseができない（英語の目的語関係節）。
&lt;ul&gt;
&lt;li&gt;full incremental parseを実現しようとDコンビネータを導入すると過剰生成する、という指摘。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13312&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanojevic et al. (2023; Cognitive Science)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;CCGによる、英語文処理（fMRIによる&lt;a href=&#34;https://bsd.neuroinf.jp/wiki/%E6%A9%9F%E8%83%BD%E7%9A%84%E7%A3%81%E6%B0%97%E5%85%B1%E9%B3%B4%E7%94%BB%E5%83%8F%E6%B3%95#BOLD%E4%BF%A1%E5%8F%B7%E3%81%AE%E7%99%BA%E8%A6%8B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BOLD信号&lt;/a&gt;）のモデリング。&lt;/li&gt;
&lt;li&gt;貢献は大きく分けて2つ。
&lt;ol&gt;
&lt;li&gt;CCGが、CFG（文脈自由文法）よりもより高い精度でBOLD信号を予測できることを示した。言語理論としてより妥当なCCGが、逐次的な文処理のモデル化においても優れていることを示した。&lt;/li&gt;
&lt;li&gt;CCGの構造構築操作由来の予測子と、LLMで算出したsurprisal（文処理における強力な予測子）とは別にBOLD信号の予測に効いた。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://kohei-kaji.github.io/github-pages/publication/kajikawa-etal-2024-cogsci/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kajikawa et al. (2024; CogSci)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;日本語と英語の視線計測データで、CCG内の理論的に異なる文法操作が、それぞれ心理的にも異なるものとして使われていることを示唆。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010027724000520?dgcid=author&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Isono (2024; Cognition)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;文を逐次的に理解する際に起こる、短期記憶に由来する処理負荷を、CCGの木構造ベースで説明したもの。&lt;/li&gt;
&lt;li&gt;貢献・面白い点は、短期記憶由来の処理負荷は、今まで簡単な文脈自由文法（記述力は妥当ではない）や依存文法（単語間関係の記述は優れているが、逐次的に構造がどう構築されるかは不明瞭）でしかなかったが、それをCCG (記述力が妥当かつ、構造構築過程も明確) に発展させたこと。
&lt;ul&gt;
&lt;li&gt;個人的に、CCGの良さは、単に「competence grammarのままでprocessingのことをちゃんと語れそうな理論」、ということだけではなく、「いろいろなことができすぎない理論」だと思っています。具体的に、ここでは、構成素同士の合成にちゃんと制限があって、必ずしも何でも組合せられるわけではないが（つまり、単語が順に入ってきたとき、毎度毎度その単語をすでに作っている構成素に統合できるとは限らない）、この論文では、その組合せられないポイントを証拠に人間の文処理が説明できることが経験的に示されています。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;生成力関係&#34;&gt;生成力関係&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/BF01191624&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vijay-Shanker and Weir (1994; Math. Systems Theory)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;CCGの弱生成力が、Linear-Indexed Grammar (LIG), Head Grammar (HG), Tree-Adjoining Grammar (TAG) と等価であることを示した。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://direct.mit.edu/coli/article/41/2/215/1507/Lexicalization-and-Generative-Power-in-CCG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kuhlmann et al. (2015; CL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/BF01191624&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vijay-Shanker and Weir (1994)&lt;/a&gt; のときに想定されていたCCGではなく、slash-typeを導入したCCGにて、TAGと弱生成力が等価であることを示した。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.tacl-1.43/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Schiffer and Maletti (2021; TACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;CCGの強生成力が、TAGと等価だと主張。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;標準形関係&#34;&gt;標準形関係&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CCGでは、同じ意味を複数の異なる統語構造で表現することができます（spurious ambiguity; 擬似的曖昧性）。このおかげで、逐次的な合成による構造構築が可能なのですが、構造的曖昧性がなくとも構文木が一意に定まらないということなので、parserを作る上では問題になると考えられていました。
&lt;ul&gt;
&lt;li&gt;実際には、学習データのbranchingが一貫していれば、標準形の制約なしでも擬似的曖昧性の問題にはぶつからないようです (&lt;a href=&#34;https://aclanthology.org/P17-1026/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yoshikawa et al. (2017; ACL)&lt;/a&gt;, &lt;a href=&#34;https://www.jstage.jst.go.jp/article/jnlp/26/1/26_83/_article/-char/ja/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yoshikawa et al. (2019; 自然言語処理)&lt;/a&gt; より)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;標準形の定義により、（構造的曖昧性がないとき）統語構造を１つに絞ることができます。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/P96-1011/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eisner (1996; ACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.jhu.edu/~jason/papers/eisner.acl96-proof.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;証明に関する情報&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;可能な限り関数合成（function composition）を行わないという制限により、right-branchingな標準形を定義。&lt;/li&gt;
&lt;li&gt;もちろん、逆の制限にすれば、left-branchingを標準形とすることもできる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/P08-1038/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hoyt and Baldridge (2008; ACL)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Dコンビネータの導入と、それを含めた標準形の定義。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/C10-1053/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hockenmaier and Bisk (2010; COLING)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/P96-1011/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eisner (1996; ACL)&lt;/a&gt;の拡張。&lt;/li&gt;
&lt;li&gt;generalized compositionとgrammatical type-raisingを考慮した拡張。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ccg-parserを触ってみよう&#34;&gt;CCG parserを触ってみよう！&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;CCGは、他の文法理論に比べ、高精度な構文解析器（parser）が数おおく整備されている、という点で非常に有用です。
&lt;ul&gt;
&lt;li&gt;semantic parsingに適度に使いやすいといったことや、ツリーバンクの整備が早かった、という点が要因な気がしています（当時を知らないので妄想です）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;特に、下記のparserたちは動かすのにそこまで難易度が高くないのでおすすめです。&lt;/li&gt;
&lt;li&gt;そもそも構文解析器とはなんぞやという方へ
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coronasha.co.jp/np/isbn/9784339027594/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;構文解析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ohmsha.co.jp/book/9784274229008/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;自然言語処理の基礎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech and Language Processing (3rd ed. draft)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;17章の &lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/17.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Context-Free Grammars and Constituency Parsing&lt;/a&gt; や、18章の &lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/18.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dependency Parsing&lt;/a&gt; あたり。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;depccghttpsgithubcommasashi-ydepccg&#34;&gt;&lt;a href=&#34;https://github.com/masashi-y/depccg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;depccg&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pythonによる英日CCG parser。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;mailto:Python@3.6&#34;&gt;Python@3.6&lt;/a&gt;以上、gcc@4.8以上が必要。&lt;/li&gt;
&lt;li&gt;READMEが丁寧なので、基本そのまま従えば動かせる。
&lt;ul&gt;
&lt;li&gt;1点だけ、&lt;code&gt;depccg_{en/ja} download&lt;/code&gt;コマンドはうまくいかないので、モデルの学習済みパラメータはリンク先のGoogle Driveから直接落としてこないといけない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;（追記：）最新のmacだとAllenNLPがローカルで動かせないという噂。Dockerで解決するしかない、という話を小耳に挟んだことがある。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lightbluehttpsgithubcomdaisukebekkilightblue&#34;&gt;&lt;a href=&#34;https://github.com/DaisukeBekki/lightblue&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lightblue&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Haskellによる日本語CCG parser。&lt;/li&gt;
&lt;li&gt;Macユーザーなら、tidyは（おそらく）初めから入っているし、JUMAN++はHomebrew経由で入れられる。&lt;/li&gt;
&lt;li&gt;残りはREADMEに従えば動かせる。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.9640.jp/book_view/?468&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;戸次 (2010)&lt;/a&gt;+アルファがそのまま実装されているので、語彙項目を参照するのに使い勝手が良い。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ccgtoolshttpsgithubcomstanojevicccgtools&#34;&gt;&lt;a href=&#34;https://github.com/stanojevic/ccgtools&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ccgtools&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python, Cythonによる英中CCG parser。&lt;/li&gt;
&lt;li&gt;高性能 (元State-of-the-Art)。&lt;/li&gt;
&lt;li&gt;作成者であるMilos Stanojevic氏は、Scalaで &lt;a href=&#34;https://github.com/stanojevic/Rotating-CCG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating-CCG&lt;/a&gt; というparserも作っている（論文は &lt;a href=&#34;https://aclanthology.org/N19-1020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanojevic and Steedman (2019; NAACL)&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;Google Colab上で動かせるようにしてくれているので、環境構築の必要がなく非常に便利。&lt;/li&gt;
&lt;li&gt;（追記：）読み込みができなくなっていた（？）が、エラーメッセージでpre-trained modelのリンクを教えてくれるので、それらをダウンロードすれば、ローカルで動かすことができる。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ccgの導出木をlatexでかく&#34;&gt;CCGの導出木をLaTeXでかく&lt;/h1&gt;
&lt;h3 id=&#34;ccgstyhttpsgithubcomjasonbaldridgecg-latex&#34;&gt;&lt;a href=&#34;https://github.com/jasonbaldridge/cg-latex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ccg.sty&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jason Baldridge氏によるスタイルファイル。&lt;/li&gt;
&lt;li&gt;その他、CTL, 証明木用のスタイルファイルも公開してくれている。&lt;/li&gt;
&lt;li&gt;使い方：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ccg.sty&lt;/code&gt;をTeXファイルと同じディレクトリにおき、以下のようにする：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\documentclass[10pt,a4paper]{article}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\usepackage{ccg}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\begin{document}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\deriv{3}{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    \text{Taro} &amp;amp; \text{likes} &amp;amp; \text{Hanako} \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    \uline{1} &amp;amp; \uline{1} &amp;amp; \uline{1} \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    \mathit{NP} &amp;amp; \mathit{S\bs NP/NP} &amp;amp; \mathit{NP} \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;amp; \fapply{2} \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;amp; \mc{2}{\mathit{S\bs NP}} \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    \bapply{3} \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    \mc{3}{\mathit{S}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\end{document}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;導出木のサイズを変えるのには、&lt;code&gt;\deriv{hoge}{fuga}&lt;/code&gt;の前にたとえば&lt;code&gt;\scriptsize\deriv{hoge}{fuga}&lt;/code&gt;のようにする。&lt;/li&gt;
&lt;li&gt;linguexパッケージで文番号をつける：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\ex.\label{hoge-label}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\deriv{hoge}{fuga}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ulem&lt;/code&gt;パッケージ（下線とか打ち消し線を挿入する用のパッケージ）を入れると、ccg.sty内の&lt;code&gt;\uline{}&lt;/code&gt;が衝突してしまう。
&lt;ul&gt;
&lt;li&gt;コマンドの名前が一致していなければ良いだけなので、ccg.styに&lt;code&gt;\newcommand{\ulines}[1]{\ul{#1}}&lt;/code&gt;などと追加して、&lt;code&gt;\deriv&lt;/code&gt;内では&lt;code&gt;\ul{}&lt;/code&gt;を使うようにすれば良い。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ccg-latexstyhttpsgithubcombozsahinccg-latex&#34;&gt;&lt;a href=&#34;https://github.com/bozsahin/ccg-latex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ccg-latex.sty&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cem Bozsahin氏によるスタイルファイル。&lt;/li&gt;
&lt;li&gt;上述のccg.styよりも充実していそうに思われる（少なくとも、READMEやexampleは充実している。更新も割と頻繁にしているよう？）。&lt;/li&gt;
&lt;li&gt;個人的には、ccg.styで困っていないので使っていないが、いつか乗り換えても良いのかなぁと思っている。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Formalizing Argument Structures with Combinatory Categorial Grammar</title>
      <link>https://kohei-kaji.github.io/github-pages/international_conference/isono-etal-2022-lenls/isono-etal-2022/</link>
      <pubDate>Wed, 11 Jan 2023 06:07:50 +0000</pubDate>
      <guid>https://kohei-kaji.github.io/github-pages/international_conference/isono-etal-2022-lenls/isono-etal-2022/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
